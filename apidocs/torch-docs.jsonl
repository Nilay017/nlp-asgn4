{"snippet": "Tensor.expm1_()", "intent": "In-place version of expm1 ( )", "question_id": 0},
{"snippet": "torch.nn.quantized.functional.elu(input, scale, zero_point)", "intent": "This is the quantized version of elu ( ) . With arguments `input`, `scale`, `zero_point`.", "question_id": 1},
{"snippet": "torch.nn.quantized.functional.elu(input, scale, zero_point, alpha=1.0)", "intent": "This is the quantized version of elu ( ) . With arguments `input`, `scale`, `zero_point`, `alpha`.", "question_id": 2},
{"snippet": "Tensor.select(dim, index)", "intent": "Slices the self tensor along the selected dimension at the given `index` . With arguments `dim`.", "question_id": 3},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`.", "question_id": 4},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`.", "question_id": 5},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dropout=0.1)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dropout`.", "question_id": 6},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, activation='relu')", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `activation`.", "question_id": 7},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, layer_norm_eps=1e-05)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `layer_norm_eps`.", "question_id": 8},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, batch_first=False)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `batch_first`.", "question_id": 9},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, device=None)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `device`.", "question_id": 10},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dtype=None)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dtype`.", "question_id": 11},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `dropout`.", "question_id": 12},
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, activation='relu')", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `activation`.", "question_id": 13},
{"snippet": "transformer_encoder_layer.forward(src)", "intent": "Pass the input through the encoder layer . With arguments `src`.", "question_id": 14},
{"snippet": "transformer_encoder_layer.forward(src, src_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_mask`.", "question_id": 15},
{"snippet": "transformer_encoder_layer.forward(src, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_key_padding_mask`.", "question_id": 16},
{"snippet": "transformer_encoder_layer.forward(src, src_mask=None, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_mask`, `src_key_padding_mask`.", "question_id": 17},
{"snippet": "torch.linalg.tensorinv(A)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal .", "question_id": 18},
{"snippet": "torch.linalg.tensorinv(A, ind=2)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal .", "question_id": 19},
{"snippet": "torch.linalg.tensorinv(A, out=None)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal . With arguments `out`.", "question_id": 20},
{"snippet": "torch.linalg.tensorinv(A, ind=2, out=None)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal . With arguments `out`.", "question_id": 21},
{"snippet": "Tensor.is_cuda", "intent": "Is True if the Tensor is stored on the GPU, False otherwise.", "question_id": 22},
{"snippet": "torch.round(input)", "intent": "Returns a new tensor with each of the elements of `input` rounded to the closest integer .", "question_id": 23},
{"snippet": "torch.round(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` rounded to the closest integer . With arguments `out`.", "question_id": 24},
{"snippet": "torch.log(input)", "intent": "Returns a new tensor with the natural logarithm of the elements of `input` .", "question_id": 25},
{"snippet": "torch.log(input, out=None)", "intent": "Returns a new tensor with the natural logarithm of the elements of `input` . With arguments `out`.", "question_id": 26},
{"snippet": "Tensor.addcmul(tensor1, tensor2)", "intent": "See torch.addcmul ( ) With arguments `tensor1`, `tensor2`.", "question_id": 27},
{"snippet": "Tensor.addcmul(tensor1, tensor2, value=1)", "intent": "See torch.addcmul ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 28},
{"snippet": "Tensor.index_fill(tensor1, dim, index, value)", "intent": "Out-of-place version of torch.Tensor.index_fill_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_fill_ ( ) . With arguments `dim`, `index`, `value`.", "question_id": 29},
{"snippet": "Tensor.int_repr()", "intent": "Given a quantized Tensor , self.int_repr ( ) returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor .", "question_id": 30},
{"snippet": "Tensor.slogdet()", "intent": "See torch.slogdet ( )", "question_id": 31},
{"snippet": "torch.nn.functional.one_hot(tensor)", "intent": "Takes LongTensor with index values of shape ( * ) and returns a `tensor` of shape ( * , `num_classes` ) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor , in which case it will be 1 .", "question_id": 32},
{"snippet": "torch.nn.functional.one_hot(tensor, num_classes=- 1)", "intent": "Takes LongTensor with index values of shape ( * ) and returns a `tensor` of shape ( * , `num_classes` ) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor , in which case it will be 1 .", "question_id": 33},
{"snippet": "Tensor.to_sparse(sparseDims)", "intent": "Returns a sparse copy of the tensor . With arguments `sparseDims`.", "question_id": 34},
{"snippet": "torch.nn.functional.hardtanh(input)", "intent": "Applies the HardTanh function element-wise . With arguments `input`.", "question_id": 35},
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`.", "question_id": 36},
{"snippet": "torch.nn.functional.hardtanh(input, max_val=1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `max_val`.", "question_id": 37},
{"snippet": "torch.nn.functional.hardtanh(input, inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `inplace`.", "question_id": 38},
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., max_val=1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `max_val`.", "question_id": 39},
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `inplace`.", "question_id": 40},
{"snippet": "torch.nn.functional.hardtanh(input, max_val=1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `max_val`, `inplace`.", "question_id": 41},
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., max_val=1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `max_val`, `inplace`.", "question_id": 42},
{"snippet": "Tensor.div(value)", "intent": "See torch.div ( ) With arguments `value`.", "question_id": 43},
{"snippet": "Tensor.div(value, rounding_mode=None)", "intent": "See torch.div ( ) With arguments `value`, `rounding_mode`.", "question_id": 44},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`.", "question_id": 45},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`.", "question_id": 46},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`.", "question_id": 47},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `reduce`.", "question_id": 48},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `reduction`.", "question_id": 49},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `size_average`.", "question_id": 50},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduce`.", "question_id": 51},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduction`.", "question_id": 52},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduce`.", "question_id": 53},
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduction`.", "question_id": 54},
{"snippet": "Tensor.fill_diagonal_(fill_value)", "intent": "Fill the main diagonal of a tensor that has at least 2-dimensions . With arguments `fill_value`.", "question_id": 55},
{"snippet": "Tensor.fill_diagonal_(fill_value, wrap=False)", "intent": "Fill the main diagonal of a tensor that has at least 2-dimensions . With arguments `fill_value`, `wrap`.", "question_id": 56},
{"snippet": "torch.jit.save(m, f)", "intent": "Save an offline version of this module for use in a separate process . With arguments `m`, `f`.", "question_id": 57},
{"snippet": "torch.jit.save(m, f, _extra_files=None)", "intent": "Save an offline version of this module for use in a separate process . With arguments `m`, `f`, `_extra_files`.", "question_id": 58},
{"snippet": "torch.nn.MultiMarginLoss()", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) :", "question_id": 59},
{"snippet": "torch.nn.MultiMarginLoss(p=1)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`.", "question_id": 60},
{"snippet": "torch.nn.MultiMarginLoss(margin=1.0)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `margin`.", "question_id": 61},
{"snippet": "torch.nn.MultiMarginLoss(weight=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : Optionally , you can give non-equal weighting on the classes by passing a 1D `weight` tensor into the constructor .", "question_id": 62},
{"snippet": "torch.nn.MultiMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `size_average`.", "question_id": 63},
{"snippet": "torch.nn.MultiMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `reduce`.", "question_id": 64},
{"snippet": "torch.nn.MultiMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `reduction`.", "question_id": 65},
{"snippet": "torch.nn.MultiMarginLoss(p=1, margin=1.0)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`, `margin`.", "question_id": 66},
{"snippet": "torch.nn.MultiMarginLoss(p=1, weight=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : Optionally , you can give non-equal weighting on the classes by passing a 1D `weight` tensor into the constructor . With arguments `p`.", "question_id": 67},
{"snippet": "torch.nn.MultiMarginLoss(p=1, size_average=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`, `size_average`.", "question_id": 68},
{"snippet": "torch.nn.functional.softshrink(input)", "intent": "Applies the soft shrinkage function elementwise With arguments `input`.", "question_id": 69},
{"snippet": "torch.nn.functional.softshrink(input, lambd=0.5)", "intent": "Applies the soft shrinkage function elementwise With arguments `input`, `lambd`.", "question_id": 70},
{"snippet": "Tensor.ne_(other)", "intent": "In-place version of ne ( ) . With arguments `other`.", "question_id": 71},
{"snippet": "torch.atanh(input)", "intent": "Returns a new tensor with the inverse hyperbolic tangent of the elements of `input` .", "question_id": 72},
{"snippet": "torch.atanh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic tangent of the elements of `input` . With arguments `out`.", "question_id": 73},
{"snippet": "Tensor.tensor_split(indices_or_sections)", "intent": "See torch.tensor_split ( ) With arguments `indices_or_sections`.", "question_id": 74},
{"snippet": "Tensor.tensor_split(indices_or_sections, dim=0)", "intent": "See torch.tensor_split ( ) With arguments `indices_or_sections`, `dim`.", "question_id": 75},
{"snippet": "torch.nn.ConstantPad2d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 76},
{"snippet": "torch.linalg.pinv(A)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 77},
{"snippet": "torch.linalg.pinv(A, rcond=1e-15)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation .", "question_id": 78},
{"snippet": "torch.linalg.pinv(A, hermitian=False)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`.", "question_id": 79},
{"snippet": "torch.linalg.pinv(A, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 80},
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, hermitian=False)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `hermitian`.", "question_id": 81},
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `out`.", "question_id": 82},
{"snippet": "torch.linalg.pinv(A, hermitian=False, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`, `out`.", "question_id": 83},
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, hermitian=False, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `hermitian`, `out`.", "question_id": 84},
{"snippet": "torch.addmm(input, mat1, mat2)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result .", "question_id": 85},
{"snippet": "torch.addmm(input, mat1, mat2, beta=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 86},
{"snippet": "torch.addmm(input, mat1, mat2, alpha=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 87},
{"snippet": "torch.addmm(input, mat1, mat2, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 88},
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, alpha=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 89},
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 90},
{"snippet": "torch.addmm(input, mat1, mat2, alpha=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 91},
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, alpha=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 92},
{"snippet": "torch.nn.utils.parametrize.register_parametrization(module, tensor_name, parametrization)", "intent": "Adds a `parametrization` to a tensor in a `module` . With arguments `tensor_name`.", "question_id": 93},
{"snippet": "Tensor.requires_grad_()", "intent": "Change if autograd should record operations on this tensor : sets this tensor \u2019 s `requires_grad` attribute in-place .", "question_id": 94},
{"snippet": "Tensor.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on this tensor : sets this tensor \u2019 s `requires_grad` attribute in-place .", "question_id": 95},
{"snippet": "Tensor.negative_()", "intent": "In-place version of negative ( )", "question_id": 96},
{"snippet": "torch.autograd.set_grad_enabled(mode)", "intent": "Context-manager that sets gradient calculation to on or off . set_grad_enabled will enable or disable grads based on its argument `mode` .", "question_id": 97},
{"snippet": "torch.vstack(tensors)", "intent": "Stack `tensors` in sequence vertically ( row wise ) .", "question_id": 98},
{"snippet": "torch.vstack(tensors, out=None)", "intent": "Stack `tensors` in sequence vertically ( row wise ) . With arguments `out`.", "question_id": 99},
{"snippet": "torch.absolute(input)", "intent": "Alias for torch.abs ( ) With arguments `input`.", "question_id": 100},
{"snippet": "torch.absolute(input, out=None)", "intent": "Alias for torch.abs ( ) With arguments `input`, `out`.", "question_id": 101},
{"snippet": "torch.log1p(input)", "intent": "Returns a new tensor with the natural logarithm of ( 1 + `input` ) .", "question_id": 102},
{"snippet": "torch.log1p(input, out=None)", "intent": "Returns a new tensor with the natural logarithm of ( 1 + `input` ) . With arguments `out`.", "question_id": 103},
{"snippet": "torch.pinverse(input)", "intent": "Alias for torch.linalg.pinv ( ) With arguments `input`.", "question_id": 104},
{"snippet": "torch.pinverse(input, rcond=1e-15)", "intent": "Alias for torch.linalg.pinv ( ) With arguments `input`, `rcond`.", "question_id": 105},
{"snippet": "Tensor.bfloat16()", "intent": "self.bfloat16 ( ) is equivalent to self.to ( torch.bfloat16 ) .", "question_id": 106},
{"snippet": "Tensor.bfloat16(memory_format=torch.preserve_format)", "intent": "self.bfloat16 ( ) is equivalent to self.to ( torch.bfloat16 ) . With arguments `memory_format`.", "question_id": 107},
{"snippet": "Tensor.add_(other)", "intent": "In-place version of add ( ) With arguments `other`.", "question_id": 108},
{"snippet": "Tensor.add_(other, alpha=1)", "intent": "In-place version of add ( ) With arguments `other`, `alpha`.", "question_id": 109},
{"snippet": "torch.full_like(input, fill_value, \\*)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`.", "question_id": 110},
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`.", "question_id": 111},
{"snippet": "torch.full_like(input, fill_value, \\*, layout=torch.strided)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `layout`.", "question_id": 112},
{"snippet": "torch.full_like(input, fill_value, \\*, device=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `device`.", "question_id": 113},
{"snippet": "torch.full_like(input, fill_value, \\*, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `requires_grad`.", "question_id": 114},
{"snippet": "torch.full_like(input, fill_value, \\*, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `memory_format`.", "question_id": 115},
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, layout=torch.strided)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `layout`.", "question_id": 116},
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `device`.", "question_id": 117},
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `requires_grad`.", "question_id": 118},
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `memory_format`.", "question_id": 119},
{"snippet": "torch.nn.Softmin()", "intent": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0 , 1 ] and sum to 1 .", "question_id": 120},
{"snippet": "torch.nn.Softmin(dim=None)", "intent": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0 , 1 ] and sum to 1 . With arguments `dim`.", "question_id": 121},
{"snippet": "torch.triu(input)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 122},
{"snippet": "torch.triu(input, diagonal=0)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The upper triangular part of the matrix is defined as the elements on and above the `diagonal` .", "question_id": 123},
{"snippet": "torch.triu(input, out=None)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 124},
{"snippet": "torch.triu(input, diagonal=0, out=None)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The upper triangular part of the matrix is defined as the elements on and above the `diagonal` .", "question_id": 125},
{"snippet": "Tensor.bernoulli_()", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) .", "question_id": 126},
{"snippet": "Tensor.bernoulli_(p=0.5)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) .", "question_id": 127},
{"snippet": "Tensor.bernoulli_(generator=None)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) . With arguments `generator`.", "question_id": 128},
{"snippet": "Tensor.bernoulli_(p=0.5, generator=None)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) . With arguments `generator`.", "question_id": 129},
{"snippet": "Tensor.index_fill_(dim, index, value)", "intent": "Fills the elements of the self tensor with `value` value by selecting the indices in the order given in `index` . With arguments `dim`.", "question_id": 130},
{"snippet": "torch.baddbmm(input, batch1, batch2)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result .", "question_id": 131},
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 132},
{"snippet": "torch.baddbmm(input, batch1, batch2, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 133},
{"snippet": "torch.baddbmm(input, batch1, batch2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 134},
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 135},
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 136},
{"snippet": "torch.baddbmm(input, batch1, batch2, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 137},
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 138},
{"snippet": "Tensor.atan()", "intent": "See torch.atan ( )", "question_id": 139},
{"snippet": "Tensor.masked_fill(mask, value)", "intent": "Out-of-place version of torch.Tensor.masked_fill_ ( ) With arguments `mask`, `value`.", "question_id": 140},
{"snippet": "Tensor.take(indices)", "intent": "See torch.take ( ) With arguments `indices`.", "question_id": 141},
{"snippet": "Tensor.detach_()", "intent": "Detaches the Tensor from the graph that created it , making it a leaf .", "question_id": 142},
{"snippet": "torch.rand_like(input)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) .", "question_id": 143},
{"snippet": "torch.rand_like(input, dtype=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`.", "question_id": 144},
{"snippet": "torch.rand_like(input, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `layout`.", "question_id": 145},
{"snippet": "torch.rand_like(input, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `device`.", "question_id": 146},
{"snippet": "torch.rand_like(input, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `requires_grad`.", "question_id": 147},
{"snippet": "torch.rand_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `memory_format`.", "question_id": 148},
{"snippet": "torch.rand_like(input, dtype=None, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `layout`.", "question_id": 149},
{"snippet": "torch.rand_like(input, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `device`.", "question_id": 150},
{"snippet": "torch.rand_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `requires_grad`.", "question_id": 151},
{"snippet": "torch.rand_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `memory_format`.", "question_id": 152},
{"snippet": "torch.nn.functional.rrelu(input)", "intent": "Randomized leaky ReLU . With arguments `input`.", "question_id": 153},
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`.", "question_id": 154},
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`.", "question_id": 155},
{"snippet": "torch.nn.functional.rrelu(input, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `training`.", "question_id": 156},
{"snippet": "torch.nn.functional.rrelu(input, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `inplace`.", "question_id": 157},
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, upper=1. / 3)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `upper`.", "question_id": 158},
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `training`.", "question_id": 159},
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `inplace`.", "question_id": 160},
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`, `training`.", "question_id": 161},
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`, `inplace`.", "question_id": 162},
{"snippet": "Tensor.exp_()", "intent": "In-place version of exp ( )", "question_id": 163},
{"snippet": "torch.rsqrt(input)", "intent": "Returns a new tensor with the reciprocal of the square-root of each of the elements of `input` .", "question_id": 164},
{"snippet": "torch.rsqrt(input, out=None)", "intent": "Returns a new tensor with the reciprocal of the square-root of each of the elements of `input` . With arguments `out`.", "question_id": 165},
{"snippet": "Tensor.nansum()", "intent": "See torch.nansum ( )", "question_id": 166},
{"snippet": "Tensor.nansum(dim=None)", "intent": "See torch.nansum ( ) With arguments `dim`.", "question_id": 167},
{"snippet": "Tensor.nansum(keepdim=False)", "intent": "See torch.nansum ( ) With arguments `keepdim`.", "question_id": 168},
{"snippet": "Tensor.nansum(dtype=None)", "intent": "See torch.nansum ( ) With arguments `dtype`.", "question_id": 169},
{"snippet": "Tensor.nansum(dim=None, keepdim=False)", "intent": "See torch.nansum ( ) With arguments `dim`, `keepdim`.", "question_id": 170},
{"snippet": "Tensor.nansum(dim=None, dtype=None)", "intent": "See torch.nansum ( ) With arguments `dim`, `dtype`.", "question_id": 171},
{"snippet": "Tensor.nansum(keepdim=False, dtype=None)", "intent": "See torch.nansum ( ) With arguments `keepdim`, `dtype`.", "question_id": 172},
{"snippet": "Tensor.nansum(dim=None, keepdim=False, dtype=None)", "intent": "See torch.nansum ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 173},
{"snippet": "torch.nn.functional.l1_loss(input, target)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`.", "question_id": 174},
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`.", "question_id": 175},
{"snippet": "torch.nn.functional.l1_loss(input, target, reduce=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduce`.", "question_id": 176},
{"snippet": "torch.nn.functional.l1_loss(input, target, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduction`.", "question_id": 177},
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 178},
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 179},
{"snippet": "torch.nn.functional.l1_loss(input, target, reduce=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 180},
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 181},
{"snippet": "Tensor.cauchy_()", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution :", "question_id": 182},
{"snippet": "Tensor.cauchy_(median=0)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`.", "question_id": 183},
{"snippet": "Tensor.cauchy_(sigma=1)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `sigma`.", "question_id": 184},
{"snippet": "Tensor.cauchy_(generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `generator`.", "question_id": 185},
{"snippet": "Tensor.cauchy_(median=0, sigma=1)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `sigma`.", "question_id": 186},
{"snippet": "Tensor.cauchy_(median=0, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `generator`.", "question_id": 187},
{"snippet": "Tensor.cauchy_(sigma=1, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `sigma`, `generator`.", "question_id": 188},
{"snippet": "Tensor.cauchy_(median=0, sigma=1, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `sigma`, `generator`.", "question_id": 189},
{"snippet": "Tensor.floor_()", "intent": "In-place version of floor ( )", "question_id": 190},
{"snippet": "torch.quantization.quantize_fx.prepare_qat_fx(model, qconfig_dict)", "intent": "Prepare a `model` for quantization aware training : param model : torch.nn.Module model , must be in train mode : param `qconfig_dict` : see prepare_fx ( ) : param `prepare_custom_config_dict` : see prepare_fx ( )", "question_id": 191},
{"snippet": "torch.quantization.quantize_fx.prepare_qat_fx(model, qconfig_dict, prepare_custom_config_dict=None)", "intent": "Prepare a `model` for quantization aware training : param model : torch.nn.Module model , must be in train mode : param `qconfig_dict` : see prepare_fx ( ) : param `prepare_custom_config_dict` : see prepare_fx ( )", "question_id": 192},
{"snippet": "torch.enable_grad", "intent": "Context-manager that enables gradient calculation.", "question_id": 193},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 194},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 195},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, padding=0)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 196},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 197},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 198},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 199},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 200},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 201},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 202},
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 203},
{"snippet": "Tensor.logical_not()", "intent": "See torch.logical_not ( )", "question_id": 204},
{"snippet": "torch.quantize_per_tensor(input, scale, zero_point, dtype)", "intent": "Converts a float tensor to a quantized tensor with given `scale` and zero point . With arguments `input`, `zero_point`, `dtype`.", "question_id": 205},
{"snippet": "torch.nn.Linear(in_features, out_features)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`.", "question_id": 206},
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`.", "question_id": 207},
{"snippet": "torch.nn.Linear(in_features, out_features, device=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `device`.", "question_id": 208},
{"snippet": "torch.nn.Linear(in_features, out_features, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `dtype`.", "question_id": 209},
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, device=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `device`.", "question_id": 210},
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 211},
{"snippet": "torch.nn.Linear(in_features, out_features, device=None, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `device`, `dtype`.", "question_id": 212},
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `device`, `dtype`.", "question_id": 213},
{"snippet": "Tensor.vsplit(split_size_or_sections)", "intent": "See torch.vsplit ( ) With arguments `split_size_or_sections`.", "question_id": 214},
{"snippet": "Tensor.tile(*reps)", "intent": "See torch.tile ( ) With arguments `*reps`.", "question_id": 215},
{"snippet": "Tensor.hardshrink()", "intent": "See torch.nn.functional.hardshrink ( )", "question_id": 216},
{"snippet": "Tensor.hardshrink(lambd=0.5)", "intent": "See torch.nn.functional.hardshrink ( ) With arguments `lambd`.", "question_id": 217},
{"snippet": "torch.nn.functional.sigmoid(input)", "intent": "Applies the element-wise function Sigmoid ( x ) =11+exp\u2061 ( \u2212x ) \\text { Sigmoid } ( x ) = \\frac { 1 } { 1 + \\exp ( -x ) } Sigmoid ( x ) =1+exp ( \u2212x ) 1\u200b With arguments `input`.", "question_id": 218},
{"snippet": "Tensor.mode()", "intent": "See torch.mode ( )", "question_id": 219},
{"snippet": "Tensor.mode(dim=None)", "intent": "See torch.mode ( ) With arguments `dim`.", "question_id": 220},
{"snippet": "Tensor.mode(keepdim=False)", "intent": "See torch.mode ( ) With arguments `keepdim`.", "question_id": 221},
{"snippet": "Tensor.mode(dim=None, keepdim=False)", "intent": "See torch.mode ( ) With arguments `dim`, `keepdim`.", "question_id": 222},
{"snippet": "torch.eig(input)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`.", "question_id": 223},
{"snippet": "torch.eig(input, eigenvectors=False)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`.", "question_id": 224},
{"snippet": "torch.eig(input, out=None)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`, `out`.", "question_id": 225},
{"snippet": "torch.eig(input, eigenvectors=False, out=None)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`, `out`.", "question_id": 226},
{"snippet": "Tensor.bmm(batch2)", "intent": "See torch.bmm ( ) With arguments `batch2`.", "question_id": 227},
{"snippet": "torch.nn.functional.relu_(input)", "intent": "In-place version of relu ( ) . With arguments `input`.", "question_id": 228},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`.", "question_id": 229},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`.", "question_id": 230},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, p=2)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `p`.", "question_id": 231},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, eps=1e-06)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `eps`.", "question_id": 232},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, swap=False)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `swap`.", "question_id": 233},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, size_average=None)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `size_average`.", "question_id": 234},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, reduce=None)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `reduce`.", "question_id": 235},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, reduction='mean')", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `reduction`.", "question_id": 236},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`, `p`.", "question_id": 237},
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, eps=1e-06)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`, `eps`.", "question_id": 238},
{"snippet": "Tensor.index_copy_(dim, index, tensor)", "intent": "Copies the elements of `tensor` into the self tensor by selecting the indices in the order given in `index` . For example , if `dim` == 0 and index [ i ] == j , then the ith row of tensor is copied to the jth row of self .", "question_id": 239},
{"snippet": "torch.clamp(input)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 240},
{"snippet": "torch.clamp(input, min=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 241},
{"snippet": "torch.clamp(input, max=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 242},
{"snippet": "torch.clamp(input, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 243},
{"snippet": "torch.clamp(input, min=None, max=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 244},
{"snippet": "torch.clamp(input, min=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 245},
{"snippet": "torch.clamp(input, max=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 246},
{"snippet": "torch.clamp(input, min=None, max=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 247},
{"snippet": "Tensor.q_scale()", "intent": "Given a Tensor quantized by linear ( affine ) quantization , returns the scale of the underlying quantizer ( ) .", "question_id": 248},
{"snippet": "Tensor.stft(n_fft)", "intent": "See torch.stft ( ) With arguments `n_fft`.", "question_id": 249},
{"snippet": "Tensor.stft(n_fft, hop_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `hop_length`.", "question_id": 250},
{"snippet": "Tensor.stft(n_fft, win_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `win_length`.", "question_id": 251},
{"snippet": "Tensor.stft(n_fft, window=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `window`.", "question_id": 252},
{"snippet": "Tensor.stft(n_fft, center=True)", "intent": "See torch.stft ( ) With arguments `n_fft`, `center`.", "question_id": 253},
{"snippet": "Tensor.stft(n_fft, pad_mode='reflect')", "intent": "See torch.stft ( ) With arguments `n_fft`, `pad_mode`.", "question_id": 254},
{"snippet": "Tensor.stft(n_fft, normalized=False)", "intent": "See torch.stft ( ) With arguments `n_fft`, `normalized`.", "question_id": 255},
{"snippet": "Tensor.stft(n_fft, onesided=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `onesided`.", "question_id": 256},
{"snippet": "Tensor.stft(n_fft, return_complex=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `return_complex`.", "question_id": 257},
{"snippet": "Tensor.stft(n_fft, hop_length=None, win_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `hop_length`, `win_length`.", "question_id": 258},
{"snippet": "torch.fft.rfftfreq(n)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` .", "question_id": 259},
{"snippet": "torch.fft.rfftfreq(n, d=1.0)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`.", "question_id": 260},
{"snippet": "torch.fft.rfftfreq(n, out=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `out`.", "question_id": 261},
{"snippet": "torch.fft.rfftfreq(n, dtype=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `dtype`.", "question_id": 262},
{"snippet": "torch.fft.rfftfreq(n, layout=torch.strided)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `layout`.", "question_id": 263},
{"snippet": "torch.fft.rfftfreq(n, device=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `device`.", "question_id": 264},
{"snippet": "torch.fft.rfftfreq(n, requires_grad=False)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `requires_grad`.", "question_id": 265},
{"snippet": "torch.fft.rfftfreq(n, d=1.0, out=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `out`.", "question_id": 266},
{"snippet": "torch.fft.rfftfreq(n, d=1.0, dtype=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `dtype`.", "question_id": 267},
{"snippet": "torch.fft.rfftfreq(n, d=1.0, layout=torch.strided)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `layout`.", "question_id": 268},
{"snippet": "torch.quantization.quantize(model, run_fn, run_args)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`.", "question_id": 269},
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, mapping=None)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `mapping`.", "question_id": 270},
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, inplace=False)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `inplace`.", "question_id": 271},
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `mapping`, `inplace`.", "question_id": 272},
{"snippet": "torch.all(input)", "intent": "Tests if all elements in `input` evaluate to True .", "question_id": 273},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 274},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 275},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 276},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 277},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 278},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 279},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 280},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 281},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 282},
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 283},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`.", "question_id": 284},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 285},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduce=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 286},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 287},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 288},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 289},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduce=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 290},
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 291},
{"snippet": "Tensor.swapaxes(axis0, axis1)", "intent": "See torch.swapaxes ( ) With arguments `axis0`, `axis1`.", "question_id": 292},
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values .", "question_id": 293},
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, size=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`.", "question_id": 294},
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `scale_factor`.", "question_id": 295},
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`, `scale_factor`.", "question_id": 296},
{"snippet": "Tensor.item()", "intent": "Returns the value of this tensor as a standard Python number .", "question_id": 297},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 298},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 299},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 300},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 301},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 302},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 303},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 304},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 305},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 306},
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 307},
{"snippet": "torch.can_cast(from, to)", "intent": "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation . With arguments `from`, `to`.", "question_id": 308},
{"snippet": "torch.fft.fftfreq(n)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` .", "question_id": 309},
{"snippet": "torch.fft.fftfreq(n, d=1.0)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`.", "question_id": 310},
{"snippet": "torch.fft.fftfreq(n, out=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `out`.", "question_id": 311},
{"snippet": "torch.fft.fftfreq(n, dtype=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `dtype`.", "question_id": 312},
{"snippet": "torch.fft.fftfreq(n, layout=torch.strided)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `layout`.", "question_id": 313},
{"snippet": "torch.fft.fftfreq(n, device=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `device`.", "question_id": 314},
{"snippet": "torch.fft.fftfreq(n, requires_grad=False)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `requires_grad`.", "question_id": 315},
{"snippet": "torch.fft.fftfreq(n, d=1.0, out=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `out`.", "question_id": 316},
{"snippet": "torch.fft.fftfreq(n, d=1.0, dtype=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `dtype`.", "question_id": 317},
{"snippet": "torch.fft.fftfreq(n, d=1.0, layout=torch.strided)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `layout`.", "question_id": 318},
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 319},
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 320},
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, padding_value=0.0)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 321},
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 322},
{"snippet": "Tensor.amax()", "intent": "See torch.amax ( )", "question_id": 323},
{"snippet": "Tensor.amax(dim=None)", "intent": "See torch.amax ( ) With arguments `dim`.", "question_id": 324},
{"snippet": "Tensor.amax(keepdim=False)", "intent": "See torch.amax ( ) With arguments `keepdim`.", "question_id": 325},
{"snippet": "Tensor.amax(dim=None, keepdim=False)", "intent": "See torch.amax ( ) With arguments `dim`, `keepdim`.", "question_id": 326},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 327},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 328},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 329},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 330},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 331},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 332},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 333},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 334},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 335},
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 336},
{"snippet": "torch.logical_not(input)", "intent": "Computes the element-wise logical NOT of the given `input` tensor .", "question_id": 337},
{"snippet": "torch.logical_not(input, out=None)", "intent": "Computes the element-wise logical NOT of the given `input` tensor . With arguments `out`.", "question_id": 338},
{"snippet": "Tensor.diag_embed()", "intent": "See torch.diag_embed ( )", "question_id": 339},
{"snippet": "Tensor.diag_embed(offset=0)", "intent": "See torch.diag_embed ( ) With arguments `offset`.", "question_id": 340},
{"snippet": "Tensor.diag_embed(dim1=- 2)", "intent": "See torch.diag_embed ( ) With arguments `dim1`.", "question_id": 341},
{"snippet": "Tensor.diag_embed(dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `dim2`.", "question_id": 342},
{"snippet": "Tensor.diag_embed(offset=0, dim1=- 2)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim1`.", "question_id": 343},
{"snippet": "Tensor.diag_embed(offset=0, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim2`.", "question_id": 344},
{"snippet": "Tensor.diag_embed(dim1=- 2, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `dim1`, `dim2`.", "question_id": 345},
{"snippet": "Tensor.diag_embed(offset=0, dim1=- 2, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim1`, `dim2`.", "question_id": 346},
{"snippet": "torch.autograd.inference_mode()", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 347},
{"snippet": "torch.autograd.inference_mode(mode=True)", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 348},
{"snippet": "Tensor.digamma()", "intent": "See torch.digamma ( )", "question_id": 349},
{"snippet": "torch.tensordot(a, b)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions .", "question_id": 350},
{"snippet": "torch.tensordot(a, b, dims=2)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . When called with a non-negative integer argument `dims` = ddd , and the number of dimensions of a and b is mmm and nnn , respectively , tensordot ( ) computes", "question_id": 351},
{"snippet": "torch.tensordot(a, b, out=None)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . With arguments `out`.", "question_id": 352},
{"snippet": "torch.tensordot(a, b, dims=2, out=None)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . When called with a non-negative integer argument `dims` = ddd , and the number of dimensions of a and b is mmm and nnn , respectively , tensordot ( ) computes With arguments `out`.", "question_id": 353},
{"snippet": "torch.nn.intrinsic.ConvBnReLU2d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 2d , Batch Norm 2d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 354},
{"snippet": "Tensor.view(*shape)", "intent": "Returns a new tensor with the same data as the self tensor but of a different shape . With arguments `*shape`.", "question_id": 355},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 356},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 357},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 358},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 359},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 360},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 361},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 362},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 363},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 364},
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 365},
{"snippet": "torch.sgn(input)", "intent": "This function is an extension of torch.sign ( ) to complex tensors . It computes a new tensor whose elements have the same angles as the corresponding elements of `input` and absolute values ( i.e .", "question_id": 366},
{"snippet": "torch.sgn(input, out=None)", "intent": "This function is an extension of torch.sign ( ) to complex tensors . It computes a new tensor whose elements have the same angles as the corresponding elements of `input` and absolute values ( i.e . With arguments `out`.", "question_id": 367},
{"snippet": "Tensor.lu()", "intent": "See torch.lu ( )", "question_id": 368},
{"snippet": "Tensor.lu(pivot=True)", "intent": "See torch.lu ( ) With arguments `pivot`.", "question_id": 369},
{"snippet": "Tensor.lu(get_infos=False)", "intent": "See torch.lu ( ) With arguments `get_infos`.", "question_id": 370},
{"snippet": "Tensor.lu(pivot=True, get_infos=False)", "intent": "See torch.lu ( ) With arguments `pivot`, `get_infos`.", "question_id": 371},
{"snippet": "torch.nn.quantized.Linear(in_features, out_features)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`.", "question_id": 372},
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, bias_=True)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`.", "question_id": 373},
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, dtype=torch.qint8)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 374},
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`, `dtype`.", "question_id": 375},
{"snippet": "linear.from_float(mod)", "intent": "Create a quantized module from a float module or qparams_dict With arguments `mod`.", "question_id": 376},
{"snippet": "torch.sparse.softmax(input, dim)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`.", "question_id": 377},
{"snippet": "torch.sparse.softmax(input, dim, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `dtype`.", "question_id": 378},
{"snippet": "torch.tensor(data)", "intent": "Constructs a tensor with `data` .", "question_id": 379},
{"snippet": "torch.tensor(data, dtype=None)", "intent": "Constructs a tensor with `data` . With arguments `dtype`.", "question_id": 380},
{"snippet": "torch.tensor(data, device=None)", "intent": "Constructs a tensor with `data` . With arguments `device`.", "question_id": 381},
{"snippet": "torch.tensor(data, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `requires_grad`.", "question_id": 382},
{"snippet": "torch.tensor(data, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `pin_memory`.", "question_id": 383},
{"snippet": "torch.tensor(data, dtype=None, device=None)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `device`.", "question_id": 384},
{"snippet": "torch.tensor(data, dtype=None, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `requires_grad`.", "question_id": 385},
{"snippet": "torch.tensor(data, dtype=None, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `pin_memory`.", "question_id": 386},
{"snippet": "torch.tensor(data, device=None, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `device`, `requires_grad`.", "question_id": 387},
{"snippet": "torch.tensor(data, device=None, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `device`, `pin_memory`.", "question_id": 388},
{"snippet": "torch.real(input)", "intent": "Returns a new tensor containing real values of the self tensor . With arguments `input`.", "question_id": 389},
{"snippet": "Tensor.bincount()", "intent": "See torch.bincount ( )", "question_id": 390},
{"snippet": "Tensor.bincount(weights=None)", "intent": "See torch.bincount ( ) With arguments `weights`.", "question_id": 391},
{"snippet": "Tensor.bincount(minlength=0)", "intent": "See torch.bincount ( ) With arguments `minlength`.", "question_id": 392},
{"snippet": "Tensor.bincount(weights=None, minlength=0)", "intent": "See torch.bincount ( ) With arguments `weights`, `minlength`.", "question_id": 393},
{"snippet": "torch.linalg.multi_dot(tensors)", "intent": "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed . Every tensor in `tensors` must be 2D , except for the first and last which may be 1D .", "question_id": 394},
{"snippet": "torch.linalg.multi_dot(tensors, out=None)", "intent": "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed . Every tensor in `tensors` must be 2D , except for the first and last which may be 1D . With arguments `out`.", "question_id": 395},
{"snippet": "Tensor.renorm(p, dim, maxnorm)", "intent": "See torch.renorm ( ) With arguments `p`, `dim`, `maxnorm`.", "question_id": 396},
{"snippet": "Tensor.sqrt_()", "intent": "In-place version of sqrt ( )", "question_id": 397},
{"snippet": "Tensor.bitwise_and_()", "intent": "In-place version of bitwise_and ( )", "question_id": 398},
{"snippet": "Tensor.lstsq(A)", "intent": "See torch.lstsq ( ) With arguments `A`.", "question_id": 399},
{"snippet": "torch.nn.MarginRankingLoss()", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) .", "question_id": 400},
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`.", "question_id": 401},
{"snippet": "torch.nn.MarginRankingLoss(size_average=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 402},
{"snippet": "torch.nn.MarginRankingLoss(reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 403},
{"snippet": "torch.nn.MarginRankingLoss(reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 404},
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, size_average=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `size_average`.", "question_id": 405},
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduce`.", "question_id": 406},
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduction`.", "question_id": 407},
{"snippet": "torch.nn.MarginRankingLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 408},
{"snippet": "torch.nn.MarginRankingLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 409},
{"snippet": "torch.lu_solve(b, LU_data, LU_pivots)", "intent": "Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu ( ) . With arguments `b`, `LU_data`, `LU_pivots`.", "question_id": 410},
{"snippet": "torch.lu_solve(b, LU_data, LU_pivots, out=None)", "intent": "Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu ( ) . With arguments `b`, `LU_data`, `LU_pivots`, `out`.", "question_id": 411},
{"snippet": "torch.cuda.memory_cached()", "intent": "Deprecated ; see memory_reserved ( ) .", "question_id": 412},
{"snippet": "torch.cuda.memory_cached(device=None)", "intent": "Deprecated ; see memory_reserved ( ) . With arguments `device`.", "question_id": 413},
{"snippet": "torch.nn.RNN(*args, **kwargs)", "intent": "Applies a multi-layer Elman RNN with tanh\u2061\\tanhtanh or ReLU\\text { ReLU } ReLU non-linearity to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 414},
{"snippet": "Tensor.fill_(value)", "intent": "Fills self tensor with the specified `value` .", "question_id": 415},
{"snippet": "torch.asin(input)", "intent": "Returns a new tensor with the arcsine of the elements of `input` .", "question_id": 416},
{"snippet": "torch.asin(input, out=None)", "intent": "Returns a new tensor with the arcsine of the elements of `input` . With arguments `out`.", "question_id": 417},
{"snippet": "Tensor.floor()", "intent": "See torch.floor ( )", "question_id": 418},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 419},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 420},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 421},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 422},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 423},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 424},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 425},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 426},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 427},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 428},
{"snippet": "torch.is_storage(obj)", "intent": "Returns True if `obj` is a PyTorch storage object .", "question_id": 429},
{"snippet": "torch.fft.hfft(input)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal .", "question_id": 430},
{"snippet": "torch.fft.hfft(input, n=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`.", "question_id": 431},
{"snippet": "torch.fft.hfft(input, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`.", "question_id": 432},
{"snippet": "torch.fft.hfft(input, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `norm`.", "question_id": 433},
{"snippet": "torch.fft.hfft(input, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `out`.", "question_id": 434},
{"snippet": "torch.fft.hfft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `dim`.", "question_id": 435},
{"snippet": "torch.fft.hfft(input, n=None, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `norm`.", "question_id": 436},
{"snippet": "torch.fft.hfft(input, n=None, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `out`.", "question_id": 437},
{"snippet": "torch.fft.hfft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`, `norm`.", "question_id": 438},
{"snippet": "torch.fft.hfft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`, `out`.", "question_id": 439},
{"snippet": "Tensor.reshape(*shape)", "intent": "Returns a tensor with the same data and number of elements as self but with the specified shape . With arguments `*shape`.", "question_id": 440},
{"snippet": "torch.subtract(input, other)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`.", "question_id": 441},
{"snippet": "torch.subtract(input, other, alpha=1)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `alpha`.", "question_id": 442},
{"snippet": "torch.subtract(input, other, out=None)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `out`.", "question_id": 443},
{"snippet": "torch.subtract(input, other, alpha=1, out=None)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `alpha`, `out`.", "question_id": 444},
{"snippet": "Tensor.zero_()", "intent": "Fills self tensor with zeros .", "question_id": 445},
{"snippet": "Tensor.size()", "intent": "Returns the size of the self tensor .", "question_id": 446},
{"snippet": "torch.cosh(input)", "intent": "Returns a new tensor with the hyperbolic cosine of the elements of `input` .", "question_id": 447},
{"snippet": "torch.cosh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic cosine of the elements of `input` . With arguments `out`.", "question_id": 448},
{"snippet": "Tensor.squeeze_()", "intent": "In-place version of squeeze ( )", "question_id": 449},
{"snippet": "Tensor.squeeze_(dim=None)", "intent": "In-place version of squeeze ( ) With arguments `dim`.", "question_id": 450},
{"snippet": "torch.nn.functional.dropout(input)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 451},
{"snippet": "torch.nn.functional.dropout(input, p=0.5)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 452},
{"snippet": "torch.nn.functional.dropout(input, training=True)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 453},
{"snippet": "torch.nn.functional.dropout(input, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 454},
{"snippet": "torch.nn.functional.dropout(input, p=0.5, training=True)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 455},
{"snippet": "torch.nn.functional.dropout(input, p=0.5, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 456},
{"snippet": "torch.nn.functional.dropout(input, training=True, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 457},
{"snippet": "torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 458},
{"snippet": "Tensor.bitwise_or()", "intent": "See torch.bitwise_or ( )", "question_id": 459},
{"snippet": "torch.nn.Sigmoid", "intent": "Applies the element-wise function:", "question_id": 460},
{"snippet": "torch.matmul(input, other)", "intent": "Matrix product of two tensors . With arguments `input`, `other`.", "question_id": 461},
{"snippet": "torch.matmul(input, other, out=None)", "intent": "Matrix product of two tensors . With arguments `input`, `other`, `out`.", "question_id": 462},
{"snippet": "Tensor.logsumexp(dim)", "intent": "See torch.logsumexp ( ) With arguments `dim`.", "question_id": 463},
{"snippet": "Tensor.logsumexp(dim, keepdim=False)", "intent": "See torch.logsumexp ( ) With arguments `dim`, `keepdim`.", "question_id": 464},
{"snippet": "Tensor.expand(*sizes)", "intent": "Returns a new view of the self tensor with singleton dimensions expanded to a larger size . With arguments `*sizes`.", "question_id": 465},
{"snippet": "Tensor.unsqueeze(dim)", "intent": "See torch.unsqueeze ( ) With arguments `dim`.", "question_id": 466},
{"snippet": "torch.unbind(input)", "intent": "Removes a tensor dimension . With arguments `input`.", "question_id": 467},
{"snippet": "torch.unbind(input, dim=0)", "intent": "Removes a tensor dimension . With arguments `input`, `dim`.", "question_id": 468},
{"snippet": "Tensor.outer(vec2)", "intent": "See torch.outer ( ) . With arguments `vec2`.", "question_id": 469},
{"snippet": "torch.addcdiv(input, tensor1, tensor2)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 470},
{"snippet": "torch.addcdiv(input, tensor1, tensor2, value=1)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 471},
{"snippet": "torch.addcdiv(input, tensor1, tensor2, out=None)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 472},
{"snippet": "torch.addcdiv(input, tensor1, tensor2, value=1, out=None)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 473},
{"snippet": "torch.var_mean(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`.", "question_id": 474},
{"snippet": "torch.var_mean(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 475},
{"snippet": "torch.var_mean(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 476},
{"snippet": "torch.var_mean(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 477},
{"snippet": "Tensor.solve(A)", "intent": "See torch.solve ( ) With arguments `A`.", "question_id": 478},
{"snippet": "torch.quantization.observer.MinMaxObserver()", "intent": "Observer module for computing the quantization parameters based on the running min and max values .", "question_id": 479},
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`.", "question_id": 480},
{"snippet": "torch.quantization.observer.MinMaxObserver(qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `qscheme`.", "question_id": 481},
{"snippet": "torch.quantization.observer.MinMaxObserver(reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `reduce_range`.", "question_id": 482},
{"snippet": "torch.quantization.observer.MinMaxObserver(quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `quant_min`.", "question_id": 483},
{"snippet": "torch.quantization.observer.MinMaxObserver(quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `quant_max`.", "question_id": 484},
{"snippet": "torch.quantization.observer.MinMaxObserver(factory_kwargs=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `factory_kwargs`.", "question_id": 485},
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `qscheme`.", "question_id": 486},
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `reduce_range`.", "question_id": 487},
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `quant_min`.", "question_id": 488},
{"snippet": "min_max_observer.calculate_qparams()", "intent": "Calculates the quantization parameters .", "question_id": 489},
{"snippet": "min_max_observer.forward(x_orig)", "intent": "Records the running minimum and maximum of x . With arguments `x_orig`.", "question_id": 490},
{"snippet": "torch.renorm(input, p, dim, maxnorm)", "intent": "Returns a tensor where each sub-tensor of `input` along dimension `dim` is normalized such that the p-norm of the sub-tensor is lower than the value `maxnorm` With arguments `p`.", "question_id": 491},
{"snippet": "torch.renorm(input, p, dim, maxnorm, out=None)", "intent": "Returns a tensor where each sub-tensor of `input` along dimension `dim` is normalized such that the p-norm of the sub-tensor is lower than the value `maxnorm` With arguments `p`, `out`.", "question_id": 492},
{"snippet": "torch.zeros_like(input)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` .", "question_id": 493},
{"snippet": "torch.zeros_like(input, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`.", "question_id": 494},
{"snippet": "torch.zeros_like(input, layout=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `layout`.", "question_id": 495},
{"snippet": "torch.zeros_like(input, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `device`.", "question_id": 496},
{"snippet": "torch.zeros_like(input, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `requires_grad`.", "question_id": 497},
{"snippet": "torch.zeros_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `memory_format`.", "question_id": 498},
{"snippet": "torch.zeros_like(input, dtype=None, layout=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 499},
{"snippet": "torch.zeros_like(input, dtype=None, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 500},
{"snippet": "torch.zeros_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 501},
{"snippet": "torch.zeros_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 502},
{"snippet": "torch.quantization.quantize_fx.fuse_fx(model)", "intent": "Fuse modules like conv+bn , conv+bn+relu etc , `model` must be in eval mode .", "question_id": 503},
{"snippet": "torch.quantization.quantize_fx.fuse_fx(model, fuse_custom_config_dict=None)", "intent": "Fuse modules like conv+bn , conv+bn+relu etc , `model` must be in eval mode . Fusion rules are defined in torch.quantization.fx.fusion_pattern.py : param model : a torch.nn.Module model : param `fuse_custom_config_dict` : Dictionary for custom configurations for fuse_fx , e.g .", "question_id": 504},
{"snippet": "Tensor.addcmul_(tensor1, tensor2)", "intent": "In-place version of addcmul ( ) With arguments `tensor1`, `tensor2`.", "question_id": 505},
{"snippet": "Tensor.addcmul_(tensor1, tensor2, value=1)", "intent": "In-place version of addcmul ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 506},
{"snippet": "torch.unique_consecutive(*args, **kwargs)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `*args`, `**kwargs`.", "question_id": 507},
{"snippet": "torch.set_num_threads(int)", "intent": "Sets the number of threads used for intraop parallelism on CPU . With arguments `int`.", "question_id": 508},
{"snippet": "torch.fmax(input, other)", "intent": "Computes the element-wise maximum of `input` and `other` .", "question_id": 509},
{"snippet": "torch.fmax(input, other, out=None)", "intent": "Computes the element-wise maximum of `input` and `other` . With arguments `out`.", "question_id": 510},
{"snippet": "torch.nn.modules.module.register_module_full_backward_hook(hook)", "intent": "Registers a backward `hook` common to all the modules .", "question_id": 511},
{"snippet": "Tensor.pin_memory()", "intent": "Copies the tensor to pinned memory , if it \u2019 s not already pinned .", "question_id": 512},
{"snippet": "Tensor.logaddexp(other)", "intent": "See torch.logaddexp ( ) With arguments `other`.", "question_id": 513},
{"snippet": "Tensor.clone()", "intent": "See torch.clone ( )", "question_id": 514},
{"snippet": "Tensor.clone(memory_format=torch.preserve_format)", "intent": "See torch.clone ( ) With arguments `memory_format`.", "question_id": 515},
{"snippet": "torch.quantization.quantize_fx.prepare_fx(model, qconfig_dict)", "intent": "Prepare a `model` for post training static quantization `qconfig_dict` = { \u201c \u201d : qconfig } prepared_model = prepare_fx ( float_model , qconfig_dict ) # Run calibration calibrate ( prepared_model , sample_inference_data ) `` `", "question_id": 516},
{"snippet": "torch.quantization.quantize_fx.prepare_fx(model, qconfig_dict, prepare_custom_config_dict=None)", "intent": "Prepare a `model` for post training static quantization `qconfig_dict` = { \u201c \u201d : qconfig } prepared_model = prepare_fx ( float_model , qconfig_dict ) # Run calibration calibrate ( prepared_model , sample_inference_data ) `` ` : param : : param # priority : global , object_type , module_name_regex , module_name : type # priority : in increasing order : param # qconfig == None means fusion and quantization should be skipped for anything : : param # matching the rule : : param } : : param `prepare_custom_config_dict` : customization configuration dictionary for : param quantization tool : : param prepare_custom_config_dict = { : # optional : specify the path for standalone modules", "question_id": 517},
{"snippet": "Tensor.mvlgamma(p)", "intent": "See torch.mvlgamma ( ) With arguments `p`.", "question_id": 518},
{"snippet": "torch.atan2(input, other)", "intent": "Element-wise arctangent of inputi/otheri\\text { `input` } _ { i } / \\text { `other` } _ { i } inputi\u200b/otheri\u200b with consideration of the quadrant .", "question_id": 519},
{"snippet": "torch.atan2(input, other, out=None)", "intent": "Element-wise arctangent of inputi/otheri\\text { `input` } _ { i } / \\text { `other` } _ { i } inputi\u200b/otheri\u200b with consideration of the quadrant . With arguments `out`.", "question_id": 520},
{"snippet": "torch.count_nonzero(input)", "intent": "Counts the number of non-zero values in the tensor `input` along the given `dim` .", "question_id": 521},
{"snippet": "torch.count_nonzero(input, dim=None)", "intent": "Counts the number of non-zero values in the tensor `input` along the given `dim` .", "question_id": 522},
{"snippet": "Tensor.nelement()", "intent": "Alias for numel ( )", "question_id": 523},
{"snippet": "torch.cuda.memory_allocated()", "intent": "Returns the current GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 524},
{"snippet": "torch.cuda.memory_allocated(device=None)", "intent": "Returns the current GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 525},
{"snippet": "torch.gather(input, dim, index)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions .", "question_id": 526},
{"snippet": "torch.gather(input, dim, index, sparse_grad=False)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . With arguments `sparse_grad`.", "question_id": 527},
{"snippet": "torch.gather(input, dim, index, out=None)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . `out` will have the same shape as index .", "question_id": 528},
{"snippet": "torch.gather(input, dim, index, sparse_grad=False, out=None)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . `out` will have the same shape as index . With arguments `sparse_grad`.", "question_id": 529},
{"snippet": "Tensor.diag()", "intent": "See torch.diag ( )", "question_id": 530},
{"snippet": "Tensor.diag(diagonal=0)", "intent": "See torch.diag ( ) With arguments `diagonal`.", "question_id": 531},
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` .", "question_id": 532},
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, dim=1)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` .", "question_id": 533},
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, eps=1e-8)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` . With arguments `eps`.", "question_id": 534},
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` . With arguments `eps`.", "question_id": 535},
{"snippet": "torch.nn.Tanh", "intent": "Applies the element-wise function:", "question_id": 536},
{"snippet": "Tensor.gcd(other)", "intent": "See torch.gcd ( ) With arguments `other`.", "question_id": 537},
{"snippet": "torch.quantization.QuantStub()", "intent": "Quantize stub module , before calibration , this is same as an observer , it will be swapped as nnq.Quantize in convert .", "question_id": 538},
{"snippet": "torch.quantization.QuantStub(qconfig=None)", "intent": "Quantize stub module , before calibration , this is same as an observer , it will be swapped as nnq.Quantize in convert . With arguments `qconfig`.", "question_id": 539},
{"snippet": "torch.addbmm(input, batch1, batch2)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result .", "question_id": 540},
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated .", "question_id": 541},
{"snippet": "torch.addbmm(input, batch1, batch2, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers .", "question_id": 542},
{"snippet": "torch.addbmm(input, batch1, batch2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 543},
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers .", "question_id": 544},
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 545},
{"snippet": "torch.addbmm(input, batch1, batch2, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 546},
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 547},
{"snippet": "torch.column_stack(tensors)", "intent": "Creates a new tensor by horizontally stacking the `tensors` in tensors .", "question_id": 548},
{"snippet": "torch.column_stack(tensors, out=None)", "intent": "Creates a new tensor by horizontally stacking the `tensors` in tensors . With arguments `out`.", "question_id": 549},
{"snippet": "torch.tensor_split(input, indices_or_sections)", "intent": "Splits a tensor into multiple sub-tensors , all of which are views of `input` , along dimension `dim` according to the indices or number of sections specified by `indices_or_sections` .", "question_id": 550},
{"snippet": "torch.tensor_split(input, indices_or_sections, dim=0)", "intent": "Splits a tensor into multiple sub-tensors , all of which are views of `input` , along dimension `dim` according to the indices or number of sections specified by `indices_or_sections` .", "question_id": 551},
{"snippet": "torch.nn.functional.relu6(input)", "intent": "Applies the element-wise function ReLU6 ( x ) =min\u2061 ( max\u2061 ( 0 , x ) ,6 ) \\text { ReLU6 } ( x ) = \\min ( \\max ( 0 , x ) , 6 ) ReLU6 ( x ) =min ( max ( 0 , x ) ,6 ) . With arguments `input`.", "question_id": 552},
{"snippet": "torch.nn.functional.relu6(input, inplace=False)", "intent": "Applies the element-wise function ReLU6 ( x ) =min\u2061 ( max\u2061 ( 0 , x ) ,6 ) \\text { ReLU6 } ( x ) = \\min ( \\max ( 0 , x ) , 6 ) ReLU6 ( x ) =min ( max ( 0 , x ) ,6 ) . With arguments `input`, `inplace`.", "question_id": 553},
{"snippet": "Tensor.sgn_()", "intent": "In-place version of sgn ( )", "question_id": 554},
{"snippet": "Tensor.pinverse()", "intent": "See torch.pinverse ( )", "question_id": 555},
{"snippet": "torch.jit.load(f)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`.", "question_id": 556},
{"snippet": "torch.jit.load(f, map_location=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `map_location`.", "question_id": 557},
{"snippet": "torch.jit.load(f, _extra_files=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `_extra_files`.", "question_id": 558},
{"snippet": "torch.jit.load(f, map_location=None, _extra_files=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `map_location`, `_extra_files`.", "question_id": 559},
{"snippet": "Function.forward(ctx, *args, **kwargs)", "intent": "Performs the operation . It must accept a context `ctx` as the first argument , followed by any number of arguments ( tensors or other types ) . With arguments `*args`, `**kwargs`.", "question_id": 560},
{"snippet": "torch.cartesian_prod(*tensors)", "intent": "Do cartesian product of the given sequence of tensors . With arguments `*tensors`.", "question_id": 561},
{"snippet": "Tensor.chunk(chunks)", "intent": "See torch.chunk ( ) With arguments `chunks`.", "question_id": 562},
{"snippet": "Tensor.chunk(chunks, dim=0)", "intent": "See torch.chunk ( ) With arguments `chunks`, `dim`.", "question_id": 563},
{"snippet": "Tensor.dense_dim()", "intent": "Return the number of dense dimensions in a sparse tensor self .", "question_id": 564},
{"snippet": "Tensor.logical_xor()", "intent": "See torch.logical_xor ( )", "question_id": 565},
{"snippet": "torch.cuda.memory_stats()", "intent": "Returns a dictionary of CUDA memory allocator statistics for a given `device` .", "question_id": 566},
{"snippet": "torch.cuda.memory_stats(device=None)", "intent": "Returns a dictionary of CUDA memory allocator statistics for a given `device` .", "question_id": 567},
{"snippet": "torch.randint_like(input, high, \\*)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`.", "question_id": 568},
{"snippet": "torch.randint_like(input, high, \\*, low=0)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`.", "question_id": 569},
{"snippet": "torch.randint_like(input, high, \\*, dtype=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `dtype`.", "question_id": 570},
{"snippet": "torch.randint_like(input, high, \\*, layout=torch.strided)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `layout`.", "question_id": 571},
{"snippet": "torch.randint_like(input, high, \\*, device=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `device`.", "question_id": 572},
{"snippet": "torch.randint_like(input, high, \\*, requires_grad=False)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `requires_grad`.", "question_id": 573},
{"snippet": "torch.randint_like(input, high, \\*, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `memory_format`.", "question_id": 574},
{"snippet": "torch.randint_like(input, high, \\*, low=0, dtype=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `dtype`.", "question_id": 575},
{"snippet": "torch.randint_like(input, high, \\*, low=0, layout=torch.strided)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `layout`.", "question_id": 576},
{"snippet": "torch.randint_like(input, high, \\*, low=0, device=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `device`.", "question_id": 577},
{"snippet": "torch.nn.PReLU()", "intent": "Applies the element-wise function :", "question_id": 578},
{"snippet": "torch.nn.PReLU(num_parameters=1)", "intent": "Applies the element-wise function : With arguments `num_parameters`.", "question_id": 579},
{"snippet": "torch.nn.PReLU(init=0.25)", "intent": "Applies the element-wise function : With arguments `init`.", "question_id": 580},
{"snippet": "torch.nn.PReLU(device=None)", "intent": "Applies the element-wise function : With arguments `device`.", "question_id": 581},
{"snippet": "torch.nn.PReLU(dtype=None)", "intent": "Applies the element-wise function : With arguments `dtype`.", "question_id": 582},
{"snippet": "torch.nn.PReLU(num_parameters=1, init=0.25)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `init`.", "question_id": 583},
{"snippet": "torch.nn.PReLU(num_parameters=1, device=None)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `device`.", "question_id": 584},
{"snippet": "torch.nn.PReLU(num_parameters=1, dtype=None)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `dtype`.", "question_id": 585},
{"snippet": "torch.nn.PReLU(init=0.25, device=None)", "intent": "Applies the element-wise function : With arguments `init`, `device`.", "question_id": 586},
{"snippet": "torch.nn.PReLU(init=0.25, dtype=None)", "intent": "Applies the element-wise function : With arguments `init`, `dtype`.", "question_id": 587},
{"snippet": "Tensor.q_zero_point()", "intent": "Given a Tensor quantized by linear ( affine ) quantization , returns the zero_point of the underlying quantizer ( ) .", "question_id": 588},
{"snippet": "torch.nn.functional.elu_(input)", "intent": "In-place version of elu ( ) . With arguments `input`.", "question_id": 589},
{"snippet": "torch.nn.functional.elu_(input, alpha=1.)", "intent": "In-place version of elu ( ) . With arguments `input`, `alpha`.", "question_id": 590},
{"snippet": "Tensor.random_()", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 591},
{"snippet": "Tensor.random_(from=0)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 592},
{"snippet": "Tensor.random_(to=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 593},
{"snippet": "Tensor.random_(generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 594},
{"snippet": "Tensor.random_(from=0, to=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 595},
{"snippet": "Tensor.random_(from=0, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 596},
{"snippet": "Tensor.random_(to=None, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 597},
{"snippet": "Tensor.random_(from=0, to=None, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 598},
{"snippet": "Tensor.short()", "intent": "self.short ( ) is equivalent to self.to ( torch.int16 ) .", "question_id": 599},
{"snippet": "Tensor.short(memory_format=torch.preserve_format)", "intent": "self.short ( ) is equivalent to self.to ( torch.int16 ) . With arguments `memory_format`.", "question_id": 600},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss()", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) .", "question_id": 601},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`.", "question_id": 602},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`.", "question_id": 603},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `reduce`.", "question_id": 604},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `reduction`.", "question_id": 605},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `size_average`.", "question_id": 606},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `reduce`.", "question_id": 607},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `reduction`.", "question_id": 608},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`, `reduce`.", "question_id": 609},
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`, `reduction`.", "question_id": 610},
{"snippet": "torch.nn.intrinsic.ConvReLU2d(conv, relu)", "intent": "This is a sequential container which calls the Conv2d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 611},
{"snippet": "torch.block_diag(*tensors)", "intent": "Create a block diagonal matrix from provided tensors . With arguments `*tensors`.", "question_id": 612},
{"snippet": "Tensor.float_power(exponent)", "intent": "See torch.float_power ( ) With arguments `exponent`.", "question_id": 613},
{"snippet": "Tensor.isposinf()", "intent": "See torch.isposinf ( )", "question_id": 614},
{"snippet": "torch.nn.InstanceNorm1d(num_features)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 615},
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 616},
{"snippet": "torch.nn.InstanceNorm1d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 617},
{"snippet": "torch.nn.InstanceNorm1d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 618},
{"snippet": "torch.nn.InstanceNorm1d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 619},
{"snippet": "torch.nn.InstanceNorm1d(num_features, device=None)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 620},
{"snippet": "torch.nn.InstanceNorm1d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 621},
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 622},
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 623},
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 624},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`.", "question_id": 625},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`.", "question_id": 626},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `dtype`.", "question_id": 627},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `qscheme`.", "question_id": 628},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `reduce_range`.", "question_id": 629},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `quant_min`.", "question_id": 630},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `quant_max`.", "question_id": 631},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `dtype`.", "question_id": 632},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `qscheme`.", "question_id": 633},
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `reduce_range`.", "question_id": 634},
{"snippet": "torch.vander(x)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 .", "question_id": 635},
{"snippet": "torch.vander(x, N=None)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . With arguments `N`.", "question_id": 636},
{"snippet": "torch.vander(x, increasing=False)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . If `increasing` is True , the order of the columns is reversed x0 , x1 , ... , x ( N\u22121 ) x^0 , x^1 , ... , x^ { ( N-1 ) } x0 , x1 , ... , x ( N\u22121 ) .", "question_id": 637},
{"snippet": "torch.vander(x, N=None, increasing=False)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . If `increasing` is True , the order of the columns is reversed x0 , x1 , ... , x ( N\u22121 ) x^0 , x^1 , ... , x^ { ( N-1 ) } x0 , x1 , ... , x ( N\u22121 ) . With arguments `N`.", "question_id": 638},
{"snippet": "Tensor.baddbmm_(batch1, batch2)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 639},
{"snippet": "Tensor.baddbmm_(batch1, batch2, beta=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 640},
{"snippet": "Tensor.baddbmm_(batch1, batch2, alpha=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 641},
{"snippet": "Tensor.baddbmm_(batch1, batch2, beta=1, alpha=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 642},
{"snippet": "torch.nn.functional.bilinear(input1, input2, weight)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `input1`, `input2`, `weight`.", "question_id": 643},
{"snippet": "torch.nn.functional.bilinear(input1, input2, weight, bias=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `input1`, `input2`, `weight`, `bias`.", "question_id": 644},
{"snippet": "Tensor.arcsin_()", "intent": "In-place version of arcsin ( )", "question_id": 645},
{"snippet": "Tensor.log1p_()", "intent": "In-place version of log1p ( )", "question_id": 646},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 647},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 648},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 649},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 650},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 651},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 652},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 653},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 654},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 655},
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 656},
{"snippet": "torch.cuda.device(device)", "intent": "Context-manager that changes the selected `device` .", "question_id": 657},
{"snippet": "Tensor.erfinv_()", "intent": "In-place version of erfinv ( )", "question_id": 658},
{"snippet": "torch.cuda.current_stream()", "intent": "Returns the currently selected Stream for a given `device` .", "question_id": 659},
{"snippet": "torch.cuda.current_stream(device=None)", "intent": "Returns the currently selected Stream for a given `device` .", "question_id": 660},
{"snippet": "Tensor.i0()", "intent": "See torch.i0 ( )", "question_id": 661},
{"snippet": "torch.nn.intrinsic.ConvReLU1d(conv, relu)", "intent": "This is a sequential container which calls the Conv1d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 662},
{"snippet": "Tensor.isreal()", "intent": "See torch.isreal ( )", "question_id": 663},
{"snippet": "Tensor.register_hook(hook)", "intent": "Registers a backward `hook` .", "question_id": 664},
{"snippet": "torch.tan(input)", "intent": "Returns a new tensor with the tangent of the elements of `input` .", "question_id": 665},
{"snippet": "torch.tan(input, out=None)", "intent": "Returns a new tensor with the tangent of the elements of `input` . With arguments `out`.", "question_id": 666},
{"snippet": "Tensor.remainder(divisor)", "intent": "See torch.remainder ( ) With arguments `divisor`.", "question_id": 667},
{"snippet": "torch.nn.ReplicationPad1d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 668},
{"snippet": "torch.cuda.reset_max_memory_cached()", "intent": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given `device` .", "question_id": 669},
{"snippet": "torch.cuda.reset_max_memory_cached(device=None)", "intent": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given `device` .", "question_id": 670},
{"snippet": "torch.qr(input)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices .", "question_id": 671},
{"snippet": "torch.qr(input, some=True)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . If `some` is True , then this function returns the thin ( reduced ) QR factorization .", "question_id": 672},
{"snippet": "torch.qr(input, out=None)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . With arguments `out`.", "question_id": 673},
{"snippet": "torch.qr(input, some=True, out=None)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . If `some` is True , then this function returns the thin ( reduced ) QR factorization . With arguments `out`.", "question_id": 674},
{"snippet": "Tensor.dsplit(split_size_or_sections)", "intent": "See torch.dsplit ( ) With arguments `split_size_or_sections`.", "question_id": 675},
{"snippet": "torch.square(input)", "intent": "Returns a new tensor with the square of the elements of `input` .", "question_id": 676},
{"snippet": "torch.square(input, out=None)", "intent": "Returns a new tensor with the square of the elements of `input` . With arguments `out`.", "question_id": 677},
{"snippet": "Tensor.real", "intent": "Returns a new tensor containing real values of the self tensor.", "question_id": 678},
{"snippet": "torch.nn.functional.hardswish(input)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `input`.", "question_id": 679},
{"snippet": "torch.nn.functional.hardswish(input, inplace=False)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `input`, `inplace`.", "question_id": 680},
{"snippet": "Tensor.div_(value)", "intent": "In-place version of div ( ) With arguments `value`.", "question_id": 681},
{"snippet": "Tensor.div_(value, rounding_mode=None)", "intent": "In-place version of div ( ) With arguments `value`, `rounding_mode`.", "question_id": 682},
{"snippet": "Tensor.cumprod(dim)", "intent": "See torch.cumprod ( ) With arguments `dim`.", "question_id": 683},
{"snippet": "Tensor.cumprod(dim, dtype=None)", "intent": "See torch.cumprod ( ) With arguments `dim`, `dtype`.", "question_id": 684},
{"snippet": "torch.optim.Adamax(params, 0.999))", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`.", "question_id": 685},
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`.", "question_id": 686},
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`.", "question_id": 687},
{"snippet": "torch.optim.Adamax(params, 0.999), eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `eps`.", "question_id": 688},
{"snippet": "torch.optim.Adamax(params, 0.999), weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 689},
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, betas=(0.9)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 690},
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 691},
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 692},
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9, eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`, `eps`.", "question_id": 693},
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9, weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`, `weight_decay`.", "question_id": 694},
{"snippet": "adamax.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 695},
{"snippet": "adamax.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 696},
{"snippet": "adamax.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 697},
{"snippet": "adamax.step()", "intent": "Performs a single optimization step .", "question_id": 698},
{"snippet": "adamax.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 699},
{"snippet": "adamax.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 700},
{"snippet": "adamax.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 701},
{"snippet": "torch.lgamma(input)", "intent": "Computes the natural logarithm of the absolute value of the gamma function on `input` .", "question_id": 702},
{"snippet": "torch.lgamma(input, out=None)", "intent": "Computes the natural logarithm of the absolute value of the gamma function on `input` . With arguments `out`.", "question_id": 703},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`.", "question_id": 704},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`.", "question_id": 705},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, size_average=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `size_average`.", "question_id": 706},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, reduce=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `reduce`.", "question_id": 707},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, reduction='mean')", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `reduction`.", "question_id": 708},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, pos_weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `pos_weight`.", "question_id": 709},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `size_average`.", "question_id": 710},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, reduce=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `reduce`.", "question_id": 711},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, reduction='mean')", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `reduction`.", "question_id": 712},
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, pos_weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `pos_weight`.", "question_id": 713},
{"snippet": "Tensor.record_stream(stream)", "intent": "Ensures that the tensor memory is not reused for another tensor until all current work queued on `stream` are complete .", "question_id": 714},
{"snippet": "torch.multinomial(input, num_samples)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` .", "question_id": 715},
{"snippet": "torch.multinomial(input, num_samples, replacement=False)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement .", "question_id": 716},
{"snippet": "torch.multinomial(input, num_samples, generator=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . With arguments `generator`.", "question_id": 717},
{"snippet": "torch.multinomial(input, num_samples, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If input is a vector , `out` is a vector of size num_samples .", "question_id": 718},
{"snippet": "torch.multinomial(input, num_samples, replacement=False, generator=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . With arguments `generator`.", "question_id": 719},
{"snippet": "torch.multinomial(input, num_samples, replacement=False, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . If input is a vector , `out` is a vector of size num_samples .", "question_id": 720},
{"snippet": "torch.multinomial(input, num_samples, generator=None, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If input is a vector , `out` is a vector of size num_samples . With arguments `generator`.", "question_id": 721},
{"snippet": "torch.multinomial(input, num_samples, replacement=False, generator=None, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . If input is a vector , `out` is a vector of size num_samples . With arguments `generator`.", "question_id": 722},
{"snippet": "torch.nn.MaxUnpool2d(kernel_size)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`.", "question_id": 723},
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `stride`.", "question_id": 724},
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `padding`.", "question_id": 725},
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 726},
{"snippet": "Tensor.isnan()", "intent": "See torch.isnan ( )", "question_id": 727},
{"snippet": "Tensor.mul_(value)", "intent": "In-place version of mul ( ) . With arguments `value`.", "question_id": 728},
{"snippet": "Tensor.ravel(input)", "intent": "see torch.ravel ( ) With arguments `input`.", "question_id": 729},
{"snippet": "Tensor.arcsin()", "intent": "See torch.arcsin ( )", "question_id": 730},
{"snippet": "Tensor.gather(dim, index)", "intent": "See torch.gather ( ) With arguments `dim`, `index`.", "question_id": 731},
{"snippet": "Tensor.absolute()", "intent": "Alias for abs ( )", "question_id": 732},
{"snippet": "Tensor.nanmedian()", "intent": "See torch.nanmedian ( )", "question_id": 733},
{"snippet": "Tensor.nanmedian(dim=None)", "intent": "See torch.nanmedian ( ) With arguments `dim`.", "question_id": 734},
{"snippet": "Tensor.nanmedian(keepdim=False)", "intent": "See torch.nanmedian ( ) With arguments `keepdim`.", "question_id": 735},
{"snippet": "Tensor.nanmedian(dim=None, keepdim=False)", "intent": "See torch.nanmedian ( ) With arguments `dim`, `keepdim`.", "question_id": 736},
{"snippet": "Tensor.grad", "intent": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.", "question_id": 737},
{"snippet": "Tensor.min()", "intent": "See torch.min ( )", "question_id": 738},
{"snippet": "Tensor.min(dim=None)", "intent": "See torch.min ( ) With arguments `dim`.", "question_id": 739},
{"snippet": "Tensor.min(keepdim=False)", "intent": "See torch.min ( ) With arguments `keepdim`.", "question_id": 740},
{"snippet": "Tensor.min(dim=None, keepdim=False)", "intent": "See torch.min ( ) With arguments `dim`, `keepdim`.", "question_id": 741},
{"snippet": "torch.linalg.eig(A)", "intent": "Computes the eigenvalue decomposition of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 742},
{"snippet": "torch.linalg.eig(A, out=None)", "intent": "Computes the eigenvalue decomposition of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 743},
{"snippet": "torch.expm1(input)", "intent": "Alias for torch.special.expm1 ( ) . With arguments `input`.", "question_id": 744},
{"snippet": "torch.expm1(input, out=None)", "intent": "Alias for torch.special.expm1 ( ) . With arguments `input`, `out`.", "question_id": 745},
{"snippet": "Tensor.exp()", "intent": "See torch.exp ( )", "question_id": 746},
{"snippet": "torch.pca_lowrank(A)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`.", "question_id": 747},
{"snippet": "torch.pca_lowrank(A, q=None)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`.", "question_id": 748},
{"snippet": "torch.pca_lowrank(A, center=True)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `center`.", "question_id": 749},
{"snippet": "torch.pca_lowrank(A, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `niter`.", "question_id": 750},
{"snippet": "torch.pca_lowrank(A, q=None, center=True)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `center`.", "question_id": 751},
{"snippet": "torch.pca_lowrank(A, q=None, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `niter`.", "question_id": 752},
{"snippet": "torch.pca_lowrank(A, center=True, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `center`, `niter`.", "question_id": 753},
{"snippet": "torch.pca_lowrank(A, q=None, center=True, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `center`, `niter`.", "question_id": 754},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`.", "question_id": 755},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `mode`.", "question_id": 756},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1)", "intent": "Reduce learning rate when a metric has stopped improving . Models often benefit from reducing the learning rate by a `factor` of 2-10 once learning stagnates . With arguments `optimizer`.", "question_id": 757},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)", "intent": "Reduce learning rate when a metric has stopped improving . This scheduler reads a metrics quantity and if no improvement is seen for a \u2018 `patience` \u2019 number of epochs , the learning rate is reduced . With arguments `optimizer`.", "question_id": 758},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.0001)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `threshold`.", "question_id": 759},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold_mode='rel')", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `threshold_mode`.", "question_id": 760},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=0)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `cooldown`.", "question_id": 761},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=0)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `min_lr`.", "question_id": 762},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, eps=1e-08)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `eps`.", "question_id": 763},
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=False)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `verbose`.", "question_id": 764},
{"snippet": "Tensor.xlogy(other)", "intent": "See torch.xlogy ( ) With arguments `other`.", "question_id": 765},
{"snippet": "Tensor.sub(other)", "intent": "See torch.sub ( ) . With arguments `other`.", "question_id": 766},
{"snippet": "Tensor.sub(other, alpha=1)", "intent": "See torch.sub ( ) . With arguments `other`, `alpha`.", "question_id": 767},
{"snippet": "Tensor.add(other)", "intent": "Add a scalar or tensor to self tensor . If both `alpha` and `other` are specified , each element of other is scaled by alpha before being used .", "question_id": 768},
{"snippet": "Tensor.add(other, alpha=1)", "intent": "Add a scalar or tensor to self tensor . If both `alpha` and `other` are specified , each element of other is scaled by alpha before being used .", "question_id": 769},
{"snippet": "Tensor.requires_grad", "intent": "Is True if gradients need to be computed for this Tensor, False otherwise.", "question_id": 770},
{"snippet": "torch.nn.utils.prune.is_pruned(module)", "intent": "Check whether `module` is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod .", "question_id": 771},
{"snippet": "torch.diag(input)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal :", "question_id": 772},
{"snippet": "torch.diag(input, diagonal=0)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal :", "question_id": 773},
{"snippet": "torch.diag(input, out=None)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal : With arguments `out`.", "question_id": 774},
{"snippet": "torch.diag(input, diagonal=0, out=None)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal : With arguments `out`.", "question_id": 775},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 776},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 777},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 778},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 779},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 780},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 781},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dilation`.", "question_id": 782},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 783},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 784},
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 785},
{"snippet": "torch.nn.functional.adaptive_avg_pool2d(input, output_size)", "intent": "Applies a 2D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 786},
{"snippet": "Tensor.diff()", "intent": "See torch.diff ( )", "question_id": 787},
{"snippet": "Tensor.diff(n=1)", "intent": "See torch.diff ( ) With arguments `n`.", "question_id": 788},
{"snippet": "Tensor.diff(dim=- 1)", "intent": "See torch.diff ( ) With arguments `dim`.", "question_id": 789},
{"snippet": "Tensor.diff(prepend=None)", "intent": "See torch.diff ( ) With arguments `prepend`.", "question_id": 790},
{"snippet": "Tensor.diff(append=None)", "intent": "See torch.diff ( ) With arguments `append`.", "question_id": 791},
{"snippet": "Tensor.diff(n=1, dim=- 1)", "intent": "See torch.diff ( ) With arguments `n`, `dim`.", "question_id": 792},
{"snippet": "Tensor.diff(n=1, prepend=None)", "intent": "See torch.diff ( ) With arguments `n`, `prepend`.", "question_id": 793},
{"snippet": "Tensor.diff(n=1, append=None)", "intent": "See torch.diff ( ) With arguments `n`, `append`.", "question_id": 794},
{"snippet": "Tensor.diff(dim=- 1, prepend=None)", "intent": "See torch.diff ( ) With arguments `dim`, `prepend`.", "question_id": 795},
{"snippet": "Tensor.diff(dim=- 1, append=None)", "intent": "See torch.diff ( ) With arguments `dim`, `append`.", "question_id": 796},
{"snippet": "Tensor.t()", "intent": "See torch.t ( )", "question_id": 797},
{"snippet": "Tensor.orgqr(input2)", "intent": "See torch.orgqr ( ) With arguments `input2`.", "question_id": 798},
{"snippet": "torch.nn.ModuleDict()", "intent": "Holds submodules in a dictionary .", "question_id": 799},
{"snippet": "torch.nn.ModuleDict(modules=None)", "intent": "Holds submodules in a dictionary . ModuleDict can be indexed like a regular Python dictionary , but `modules` it contains are properly registered , and will be visible by all Module methods .", "question_id": 800},
{"snippet": "module_dict.clear()", "intent": "Remove all items from the ModuleDict .", "question_id": 801},
{"snippet": "module_dict.items()", "intent": "Return an iterable of the ModuleDict key/value pairs .", "question_id": 802},
{"snippet": "module_dict.keys()", "intent": "Return an iterable of the ModuleDict keys .", "question_id": 803},
{"snippet": "module_dict.pop(key)", "intent": "Remove `key` from the ModuleDict and return its module .", "question_id": 804},
{"snippet": "module_dict.update(modules)", "intent": "Update the ModuleDict with the key-value pairs from a mapping or an iterable , overwriting existing keys . With arguments `modules`.", "question_id": 805},
{"snippet": "module_dict.values()", "intent": "Return an iterable of the ModuleDict values .", "question_id": 806},
{"snippet": "torch.nn.ReflectionPad2d(padding)", "intent": "Pads the input tensor using the reflection of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 807},
{"snippet": "torch.result_type(tensor1, tensor2)", "intent": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors . With arguments `tensor1`, `tensor2`.", "question_id": 808},
{"snippet": "torch.sigmoid(input)", "intent": "Alias for torch.special.expit ( ) . With arguments `input`.", "question_id": 809},
{"snippet": "torch.sigmoid(input, out=None)", "intent": "Alias for torch.special.expit ( ) . With arguments `input`, `out`.", "question_id": 810},
{"snippet": "torch.nn.intrinsic.BNReLU3d(batch_norm, relu)", "intent": "This is a sequential container which calls the BatchNorm 3d and ReLU modules . With arguments `batch_norm`, `relu`.", "question_id": 811},
{"snippet": "Tensor.square()", "intent": "See torch.square ( )", "question_id": 812},
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`.", "question_id": 813},
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 814},
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, dtype=torch.qint8)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 815},
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 816},
{"snippet": "torch.topk(input, k)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension .", "question_id": 817},
{"snippet": "torch.topk(input, k, dim=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 818},
{"snippet": "torch.topk(input, k, largest=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension .", "question_id": 819},
{"snippet": "torch.topk(input, k, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 820},
{"snippet": "torch.topk(input, k, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . With arguments `out`.", "question_id": 821},
{"snippet": "torch.topk(input, k, dim=None, largest=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 822},
{"snippet": "torch.topk(input, k, dim=None, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 823},
{"snippet": "torch.topk(input, k, dim=None, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen . With arguments `out`.", "question_id": 824},
{"snippet": "torch.topk(input, k, largest=True, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 825},
{"snippet": "torch.topk(input, k, largest=True, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . With arguments `out`.", "question_id": 826},
{"snippet": "torch.nn.Hardsigmoid()", "intent": "Applies the element-wise function :", "question_id": 827},
{"snippet": "torch.nn.Hardsigmoid(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 828},
{"snippet": "torch.divide(input, other)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`.", "question_id": 829},
{"snippet": "torch.divide(input, other, rounding_mode=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `rounding_mode`.", "question_id": 830},
{"snippet": "torch.divide(input, other, out=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `out`.", "question_id": 831},
{"snippet": "torch.divide(input, other, rounding_mode=None, out=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `rounding_mode`, `out`.", "question_id": 832},
{"snippet": "Tensor.absolute_()", "intent": "In-place version of absolute ( ) Alias for abs_ ( )", "question_id": 833},
{"snippet": "torch.nn.L1Loss()", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy .", "question_id": 834},
{"snippet": "torch.nn.L1Loss(size_average=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `size_average`.", "question_id": 835},
{"snippet": "torch.nn.L1Loss(reduce=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `reduce`.", "question_id": 836},
{"snippet": "torch.nn.L1Loss(reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 837},
{"snippet": "torch.nn.L1Loss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `size_average`, `reduce`.", "question_id": 838},
{"snippet": "torch.nn.L1Loss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 839},
{"snippet": "torch.nn.L1Loss(reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 840},
{"snippet": "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`, `reduce`.", "question_id": 841},
{"snippet": "Tensor.round()", "intent": "See torch.round ( )", "question_id": 842},
{"snippet": "torch.nn.functional.gelu(input)", "intent": "Applies element-wise the function GELU ( x ) =x\u2217\u03a6 ( x ) \\text { GELU } ( x ) = x * \\Phi ( x ) GELU ( x ) =x\u2217\u03a6 ( x ) With arguments `input`.", "question_id": 843},
{"snippet": "torch.fft.ifft(input)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` .", "question_id": 844},
{"snippet": "torch.fft.ifft(input, n=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`.", "question_id": 845},
{"snippet": "torch.fft.ifft(input, dim=- 1)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 846},
{"snippet": "torch.fft.ifft(input, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 847},
{"snippet": "torch.fft.ifft(input, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `out`.", "question_id": 848},
{"snippet": "torch.fft.ifft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `dim`.", "question_id": 849},
{"snippet": "torch.fft.ifft(input, n=None, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `norm`.", "question_id": 850},
{"snippet": "torch.fft.ifft(input, n=None, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `out`.", "question_id": 851},
{"snippet": "torch.fft.ifft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 852},
{"snippet": "torch.fft.ifft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 853},
{"snippet": "torch.logaddexp2(input, other)", "intent": "Logarithm of the sum of exponentiations of the inputs in base-2 . With arguments `input`, `other`.", "question_id": 854},
{"snippet": "torch.logaddexp2(input, other, out=None)", "intent": "Logarithm of the sum of exponentiations of the inputs in base-2 . With arguments `input`, `other`, `out`.", "question_id": 855},
{"snippet": "torch.nn.functional.embedding(input, weight)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`.", "question_id": 856},
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`.", "question_id": 857},
{"snippet": "torch.nn.functional.embedding(input, weight, max_norm=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `max_norm`.", "question_id": 858},
{"snippet": "torch.nn.functional.embedding(input, weight, norm_type=2.0)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `norm_type`.", "question_id": 859},
{"snippet": "torch.nn.functional.embedding(input, weight, scale_grad_by_freq=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `scale_grad_by_freq`.", "question_id": 860},
{"snippet": "torch.nn.functional.embedding(input, weight, sparse=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `sparse`.", "question_id": 861},
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `max_norm`.", "question_id": 862},
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, norm_type=2.0)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `norm_type`.", "question_id": 863},
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, scale_grad_by_freq=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `scale_grad_by_freq`.", "question_id": 864},
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, sparse=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `sparse`.", "question_id": 865},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 866},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 867},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 868},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 869},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 870},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 871},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 872},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 873},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 874},
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 875},
{"snippet": "lazy_conv2d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv2d", "question_id": 876},
{"snippet": "Tensor.copy_(src)", "intent": "Copies the elements from `src` into self tensor and returns self .", "question_id": 877},
{"snippet": "Tensor.copy_(src, non_blocking=False)", "intent": "Copies the elements from `src` into self tensor and returns self . With arguments `non_blocking`.", "question_id": 878},
{"snippet": "torch.get_num_interop_threads()", "intent": "Returns the number of threads used for inter-op parallelism on CPU ( e.g .", "question_id": 879},
{"snippet": "Tensor.acosh_()", "intent": "In-place version of acosh ( )", "question_id": 880},
{"snippet": "Tensor.less()", "intent": "lt ( other ) - > Tensor", "question_id": 881},
{"snippet": "torch.linalg.inv_ex(A)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes .", "question_id": 882},
{"snippet": "torch.linalg.inv_ex(A, check_errors=False)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `check_errors`.", "question_id": 883},
{"snippet": "torch.linalg.inv_ex(A, out=None)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `out`.", "question_id": 884},
{"snippet": "torch.linalg.inv_ex(A, check_errors=False, out=None)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `check_errors`, `out`.", "question_id": 885},
{"snippet": "Tensor.is_sparse", "intent": "Is True if the Tensor uses sparse storage layout, False otherwise.", "question_id": 886},
{"snippet": "torch.nn.functional.kl_div(input, target)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`.", "question_id": 887},
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`.", "question_id": 888},
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`.", "question_id": 889},
{"snippet": "torch.nn.functional.kl_div(input, target, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduction`.", "question_id": 890},
{"snippet": "torch.nn.functional.kl_div(input, target, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `log_target`.", "question_id": 891},
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, reduce=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 892},
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 893},
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `log_target`.", "question_id": 894},
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 895},
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`, `log_target`.", "question_id": 896},
{"snippet": "torch.quantization.QuantWrapper(module)", "intent": "A wrapper class that wraps the input `module` , adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules .", "question_id": 897},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 898},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 899},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 900},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 901},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 902},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 903},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 904},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 905},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 906},
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 907},
{"snippet": "conv3d.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 908},
{"snippet": "torch.istft(input, n_fft)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 909},
{"snippet": "torch.istft(input, n_fft, hop_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 910},
{"snippet": "torch.istft(input, n_fft, win_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 911},
{"snippet": "torch.istft(input, n_fft, window=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . Important consideration in the parameters `window` and `center` so that the envelop created by the summation of all the windows is never zero at certain point in time . With arguments `input`.", "question_id": 912},
{"snippet": "torch.istft(input, n_fft, center=True)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . Important consideration in the parameters `window` and `center` so that the envelop created by the summation of all the windows is never zero at certain point in time . With arguments `input`.", "question_id": 913},
{"snippet": "torch.istft(input, n_fft, normalized=False)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `normalized`.", "question_id": 914},
{"snippet": "torch.istft(input, n_fft, onesided=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `onesided`.", "question_id": 915},
{"snippet": "torch.istft(input, n_fft, length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . It has the same parameters ( + additional optional parameter of `length` ) and it should return the least squares estimation of the original signal . With arguments `input`.", "question_id": 916},
{"snippet": "torch.istft(input, n_fft, return_complex=False)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `return_complex`.", "question_id": 917},
{"snippet": "torch.istft(input, n_fft, hop_length=None, win_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 918},
{"snippet": "torch.nn.SmoothL1Loss()", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise .", "question_id": 919},
{"snippet": "torch.nn.SmoothL1Loss(size_average=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`.", "question_id": 920},
{"snippet": "torch.nn.SmoothL1Loss(reduce=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `reduce`.", "question_id": 921},
{"snippet": "torch.nn.SmoothL1Loss(reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then :", "question_id": 922},
{"snippet": "torch.nn.SmoothL1Loss(beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise .", "question_id": 923},
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, reduce=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`, `reduce`.", "question_id": 924},
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then : With arguments `size_average`.", "question_id": 925},
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`.", "question_id": 926},
{"snippet": "torch.nn.SmoothL1Loss(reduce=None, reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then : With arguments `reduce`.", "question_id": 927},
{"snippet": "torch.nn.SmoothL1Loss(reduce=None, beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `reduce`.", "question_id": 928},
{"snippet": "torch.inference_mode()", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 929},
{"snippet": "torch.inference_mode(mode=True)", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 930},
{"snippet": "Tensor.masked_fill_(mask, value)", "intent": "Fills elements of self tensor with `value` where `mask` is True .", "question_id": 931},
{"snippet": "Tensor.arctanh()", "intent": "See torch.arctanh ( )", "question_id": 932},
{"snippet": "torch.arctanh(input)", "intent": "Alias for torch.atanh ( ) . With arguments `input`.", "question_id": 933},
{"snippet": "torch.arctanh(input, out=None)", "intent": "Alias for torch.atanh ( ) . With arguments `input`, `out`.", "question_id": 934},
{"snippet": "Tensor.is_pinned()", "intent": "Returns true if this tensor resides in pinned memory .", "question_id": 935},
{"snippet": "Tensor.bernoulli()", "intent": "Returns a result tensor where each result [ i ] \\texttt { result [ i ] } result [ i ] is independently sampled from Bernoulli ( self [ i ] ) \\text { Bernoulli } ( \\texttt { self [ i ] } ) Bernoulli ( self [ i ] ) .", "question_id": 936},
{"snippet": "Tensor.bernoulli(generator=None)", "intent": "Returns a result tensor where each result [ i ] \\texttt { result [ i ] } result [ i ] is independently sampled from Bernoulli ( self [ i ] ) \\text { Bernoulli } ( \\texttt { self [ i ] } ) Bernoulli ( self [ i ] ) . With arguments `generator`.", "question_id": 937},
{"snippet": "torch.nn.Mish()", "intent": "Applies the Mish function , element-wise .", "question_id": 938},
{"snippet": "torch.nn.Mish(inplace=False)", "intent": "Applies the Mish function , element-wise . With arguments `inplace`.", "question_id": 939},
{"snippet": "Tensor.ndim", "intent": "Alias for dim()", "question_id": 940},
{"snippet": "torch.autograd.grad(outputs, inputs)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` .", "question_id": 941},
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t .", "question_id": 942},
{"snippet": "torch.autograd.grad(outputs, inputs, retain_graph=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `retain_graph`.", "question_id": 943},
{"snippet": "torch.autograd.grad(outputs, inputs, create_graph=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `create_graph`.", "question_id": 944},
{"snippet": "torch.autograd.grad(outputs, inputs, only_inputs=True)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . If `only_inputs` is True , the function will only return a list of gradients w.r.t the specified inputs .", "question_id": 945},
{"snippet": "torch.autograd.grad(outputs, inputs, allow_unused=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `allow_unused`.", "question_id": 946},
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `retain_graph`.", "question_id": 947},
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, create_graph=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `create_graph`.", "question_id": 948},
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, only_inputs=True)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . If `only_inputs` is True , the function will only return a list of gradients w.r.t the specified inputs .", "question_id": 949},
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, allow_unused=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `allow_unused`.", "question_id": 950},
{"snippet": "Tensor.cross(other)", "intent": "See torch.cross ( ) With arguments `other`.", "question_id": 951},
{"snippet": "Tensor.cross(other, dim=- 1)", "intent": "See torch.cross ( ) With arguments `other`, `dim`.", "question_id": 952},
{"snippet": "Tensor.ceil()", "intent": "See torch.ceil ( )", "question_id": 953},
{"snippet": "Tensor.sparse_resize_(size, sparse_dim, dense_dim)", "intent": "Resizes self sparse tensor to the desired `size` and the number of sparse and dense dimensions . With arguments `sparse_dim`, `dense_dim`.", "question_id": 954},
{"snippet": "Tensor.nonzero()", "intent": "See torch.nonzero ( )", "question_id": 955},
{"snippet": "torch.swapaxes(input, axis0, axis1)", "intent": "Alias for torch.transpose ( ) . With arguments `input`, `axis0`, `axis1`.", "question_id": 956},
{"snippet": "torch.nn.utils.clip_grad_value_(parameters, clip_value)", "intent": "Clips gradient of an iterable of `parameters` at specified value . With arguments `clip_value`.", "question_id": 957},
{"snippet": "torch.eye(n)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`.", "question_id": 958},
{"snippet": "torch.eye(n, m=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`.", "question_id": 959},
{"snippet": "torch.eye(n, out=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `out`.", "question_id": 960},
{"snippet": "torch.eye(n, dtype=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `dtype`.", "question_id": 961},
{"snippet": "torch.eye(n, layout=torch.strided)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `layout`.", "question_id": 962},
{"snippet": "torch.eye(n, device=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `device`.", "question_id": 963},
{"snippet": "torch.eye(n, requires_grad=False)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `requires_grad`.", "question_id": 964},
{"snippet": "torch.eye(n, m=None, out=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `out`.", "question_id": 965},
{"snippet": "torch.eye(n, m=None, dtype=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `dtype`.", "question_id": 966},
{"snippet": "torch.eye(n, m=None, layout=torch.strided)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `layout`.", "question_id": 967},
{"snippet": "Tensor.type_as(tensor)", "intent": "Returns this `tensor` cast to the type of the given tensor .", "question_id": 968},
{"snippet": "torch.minimum(input, other)", "intent": "Computes the element-wise minimum of `input` and `other` .", "question_id": 969},
{"snippet": "torch.minimum(input, other, out=None)", "intent": "Computes the element-wise minimum of `input` and `other` . With arguments `out`.", "question_id": 970},
{"snippet": "torch.is_inference_mode_enabled()", "intent": "Returns True if inference mode is currently enabled .", "question_id": 971},
{"snippet": "torch.deg2rad(input)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians .", "question_id": 972},
{"snippet": "torch.deg2rad(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians . With arguments `out`.", "question_id": 973},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels .", "question_id": 974},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`.", "question_id": 975},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True .", "question_id": 976},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `device`.", "question_id": 977},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `dtype`.", "question_id": 978},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `eps`.", "question_id": 979},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`, `device`.", "question_id": 980},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`, `dtype`.", "question_id": 981},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `device`.", "question_id": 982},
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `dtype`.", "question_id": 983},
{"snippet": "torch.nn.quantized.dynamic.GRU(*args, **kwargs)", "intent": "Applies a multi-layer gated recurrent unit ( GRU ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 984},
{"snippet": "torch.nn.HuberLoss()", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise .", "question_id": 985},
{"snippet": "torch.nn.HuberLoss(reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . If `reduction` is not none , then :", "question_id": 986},
{"snippet": "torch.nn.HuberLoss(delta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise .", "question_id": 987},
{"snippet": "torch.nn.HuberLoss(reduction='mean', delta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . If `reduction` is not none , then :", "question_id": 988},
{"snippet": "Tensor.tril()", "intent": "See torch.tril ( )", "question_id": 989},
{"snippet": "Tensor.tril(k=0)", "intent": "See torch.tril ( ) With arguments `k`.", "question_id": 990},
{"snippet": "torch.quantization.get_observer_dict(mod, target_dict)", "intent": "Traverse the modules and save all observers into dict . This is mainly used for quantization accuracy debug : param `mod` : the top module we want to save all observers : param `prefix` : the prefix for the current module : param `target_dict` : the dictionary used to save all the observers", "question_id": 991},
{"snippet": "torch.quantization.get_observer_dict(mod, target_dict, prefix='')", "intent": "Traverse the modules and save all observers into dict . This is mainly used for quantization accuracy debug : param `mod` : the top module we want to save all observers : param `prefix` : the prefix for the current module : param `target_dict` : the dictionary used to save all the observers", "question_id": 992},
{"snippet": "torch.nn.utils.prune.LnStructured(amount, n)", "intent": "Prune entire ( currently unpruned ) channels in a tensor based on their Ln-norm . With arguments `amount`, `n`.", "question_id": 993},
{"snippet": "torch.nn.utils.prune.LnStructured(amount, n, dim=- 1)", "intent": "Prune entire ( currently unpruned ) channels in a tensor based on their Ln-norm . With arguments `amount`, `n`, `dim`.", "question_id": 994},
{"snippet": "ln_structured.apply(module, name, amount, n, dim)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `n`, `dim`.", "question_id": 995},
{"snippet": "ln_structured.apply(module, name, amount, n, dim, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `n`, `dim`, `importance_scores`.", "question_id": 996},
{"snippet": "ln_structured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 997},
{"snippet": "ln_structured.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm . With arguments `t`.", "question_id": 998},
{"snippet": "ln_structured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 999},
{"snippet": "ln_structured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 1000},
{"snippet": "ln_structured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 1001},
{"snippet": "ln_structured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 1002},
{"snippet": "ln_structured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 1003},
{"snippet": "Tensor.asin()", "intent": "See torch.asin ( )", "question_id": 1004},
{"snippet": "Tensor.half()", "intent": "self.half ( ) is equivalent to self.to ( torch.float16 ) .", "question_id": 1005},
{"snippet": "Tensor.half(memory_format=torch.preserve_format)", "intent": "self.half ( ) is equivalent to self.to ( torch.float16 ) . With arguments `memory_format`.", "question_id": 1006},
{"snippet": "torch.nn.utils.vector_to_parameters(vec, parameters)", "intent": "Convert one vector to the `parameters` With arguments `vec`.", "question_id": 1007},
{"snippet": "Tensor.clip_()", "intent": "Alias for clamp_ ( ) .", "question_id": 1008},
{"snippet": "Tensor.clip_(min=None)", "intent": "Alias for clamp_ ( ) . With arguments `min`.", "question_id": 1009},
{"snippet": "Tensor.clip_(max=None)", "intent": "Alias for clamp_ ( ) . With arguments `max`.", "question_id": 1010},
{"snippet": "Tensor.clip_(min=None, max=None)", "intent": "Alias for clamp_ ( ) . With arguments `min`, `max`.", "question_id": 1011},
{"snippet": "Tensor.msort()", "intent": "See torch.msort ( )", "question_id": 1012},
{"snippet": "torch.empty(*size)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`.", "question_id": 1013},
{"snippet": "torch.empty(*size, out=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`.", "question_id": 1014},
{"snippet": "torch.empty(*size, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `dtype`.", "question_id": 1015},
{"snippet": "torch.empty(*size, layout=torch.strided)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `layout`.", "question_id": 1016},
{"snippet": "torch.empty(*size, device=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `device`.", "question_id": 1017},
{"snippet": "torch.empty(*size, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `requires_grad`.", "question_id": 1018},
{"snippet": "torch.empty(*size, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `pin_memory`.", "question_id": 1019},
{"snippet": "torch.empty(*size, memory_format=torch.contiguous_format)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `memory_format`.", "question_id": 1020},
{"snippet": "torch.empty(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`, `dtype`.", "question_id": 1021},
{"snippet": "torch.empty(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`, `layout`.", "question_id": 1022},
{"snippet": "torch.nn.functional.max_pool3d(*args, **kwargs)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 1023},
{"snippet": "Tensor.moveaxis(source, destination)", "intent": "See torch.moveaxis ( ) With arguments `source`, `destination`.", "question_id": 1024},
{"snippet": "torch.linalg.tensorsolve(A, B)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` .", "question_id": 1025},
{"snippet": "torch.linalg.tensorsolve(A, B, dims=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . If `dims` is specified , A will be reshaped as", "question_id": 1026},
{"snippet": "torch.linalg.tensorsolve(A, B, out=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . With arguments `out`.", "question_id": 1027},
{"snippet": "torch.linalg.tensorsolve(A, B, dims=None, out=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . If `dims` is specified , A will be reshaped as With arguments `out`.", "question_id": 1028},
{"snippet": "Tensor.arcsinh_()", "intent": "In-place version of arcsinh ( )", "question_id": 1029},
{"snippet": "Tensor.fix()", "intent": "See torch.fix ( ) .", "question_id": 1030},
{"snippet": "torch.nn.quantized.dynamic.LSTM(*args, **kwargs)", "intent": "A dynamic quantized LSTM module with floating point tensor as inputs and outputs . With arguments `*args`, `**kwargs`.", "question_id": 1031},
{"snippet": "torch.cuda.comm.reduce_add(inputs)", "intent": "Sums tensors from multiple GPUs . All `inputs` should have matching shapes , dtype , and layout .", "question_id": 1032},
{"snippet": "torch.cuda.comm.reduce_add(inputs, destination=None)", "intent": "Sums tensors from multiple GPUs . All `inputs` should have matching shapes , dtype , and layout . With arguments `destination`.", "question_id": 1033},
{"snippet": "torch.nn.functional.group_norm(input, num_groups)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`.", "question_id": 1034},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`.", "question_id": 1035},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, bias=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `bias`.", "question_id": 1036},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `eps`.", "question_id": 1037},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, bias=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `bias`.", "question_id": 1038},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `eps`.", "question_id": 1039},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, bias=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `bias`, `eps`.", "question_id": 1040},
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, bias=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `bias`, `eps`.", "question_id": 1041},
{"snippet": "Tensor.cosh_()", "intent": "In-place version of cosh ( )", "question_id": 1042},
{"snippet": "Tensor.qscheme()", "intent": "Returns the quantization scheme of a given QTensor .", "question_id": 1043},
{"snippet": "Tensor.fmin(other)", "intent": "See torch.fmin ( ) With arguments `other`.", "question_id": 1044},
{"snippet": "Tensor.mm(mat2)", "intent": "See torch.mm ( ) With arguments `mat2`.", "question_id": 1045},
{"snippet": "torch.arccosh(input)", "intent": "Alias for torch.acosh ( ) . With arguments `input`.", "question_id": 1046},
{"snippet": "torch.arccosh(input, out=None)", "intent": "Alias for torch.acosh ( ) . With arguments `input`, `out`.", "question_id": 1047},
{"snippet": "torch.nn.functional.upsample_nearest(input)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values .", "question_id": 1048},
{"snippet": "torch.nn.functional.upsample_nearest(input, size=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`.", "question_id": 1049},
{"snippet": "torch.nn.functional.upsample_nearest(input, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `scale_factor`.", "question_id": 1050},
{"snippet": "torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`, `scale_factor`.", "question_id": 1051},
{"snippet": "torch.nn.utils.parametrize.is_parametrized(module)", "intent": "Returns True if `module` has an active parametrization .", "question_id": 1052},
{"snippet": "torch.nn.utils.parametrize.is_parametrized(module, tensor_name=None)", "intent": "Returns True if `module` has an active parametrization . If the argument `tensor_name` is specified , returns True if module [ tensor_name ] is parametrized .", "question_id": 1053},
{"snippet": "Tensor.not_equal(other)", "intent": "See torch.not_equal ( ) . With arguments `other`.", "question_id": 1054},
{"snippet": "Tensor.quantile(q)", "intent": "See torch.quantile ( ) With arguments `q`.", "question_id": 1055},
{"snippet": "Tensor.quantile(q, dim=None)", "intent": "See torch.quantile ( ) With arguments `q`, `dim`.", "question_id": 1056},
{"snippet": "Tensor.quantile(q, keepdim=False)", "intent": "See torch.quantile ( ) With arguments `q`, `keepdim`.", "question_id": 1057},
{"snippet": "Tensor.quantile(q, dim=None, keepdim=False)", "intent": "See torch.quantile ( ) With arguments `q`, `dim`, `keepdim`.", "question_id": 1058},
{"snippet": "torch.view_as_complex(input)", "intent": "Returns a view of `input` as a complex tensor .", "question_id": 1059},
{"snippet": "torch.sum(input)", "intent": "Returns the sum of all elements in the `input` tensor .", "question_id": 1060},
{"snippet": "torch.sum(input, dtype=None)", "intent": "Returns the sum of all elements in the `input` tensor . With arguments `dtype`.", "question_id": 1061},
{"snippet": "Tensor.fix_()", "intent": "In-place version of fix ( )", "question_id": 1062},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`.", "question_id": 1063},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`.", "question_id": 1064},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, eps=1e-06)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `eps`.", "question_id": 1065},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `keepdim`.", "question_id": 1066},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `eps`.", "question_id": 1067},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `keepdim`.", "question_id": 1068},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, eps=1e-06, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `eps`, `keepdim`.", "question_id": 1069},
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `eps`, `keepdim`.", "question_id": 1070},
{"snippet": "torch.scatter(input, dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_ ( ) With arguments `input`, `dim`, `index`, `src`.", "question_id": 1071},
{"snippet": "torch.bitwise_or(input, other)", "intent": "Computes the bitwise OR of `input` and `other` .", "question_id": 1072},
{"snippet": "torch.bitwise_or(input, other, out=None)", "intent": "Computes the bitwise OR of `input` and `other` . With arguments `out`.", "question_id": 1073},
{"snippet": "torch.nn.functional.grid_sample(input, grid)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid .", "question_id": 1074},
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels .", "question_id": 1075},
{"snippet": "torch.nn.functional.grid_sample(input, grid, padding_mode='zeros')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` .", "question_id": 1076},
{"snippet": "torch.nn.functional.grid_sample(input, grid, align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . With arguments `align_corners`.", "question_id": 1077},
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` .", "question_id": 1078},
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . With arguments `align_corners`.", "question_id": 1079},
{"snippet": "torch.nn.functional.grid_sample(input, grid, padding_mode='zeros', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` . With arguments `align_corners`.", "question_id": 1080},
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` . With arguments `align_corners`.", "question_id": 1081},
{"snippet": "torch.combinations(input)", "intent": "Compute combinations of length rrr of the given tensor . With arguments `input`.", "question_id": 1082},
{"snippet": "torch.combinations(input, r=2)", "intent": "Compute combinations of length rrr of the given tensor . With arguments `input`, `r`.", "question_id": 1083},
{"snippet": "torch.combinations(input, with_replacement=False)", "intent": "Compute combinations of length rrr of the given tensor . The behavior is similar to python \u2019 s itertools.combinations when `with_replacement` is set to False , and itertools.combinations_with_replacement when with_replacement is set to True . With arguments `input`.", "question_id": 1084},
{"snippet": "torch.combinations(input, r=2, with_replacement=False)", "intent": "Compute combinations of length rrr of the given tensor . The behavior is similar to python \u2019 s itertools.combinations when `with_replacement` is set to False , and itertools.combinations_with_replacement when with_replacement is set to True . With arguments `input`, `r`.", "question_id": 1085},
{"snippet": "torch.logical_or(input, other)", "intent": "Computes the element-wise logical OR of the given `input` tensors . With arguments `other`.", "question_id": 1086},
{"snippet": "torch.logical_or(input, other, out=None)", "intent": "Computes the element-wise logical OR of the given `input` tensors . With arguments `other`, `out`.", "question_id": 1087},
{"snippet": "torch.autograd.functional.hvp(func, inputs)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`.", "question_id": 1088},
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`.", "question_id": 1089},
{"snippet": "torch.autograd.functional.hvp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 1090},
{"snippet": "torch.autograd.functional.hvp(func, inputs, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 1091},
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 1092},
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 1093},
{"snippet": "torch.autograd.functional.hvp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 1094},
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 1095},
{"snippet": "Tensor.expm1()", "intent": "See torch.expm1 ( )", "question_id": 1096},
{"snippet": "Tensor.angle()", "intent": "See torch.angle ( )", "question_id": 1097},
{"snippet": "Tensor.neg()", "intent": "See torch.neg ( )", "question_id": 1098},
{"snippet": "Tensor.sqrt()", "intent": "See torch.sqrt ( )", "question_id": 1099},
{"snippet": "Tensor.float_power_(exponent)", "intent": "In-place version of float_power ( ) With arguments `exponent`.", "question_id": 1100},
{"snippet": "torch.cuda.is_initialized()", "intent": "Returns whether PyTorch \u2019 s CUDA state has been initialized .", "question_id": 1101},
{"snippet": "torch.solve(input, A)", "intent": "This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of `A` , in order as a namedtuple solution , LU . With arguments `input`.", "question_id": 1102},
{"snippet": "torch.solve(input, A, out=None)", "intent": "This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of `A` , in order as a namedtuple solution , LU . With arguments `input`, `out`.", "question_id": 1103},
{"snippet": "torch.nn.intrinsic.ConvReLU3d(conv, relu)", "intent": "This is a sequential container which calls the Conv3d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 1104},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size .", "question_id": 1105},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format .", "question_id": 1106},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, padding_value=0.0)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `padding_value`.", "question_id": 1107},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `total_length`.", "question_id": 1108},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `padding_value`.", "question_id": 1109},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `total_length`.", "question_id": 1110},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, padding_value=0.0, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `padding_value`, `total_length`.", "question_id": 1111},
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `padding_value`, `total_length`.", "question_id": 1112},
{"snippet": "Tensor.copysign(other)", "intent": "See torch.copysign ( ) With arguments `other`.", "question_id": 1113},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`.", "question_id": 1114},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`.", "question_id": 1115},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `padding`.", "question_id": 1116},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `stride`.", "question_id": 1117},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `padding`.", "question_id": 1118},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `stride`.", "question_id": 1119},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `padding`, `stride`.", "question_id": 1120},
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `padding`, `stride`.", "question_id": 1121},
{"snippet": "torch.isclose(input, other)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` .", "question_id": 1122},
{"snippet": "torch.isclose(input, other, rtol=1e-05)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `rtol`.", "question_id": 1123},
{"snippet": "torch.isclose(input, other, atol=1e-08)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `atol`.", "question_id": 1124},
{"snippet": "torch.isclose(input, other, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True .", "question_id": 1125},
{"snippet": "torch.isclose(input, other, rtol=1e-05, atol=1e-08)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `rtol`, `atol`.", "question_id": 1126},
{"snippet": "torch.isclose(input, other, rtol=1e-05, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `rtol`.", "question_id": 1127},
{"snippet": "torch.isclose(input, other, atol=1e-08, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `atol`.", "question_id": 1128},
{"snippet": "torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `rtol`, `atol`.", "question_id": 1129},
{"snippet": "Tensor.t_()", "intent": "In-place version of t ( )", "question_id": 1130},
{"snippet": "Tensor.greater_(other)", "intent": "In-place version of greater ( ) . With arguments `other`.", "question_id": 1131},
{"snippet": "torch.fft.rfft2(input, - 1))", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`.", "question_id": 1132},
{"snippet": "torch.fft.rfft2(input, - 1), s=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`.", "question_id": 1133},
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`.", "question_id": 1134},
{"snippet": "torch.fft.rfft2(input, - 1), norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `norm`.", "question_id": 1135},
{"snippet": "torch.fft.rfft2(input, - 1), out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `out`.", "question_id": 1136},
{"snippet": "torch.fft.rfft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 1137},
{"snippet": "torch.fft.rfft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 1138},
{"snippet": "torch.fft.rfft2(input, - 1), s=None, out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 1139},
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 1140},
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 1141},
{"snippet": "torch.nn.TransformerDecoder(decoder_layer, num_layers)", "intent": "TransformerDecoder is a stack of N decoder layers With arguments `decoder_layer`, `num_layers`.", "question_id": 1142},
{"snippet": "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)", "intent": "TransformerDecoder is a stack of N decoder layers With arguments `decoder_layer`, `num_layers`, `norm`.", "question_id": 1143},
{"snippet": "transformer_decoder.forward(tgt, memory)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`.", "question_id": 1144},
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`.", "question_id": 1145},
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`.", "question_id": 1146},
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_key_padding_mask`.", "question_id": 1147},
{"snippet": "transformer_decoder.forward(tgt, memory, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_key_padding_mask`.", "question_id": 1148},
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `memory_mask`.", "question_id": 1149},
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `tgt_key_padding_mask`.", "question_id": 1150},
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `memory_key_padding_mask`.", "question_id": 1151},
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`, `tgt_key_padding_mask`.", "question_id": 1152},
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`, `memory_key_padding_mask`.", "question_id": 1153},
{"snippet": "torch.cholesky(input)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . With arguments `input`.", "question_id": 1154},
{"snippet": "torch.cholesky(input, upper=False)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . If `upper` is True , the returned matrix U is upper-triangular , and the decomposition has the form : With arguments `input`.", "question_id": 1155},
{"snippet": "torch.cholesky(input, out=None)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . With arguments `input`, `out`.", "question_id": 1156},
{"snippet": "torch.cholesky(input, upper=False, out=None)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . If `upper` is True , the returned matrix U is upper-triangular , and the decomposition has the form : With arguments `input`, `out`.", "question_id": 1157},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`.", "question_id": 1158},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`.", "question_id": 1159},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 1160},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 1161},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 1162},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `size_average`.", "question_id": 1163},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `reduce`.", "question_id": 1164},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `reduction`.", "question_id": 1165},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 1166},
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 1167},
{"snippet": "Tensor.cuda()", "intent": "Returns a copy of this object in CUDA memory .", "question_id": 1168},
{"snippet": "Tensor.cuda(device=None)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned .", "question_id": 1169},
{"snippet": "Tensor.cuda(non_blocking=False)", "intent": "Returns a copy of this object in CUDA memory . With arguments `non_blocking`.", "question_id": 1170},
{"snippet": "Tensor.cuda(memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . With arguments `memory_format`.", "question_id": 1171},
{"snippet": "Tensor.cuda(device=None, non_blocking=False)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `non_blocking`.", "question_id": 1172},
{"snippet": "Tensor.cuda(device=None, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `memory_format`.", "question_id": 1173},
{"snippet": "Tensor.cuda(non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . With arguments `non_blocking`, `memory_format`.", "question_id": 1174},
{"snippet": "Tensor.cuda(device=None, non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `non_blocking`, `memory_format`.", "question_id": 1175},
{"snippet": "torch.nn.functional.dropout3d(input)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 1176},
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 1177},
{"snippet": "torch.nn.functional.dropout3d(input, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 1178},
{"snippet": "torch.nn.functional.dropout3d(input, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 1179},
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 1180},
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 1181},
{"snippet": "torch.nn.functional.dropout3d(input, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 1182},
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 1183},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 1184},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 1185},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, padding=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 1186},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, dilation=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 1187},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, groups=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 1188},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 1189},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, scale=1.0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 1190},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, zero_point=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 1191},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 1192},
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 1193},
{"snippet": "Tensor.sinc()", "intent": "See torch.sinc ( )", "question_id": 1194},
{"snippet": "Tensor.eq_(other)", "intent": "In-place version of eq ( ) With arguments `other`.", "question_id": 1195},
{"snippet": "Tensor.uniform_()", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution :", "question_id": 1196},
{"snippet": "Tensor.uniform_(from=0)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution :", "question_id": 1197},
{"snippet": "Tensor.uniform_(to=1)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution : With arguments `to`.", "question_id": 1198},
{"snippet": "Tensor.uniform_(from=0, to=1)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution : With arguments `to`.", "question_id": 1199},
{"snippet": "torch.nn.CosineEmbeddingLoss()", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 .", "question_id": 1200},
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`.", "question_id": 1201},
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`.", "question_id": 1202},
{"snippet": "torch.nn.CosineEmbeddingLoss(reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `reduce`.", "question_id": 1203},
{"snippet": "torch.nn.CosineEmbeddingLoss(reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `reduction`.", "question_id": 1204},
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `size_average`.", "question_id": 1205},
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `reduce`.", "question_id": 1206},
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `reduction`.", "question_id": 1207},
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`, `reduce`.", "question_id": 1208},
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`, `reduction`.", "question_id": 1209},
{"snippet": "Tensor.device", "intent": "Is the torch.device where this Tensor is.", "question_id": 1210},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`.", "question_id": 1211},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`.", "question_id": 1212},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`.", "question_id": 1213},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 1214},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 1215},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 1216},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 1217},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 1218},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`, `ceil_mode`.", "question_id": 1219},
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`, `count_include_pad`.", "question_id": 1220},
{"snippet": "torch.einsum(equation, *operands)", "intent": "Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention . Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention , given by `equation` . With arguments `*operands`.", "question_id": 1221},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 1222},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 1223},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 1224},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 1225},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 1226},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 1227},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 1228},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 1229},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 1230},
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 1231},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`.", "question_id": 1232},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`.", "question_id": 1233},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`.", "question_id": 1234},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `device`.", "question_id": 1235},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `dtype`.", "question_id": 1236},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, inplace=False)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `inplace`.", "question_id": 1237},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `device`.", "question_id": 1238},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `dtype`.", "question_id": 1239},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`, `device`.", "question_id": 1240},
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`, `dtype`.", "question_id": 1241},
{"snippet": "torch.from_numpy(ndarray)", "intent": "Creates a Tensor from a numpy.ndarray . The returned tensor and `ndarray` share the same memory .", "question_id": 1242},
{"snippet": "torch.nn.functional.threshold(input, threshold, value)", "intent": "Thresholds each element of the `input` Tensor . With arguments `threshold`, `value`.", "question_id": 1243},
{"snippet": "torch.nn.functional.threshold(input, threshold, value, inplace=False)", "intent": "Thresholds each element of the `input` Tensor . With arguments `threshold`, `value`, `inplace`.", "question_id": 1244},
{"snippet": "Tensor.backward()", "intent": "Computes the `gradient` of current tensor w.r.t .", "question_id": 1245},
{"snippet": "Tensor.backward(gradient=None)", "intent": "Computes the `gradient` of current tensor w.r.t .", "question_id": 1246},
{"snippet": "Tensor.backward(retain_graph=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`.", "question_id": 1247},
{"snippet": "Tensor.backward(create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `create_graph`.", "question_id": 1248},
{"snippet": "Tensor.backward(inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `inputs`.", "question_id": 1249},
{"snippet": "Tensor.backward(gradient=None, retain_graph=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`.", "question_id": 1250},
{"snippet": "Tensor.backward(gradient=None, create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `create_graph`.", "question_id": 1251},
{"snippet": "Tensor.backward(gradient=None, inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `inputs`.", "question_id": 1252},
{"snippet": "Tensor.backward(retain_graph=None, create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`, `create_graph`.", "question_id": 1253},
{"snippet": "Tensor.backward(retain_graph=None, inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`, `inputs`.", "question_id": 1254},
{"snippet": "Tensor.retain_grad()", "intent": "Enables .grad attribute for non-leaf Tensors .", "question_id": 1255},
{"snippet": "torch.nn.KLDivLoss()", "intent": "The Kullback-Leibler divergence loss measure", "question_id": 1256},
{"snippet": "torch.nn.KLDivLoss(size_average=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `size_average`.", "question_id": 1257},
{"snippet": "torch.nn.KLDivLoss(reduce=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `reduce`.", "question_id": 1258},
{"snippet": "torch.nn.KLDivLoss(reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as :", "question_id": 1259},
{"snippet": "torch.nn.KLDivLoss(log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True .", "question_id": 1260},
{"snippet": "torch.nn.KLDivLoss(size_average=None, reduce=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `size_average`, `reduce`.", "question_id": 1261},
{"snippet": "torch.nn.KLDivLoss(size_average=None, reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 1262},
{"snippet": "torch.nn.KLDivLoss(size_average=None, log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True . With arguments `size_average`.", "question_id": 1263},
{"snippet": "torch.nn.KLDivLoss(reduce=None, reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 1264},
{"snippet": "torch.nn.KLDivLoss(reduce=None, log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True . With arguments `reduce`.", "question_id": 1265},
{"snippet": "torch.nn.quantized.functional.threshold(input, threshold, value)", "intent": "Applies the quantized version of the `threshold` function element-wise : With arguments `input`, `value`.", "question_id": 1266},
{"snippet": "torch.nn.quantized.functional.upsample(input)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 1267},
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 1268},
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 1269},
{"snippet": "torch.nn.quantized.functional.upsample(input, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 1270},
{"snippet": "torch.nn.quantized.functional.upsample(input, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 1271},
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 1272},
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 1273},
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 1274},
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 1275},
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 1276},
{"snippet": "torch.bmm(input, mat2)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` .", "question_id": 1277},
{"snippet": "torch.bmm(input, mat2, deterministic=False)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . With arguments `deterministic`.", "question_id": 1278},
{"snippet": "torch.bmm(input, mat2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . If input is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , mat2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 1279},
{"snippet": "torch.bmm(input, mat2, deterministic=False, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . If input is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , mat2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor . With arguments `deterministic`.", "question_id": 1280},
{"snippet": "torch.rot90(input, k, dims)", "intent": "Rotate a n-D tensor by 90 degrees in the plane specified by `dims` axis . Rotation direction is from the first towards the second axis if `k` > 0 , and from the second towards the first for k < 0 . With arguments `input`.", "question_id": 1281},
{"snippet": "torch.abs(input)", "intent": "Computes the absolute value of each element in `input` .", "question_id": 1282},
{"snippet": "torch.abs(input, out=None)", "intent": "Computes the absolute value of each element in `input` . With arguments `out`.", "question_id": 1283},
{"snippet": "torch.linalg.vector_norm(A)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( )", "question_id": 1284},
{"snippet": "torch.linalg.vector_norm(A, ord=2)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed .", "question_id": 1285},
{"snippet": "torch.linalg.vector_norm(A, dim=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dim`.", "question_id": 1286},
{"snippet": "torch.linalg.vector_norm(A, keepdim=False)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `keepdim`.", "question_id": 1287},
{"snippet": "torch.linalg.vector_norm(A, dtype=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dtype`.", "question_id": 1288},
{"snippet": "torch.linalg.vector_norm(A, out=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `out`.", "question_id": 1289},
{"snippet": "torch.linalg.vector_norm(A, ord=2, dim=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `dim`.", "question_id": 1290},
{"snippet": "torch.linalg.vector_norm(A, ord=2, keepdim=False)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `keepdim`.", "question_id": 1291},
{"snippet": "torch.linalg.vector_norm(A, ord=2, dtype=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `dtype`.", "question_id": 1292},
{"snippet": "torch.linalg.vector_norm(A, ord=2, out=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `out`.", "question_id": 1293},
{"snippet": "Tensor.element_size()", "intent": "Returns the size in bytes of an individual element .", "question_id": 1294},
{"snippet": "torch.isfinite(input)", "intent": "Returns a new tensor with boolean elements representing if each element is finite or not . With arguments `input`.", "question_id": 1295},
{"snippet": "torch.kthvalue(input, k)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` .", "question_id": 1296},
{"snippet": "torch.kthvalue(input, k, dim=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` .", "question_id": 1297},
{"snippet": "torch.kthvalue(input, k, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 .", "question_id": 1298},
{"snippet": "torch.kthvalue(input, k, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 1299},
{"snippet": "torch.kthvalue(input, k, dim=None, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 .", "question_id": 1300},
{"snippet": "torch.kthvalue(input, k, dim=None, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 1301},
{"snippet": "torch.kthvalue(input, k, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 1302},
{"snippet": "torch.kthvalue(input, k, dim=None, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 1303},
{"snippet": "torch.copysign(input, other)", "intent": "Create a new floating-point tensor with the magnitude of `input` and the sign of `other` , elementwise .", "question_id": 1304},
{"snippet": "torch.copysign(input, other, out=None)", "intent": "Create a new floating-point tensor with the magnitude of `input` and the sign of `other` , elementwise . With arguments `out`.", "question_id": 1305},
{"snippet": "Tensor.log2_()", "intent": "In-place version of log2 ( )", "question_id": 1306},
{"snippet": "torch.nn.utils.prune.custom_from_mask(module, name, mask)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by applying the pre-computed `mask` in mask .", "question_id": 1307},
{"snippet": "torch.swapdims(input, dim0, dim1)", "intent": "Alias for torch.transpose ( ) . With arguments `input`, `dim0`, `dim1`.", "question_id": 1308},
{"snippet": "Tensor.erfc_()", "intent": "In-place version of erfc ( )", "question_id": 1309},
{"snippet": "Tensor.normal_()", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 1310},
{"snippet": "Tensor.normal_(mean=0)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 1311},
{"snippet": "Tensor.normal_(std=1)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 1312},
{"snippet": "Tensor.normal_(generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 1313},
{"snippet": "Tensor.normal_(mean=0, std=1)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 1314},
{"snippet": "Tensor.normal_(mean=0, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 1315},
{"snippet": "Tensor.normal_(std=1, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 1316},
{"snippet": "Tensor.normal_(mean=0, std=1, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 1317},
{"snippet": "Tensor.new_zeros(size)", "intent": "Returns a Tensor of `size` size filled with 0 .", "question_id": 1318},
{"snippet": "Tensor.new_zeros(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`.", "question_id": 1319},
{"snippet": "Tensor.new_zeros(size, device=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `device`.", "question_id": 1320},
{"snippet": "Tensor.new_zeros(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `requires_grad`.", "question_id": 1321},
{"snippet": "Tensor.new_zeros(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `device`.", "question_id": 1322},
{"snippet": "Tensor.new_zeros(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `requires_grad`.", "question_id": 1323},
{"snippet": "Tensor.new_zeros(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `device`, `requires_grad`.", "question_id": 1324},
{"snippet": "Tensor.new_zeros(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 1325},
{"snippet": "torch.nn.GELU", "intent": "Applies the Gaussian Error Linear Units function:", "question_id": 1326},
{"snippet": "Tensor.nanquantile(q)", "intent": "See torch.nanquantile ( ) With arguments `q`.", "question_id": 1327},
{"snippet": "Tensor.nanquantile(q, dim=None)", "intent": "See torch.nanquantile ( ) With arguments `q`, `dim`.", "question_id": 1328},
{"snippet": "Tensor.nanquantile(q, keepdim=False)", "intent": "See torch.nanquantile ( ) With arguments `q`, `keepdim`.", "question_id": 1329},
{"snippet": "Tensor.nanquantile(q, dim=None, keepdim=False)", "intent": "See torch.nanquantile ( ) With arguments `q`, `dim`, `keepdim`.", "question_id": 1330},
{"snippet": "torch.log10(input)", "intent": "Returns a new tensor with the logarithm to the base 10 of the elements of `input` .", "question_id": 1331},
{"snippet": "torch.log10(input, out=None)", "intent": "Returns a new tensor with the logarithm to the base 10 of the elements of `input` . With arguments `out`.", "question_id": 1332},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 1333},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 1334},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 1335},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 1336},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 1337},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 1338},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 1339},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 1340},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 1341},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 1342},
{"snippet": "torch.nn.LazyLinear(out_features)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`.", "question_id": 1343},
{"snippet": "torch.nn.LazyLinear(out_features, bias=True)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`.", "question_id": 1344},
{"snippet": "torch.nn.LazyLinear(out_features, device=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `device`.", "question_id": 1345},
{"snippet": "torch.nn.LazyLinear(out_features, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `dtype`.", "question_id": 1346},
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, device=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `device`.", "question_id": 1347},
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `dtype`.", "question_id": 1348},
{"snippet": "torch.nn.LazyLinear(out_features, device=None, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `device`, `dtype`.", "question_id": 1349},
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, device=None, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `device`, `dtype`.", "question_id": 1350},
{"snippet": "lazy_linear.cls_to_become", "intent": "alias of torch.nn.modules.linear.Linear", "question_id": 1351},
{"snippet": "torch.cuda.ipc_collect()", "intent": "Force collects GPU memory after it has been released by CUDA IPC .", "question_id": 1352},
{"snippet": "torch.tile(input, reps)", "intent": "Constructs a tensor by repeating the elements of `input` . The `reps` argument specifies the number of repetitions in each dimension .", "question_id": 1353},
{"snippet": "torch.nn.CosineSimilarity()", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` .", "question_id": 1354},
{"snippet": "torch.nn.CosineSimilarity(dim=1)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` .", "question_id": 1355},
{"snippet": "torch.nn.CosineSimilarity(eps=1e-08)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` . With arguments `eps`.", "question_id": 1356},
{"snippet": "torch.nn.CosineSimilarity(dim=1, eps=1e-08)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` . With arguments `eps`.", "question_id": 1357},
{"snippet": "torch.autograd.profiler.load_nvprof(path)", "intent": "Opens an nvprof trace file and parses autograd annotations . With arguments `path`.", "question_id": 1358},
{"snippet": "Tensor.subtract_(other)", "intent": "In-place version of subtract ( ) . With arguments `other`.", "question_id": 1359},
{"snippet": "Tensor.subtract_(other, alpha=1)", "intent": "In-place version of subtract ( ) . With arguments `other`, `alpha`.", "question_id": 1360},
{"snippet": "torch.nn.MaxPool1d(kernel_size)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 1361},
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 1362},
{"snippet": "torch.nn.MaxPool1d(kernel_size, padding=0)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . If `padding` is non-zero , then the input is implicitly padded with negative infinity on both sides for padding number of points . With arguments `kernel_size`.", "question_id": 1363},
{"snippet": "torch.nn.MaxPool1d(kernel_size, dilation=1)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 1364},
{"snippet": "torch.nn.MaxPool1d(kernel_size, return_indices=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 1365},
{"snippet": "torch.nn.MaxPool1d(kernel_size, ceil_mode=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 1366},
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, padding=0)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . If `padding` is non-zero , then the input is implicitly padded with negative infinity on both sides for padding number of points . With arguments `kernel_size`.", "question_id": 1367},
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 1368},
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`, `return_indices`.", "question_id": 1369},
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`, `ceil_mode`.", "question_id": 1370},
{"snippet": "torch.less(input, other)", "intent": "Alias for torch.lt ( ) . With arguments `input`, `other`.", "question_id": 1371},
{"snippet": "torch.less(input, other, out=None)", "intent": "Alias for torch.lt ( ) . With arguments `input`, `other`, `out`.", "question_id": 1372},
{"snippet": "Tensor.logaddexp2(other)", "intent": "See torch.logaddexp2 ( ) With arguments `other`.", "question_id": 1373},
{"snippet": "torch.true_divide(dividend, divisor, out)", "intent": "Alias for torch.div ( ) with rounding_mode=None . With arguments `dividend`, `divisor`, `out`.", "question_id": 1374},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 1375},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`.", "question_id": 1376},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 1377},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 1378},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 1379},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, sparse=False)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 1380},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, _weight=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 1381},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, device=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `device`.", "question_id": 1382},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, dtype=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 1383},
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `max_norm`.", "question_id": 1384},
{"snippet": "embedding.from_pretrained(embeddings)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`.", "question_id": 1385},
{"snippet": "embedding.from_pretrained(embeddings, freeze=True)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`.", "question_id": 1386},
{"snippet": "embedding.from_pretrained(embeddings, padding_idx=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `padding_idx`.", "question_id": 1387},
{"snippet": "embedding.from_pretrained(embeddings, max_norm=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `max_norm`.", "question_id": 1388},
{"snippet": "embedding.from_pretrained(embeddings, norm_type=2.0)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `norm_type`.", "question_id": 1389},
{"snippet": "embedding.from_pretrained(embeddings, scale_grad_by_freq=False)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `scale_grad_by_freq`.", "question_id": 1390},
{"snippet": "embedding.from_pretrained(embeddings, sparse=False)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `sparse`.", "question_id": 1391},
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, padding_idx=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `padding_idx`.", "question_id": 1392},
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, max_norm=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `max_norm`.", "question_id": 1393},
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, norm_type=2.0)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `norm_type`.", "question_id": 1394},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`.", "question_id": 1395},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`.", "question_id": 1396},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, device=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `device`.", "question_id": 1397},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `dtype`.", "question_id": 1398},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, device=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `device`.", "question_id": 1399},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `dtype`.", "question_id": 1400},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, device=None, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `device`, `dtype`.", "question_id": 1401},
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, device=None, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `device`, `dtype`.", "question_id": 1402},
{"snippet": "Tensor.scatter_(dim, index, src)", "intent": "Writes all values from the tensor `src` into self at the indices specified in the `index` tensor . For each value in src , its output index is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim .", "question_id": 1403},
{"snippet": "Tensor.scatter_(dim, index, src, reduce=None)", "intent": "Writes all values from the tensor `src` into self at the indices specified in the `index` tensor . For each value in src , its output index is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim . Additionally accepts an optional `reduce` argument that allows specification of an optional reduction operation , which is applied to all values in the tensor src into self at the indicies specified in the index .", "question_id": 1404},
{"snippet": "torch.log2(input)", "intent": "Returns a new tensor with the logarithm to the base 2 of the elements of `input` .", "question_id": 1405},
{"snippet": "torch.log2(input, out=None)", "intent": "Returns a new tensor with the logarithm to the base 2 of the elements of `input` . With arguments `out`.", "question_id": 1406},
{"snippet": "Tensor.atan_()", "intent": "In-place version of atan ( )", "question_id": 1407},
{"snippet": "Tensor.new_empty(size)", "intent": "Returns a Tensor of `size` size filled with uninitialized data .", "question_id": 1408},
{"snippet": "Tensor.new_empty(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`.", "question_id": 1409},
{"snippet": "Tensor.new_empty(size, device=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `device`.", "question_id": 1410},
{"snippet": "Tensor.new_empty(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `requires_grad`.", "question_id": 1411},
{"snippet": "Tensor.new_empty(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `device`.", "question_id": 1412},
{"snippet": "Tensor.new_empty(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `requires_grad`.", "question_id": 1413},
{"snippet": "Tensor.new_empty(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `device`, `requires_grad`.", "question_id": 1414},
{"snippet": "Tensor.new_empty(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 1415},
{"snippet": "torch.nn.utils.prune.RandomStructured(amount)", "intent": "Prune entire ( currently unpruned ) channels in a tensor at random . With arguments `amount`.", "question_id": 1416},
{"snippet": "torch.nn.utils.prune.RandomStructured(amount, dim=- 1)", "intent": "Prune entire ( currently unpruned ) channels in a tensor at random . With arguments `amount`, `dim`.", "question_id": 1417},
{"snippet": "random_structured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 1418},
{"snippet": "random_structured.apply(module, name, amount, dim=- 1)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `dim`.", "question_id": 1419},
{"snippet": "random_structured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 1420},
{"snippet": "random_structured.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor . With arguments `t`.", "question_id": 1421},
{"snippet": "random_structured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 1422},
{"snippet": "random_structured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 1423},
{"snippet": "random_structured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 1424},
{"snippet": "random_structured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 1425},
{"snippet": "random_structured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 1426},
{"snippet": "torch.clone(input)", "intent": "Returns a copy of `input` .", "question_id": 1427},
{"snippet": "torch.clone(input, memory_format=torch.preserve_format)", "intent": "Returns a copy of `input` . With arguments `memory_format`.", "question_id": 1428},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`.", "question_id": 1429},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`.", "question_id": 1430},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `last_epoch`.", "question_id": 1431},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `verbose`.", "question_id": 1432},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `last_epoch`.", "question_id": 1433},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `verbose`.", "question_id": 1434},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, last_epoch=- 1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `last_epoch`, `verbose`.", "question_id": 1435},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=- 1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `last_epoch`, `verbose`.", "question_id": 1436},
{"snippet": "cosine_annealing_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 1437},
{"snippet": "cosine_annealing_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 1438},
{"snippet": "cosine_annealing_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 1439},
{"snippet": "cosine_annealing_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 1440},
{"snippet": "cosine_annealing_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 1441},
{"snippet": "torch.ceil(input)", "intent": "Returns a new tensor with the ceil of the elements of `input` , the smallest integer greater than or equal to each element .", "question_id": 1442},
{"snippet": "torch.ceil(input, out=None)", "intent": "Returns a new tensor with the ceil of the elements of `input` , the smallest integer greater than or equal to each element . With arguments `out`.", "question_id": 1443},
{"snippet": "torch.nn.utils.prune.PruningContainer(*args)", "intent": "Container holding a sequence of pruning methods for iterative pruning . With arguments `*args`.", "question_id": 1444},
{"snippet": "pruning_container.add_pruning_method(method)", "intent": "Adds a child pruning `method` to the container .", "question_id": 1445},
{"snippet": "pruning_container.apply(module, name, *args, **kwargs)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`.", "question_id": 1446},
{"snippet": "pruning_container.apply(module, name, *args, **kwargs, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`, `importance_scores`.", "question_id": 1447},
{"snippet": "pruning_container.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 1448},
{"snippet": "pruning_container.compute_mask(t, default_mask)", "intent": "Applies the latest method by computing the new partial masks and returning its combination with the `default_mask` . Which portions of the tensor `t` the new mask will be calculated from depends on the PRUNING_TYPE ( handled by the type handler ) :", "question_id": 1449},
{"snippet": "pruning_container.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 1450},
{"snippet": "pruning_container.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 1451},
{"snippet": "pruning_container.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 1452},
{"snippet": "pruning_container.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 1453},
{"snippet": "pruning_container.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 1454},
{"snippet": "torch.nn.functional.tanhshrink(input)", "intent": "Applies element-wise , Tanhshrink ( x ) =x\u2212Tanh ( x ) \\text { Tanhshrink } ( x ) = x - \\text { Tanh } ( x ) Tanhshrink ( x ) =x\u2212Tanh ( x ) With arguments `input`.", "question_id": 1455},
{"snippet": "torch.nn.ReLU6()", "intent": "Applies the element-wise function :", "question_id": 1456},
{"snippet": "torch.nn.ReLU6(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 1457},
{"snippet": "Tensor.asin_()", "intent": "In-place version of asin ( )", "question_id": 1458},
{"snippet": "torch.nn.CELU()", "intent": "Applies the element-wise function :", "question_id": 1459},
{"snippet": "torch.nn.CELU(alpha=1.0)", "intent": "Applies the element-wise function : With arguments `alpha`.", "question_id": 1460},
{"snippet": "torch.nn.CELU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 1461},
{"snippet": "torch.nn.CELU(alpha=1.0, inplace=False)", "intent": "Applies the element-wise function : With arguments `alpha`, `inplace`.", "question_id": 1462},
{"snippet": "torch.flipud(input)", "intent": "Flip tensor in the up/down direction , returning a new tensor . With arguments `input`.", "question_id": 1463},
{"snippet": "Tensor.logical_xor_()", "intent": "In-place version of logical_xor ( )", "question_id": 1464},
{"snippet": "torch.nn.functional.elu(input)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`.", "question_id": 1465},
{"snippet": "torch.nn.functional.elu(input, alpha=1.0)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `alpha`.", "question_id": 1466},
{"snippet": "torch.nn.functional.elu(input, inplace=False)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `inplace`.", "question_id": 1467},
{"snippet": "torch.nn.functional.elu(input, alpha=1.0, inplace=False)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `alpha`, `inplace`.", "question_id": 1468},
{"snippet": "torch.maximum(input, other)", "intent": "Computes the element-wise maximum of `input` and `other` .", "question_id": 1469},
{"snippet": "torch.maximum(input, other, out=None)", "intent": "Computes the element-wise maximum of `input` and `other` . With arguments `out`.", "question_id": 1470},
{"snippet": "Tensor.coalesce()", "intent": "Returns a coalesced copy of self if self is an uncoalesced tensor .", "question_id": 1471},
{"snippet": "Tensor.reciprocal()", "intent": "See torch.reciprocal ( )", "question_id": 1472},
{"snippet": "torch.isneginf(input)", "intent": "Tests if each element of `input` is negative infinity or not .", "question_id": 1473},
{"snippet": "torch.isneginf(input, out=None)", "intent": "Tests if each element of `input` is negative infinity or not . With arguments `out`.", "question_id": 1474},
{"snippet": "Tensor.mv(vec)", "intent": "See torch.mv ( ) With arguments `vec`.", "question_id": 1475},
{"snippet": "torch.isnan(input)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is NaN or not .", "question_id": 1476},
{"snippet": "Tensor.count_nonzero()", "intent": "See torch.count_nonzero ( )", "question_id": 1477},
{"snippet": "Tensor.count_nonzero(dim=None)", "intent": "See torch.count_nonzero ( ) With arguments `dim`.", "question_id": 1478},
{"snippet": "Tensor.logcumsumexp(dim)", "intent": "See torch.logcumsumexp ( ) With arguments `dim`.", "question_id": 1479},
{"snippet": "torch.remainder(input, other)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the divisor `other` . With arguments `input`.", "question_id": 1480},
{"snippet": "torch.remainder(input, other, out=None)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the divisor `other` . With arguments `input`, `out`.", "question_id": 1481},
{"snippet": "Optimizer.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 1482},
{"snippet": "torch.nn.functional.leaky_relu_(input)", "intent": "In-place version of leaky_relu ( ) . With arguments `input`.", "question_id": 1483},
{"snippet": "torch.nn.functional.leaky_relu_(input, negative_slope=0.01)", "intent": "In-place version of leaky_relu ( ) . With arguments `input`, `negative_slope`.", "question_id": 1484},
{"snippet": "torch.nn.ReplicationPad2d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 1485},
{"snippet": "torch.nn.functional.adaptive_avg_pool1d(input, output_size)", "intent": "Applies a 1D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 1486},
{"snippet": "torch.jit.script_if_tracing(fn)", "intent": "Compiles `fn` when it is first called during tracing .", "question_id": 1487},
{"snippet": "Tensor.transpose(dim0, dim1)", "intent": "See torch.transpose ( ) With arguments `dim0`, `dim1`.", "question_id": 1488},
{"snippet": "Tensor.multiply_(value)", "intent": "In-place version of multiply ( ) . With arguments `value`.", "question_id": 1489},
{"snippet": "torch.jit.freeze(mod)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . With arguments `mod`.", "question_id": 1490},
{"snippet": "torch.jit.freeze(mod, preserved_attrs=None)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . By default , forward will be preserved , as well as attributes & methods specified in `preserved_attrs` . With arguments `mod`.", "question_id": 1491},
{"snippet": "torch.jit.freeze(mod, optimize_numerics=True)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . With arguments `mod`, `optimize_numerics`.", "question_id": 1492},
{"snippet": "torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . By default , forward will be preserved , as well as attributes & methods specified in `preserved_attrs` . With arguments `mod`, `optimize_numerics`.", "question_id": 1493},
{"snippet": "torch.unsqueeze(input, dim)", "intent": "Returns a new tensor with a dimension of size one inserted at the specified position . A `dim` value within the range [ -input.dim ( ) - 1 , input.dim ( ) + 1 ) can be used . With arguments `input`.", "question_id": 1494},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 1495},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 1496},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, padding=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 1497},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, dilation=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 1498},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, groups=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 1499},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 1500},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, scale=1.0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 1501},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, zero_point=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 1502},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 1503},
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 1504},
{"snippet": "torch.addcmul(input, tensor1, tensor2)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 1505},
{"snippet": "torch.addcmul(input, tensor1, tensor2, value=1)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 1506},
{"snippet": "torch.addcmul(input, tensor1, tensor2, out=None)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 1507},
{"snippet": "torch.addcmul(input, tensor1, tensor2, value=1, out=None)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 1508},
{"snippet": "torch.atan(input)", "intent": "Returns a new tensor with the arctangent of the elements of `input` .", "question_id": 1509},
{"snippet": "torch.atan(input, out=None)", "intent": "Returns a new tensor with the arctangent of the elements of `input` . With arguments `out`.", "question_id": 1510},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`.", "question_id": 1511},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`.", "question_id": 1512},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`.", "question_id": 1513},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `device`.", "question_id": 1514},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `dtype`.", "question_id": 1515},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `momentum`.", "question_id": 1516},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `device`.", "question_id": 1517},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `dtype`.", "question_id": 1518},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`, `device`.", "question_id": 1519},
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`, `dtype`.", "question_id": 1520},
{"snippet": "Tensor.q_per_channel_zero_points()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns a tensor of zero_points of the underlying quantizer .", "question_id": 1521},
{"snippet": "torch.bitwise_and(input, other)", "intent": "Computes the bitwise AND of `input` and `other` .", "question_id": 1522},
{"snippet": "torch.bitwise_and(input, other, out=None)", "intent": "Computes the bitwise AND of `input` and `other` . With arguments `out`.", "question_id": 1523},
{"snippet": "Tensor.rsqrt()", "intent": "See torch.rsqrt ( )", "question_id": 1524},
{"snippet": "torch.sign(input)", "intent": "Returns a new tensor with the signs of the elements of `input` .", "question_id": 1525},
{"snippet": "torch.sign(input, out=None)", "intent": "Returns a new tensor with the signs of the elements of `input` . With arguments `out`.", "question_id": 1526},
{"snippet": "torch.linalg.svdvals(A)", "intent": "Computes the singular values of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 1527},
{"snippet": "torch.linalg.svdvals(A, out=None)", "intent": "Computes the singular values of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 1528},
{"snippet": "torch.cuda.StreamContext(stream)", "intent": "Context-manager that selects a given `stream` .", "question_id": 1529},
{"snippet": "Tensor.erfc()", "intent": "See torch.erfc ( )", "question_id": 1530},
{"snippet": "torch.fft.fft(input)", "intent": "Computes the one dimensional discrete Fourier transform of `input` .", "question_id": 1531},
{"snippet": "torch.fft.fft(input, n=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`.", "question_id": 1532},
{"snippet": "torch.fft.fft(input, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 1533},
{"snippet": "torch.fft.fft(input, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 1534},
{"snippet": "torch.fft.fft(input, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `out`.", "question_id": 1535},
{"snippet": "torch.fft.fft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `dim`.", "question_id": 1536},
{"snippet": "torch.fft.fft(input, n=None, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `norm`.", "question_id": 1537},
{"snippet": "torch.fft.fft(input, n=None, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `out`.", "question_id": 1538},
{"snippet": "torch.fft.fft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 1539},
{"snippet": "torch.fft.fft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 1540},
{"snippet": "Tensor.sum()", "intent": "See torch.sum ( )", "question_id": 1541},
{"snippet": "Tensor.sum(dim=None)", "intent": "See torch.sum ( ) With arguments `dim`.", "question_id": 1542},
{"snippet": "Tensor.sum(keepdim=False)", "intent": "See torch.sum ( ) With arguments `keepdim`.", "question_id": 1543},
{"snippet": "Tensor.sum(dtype=None)", "intent": "See torch.sum ( ) With arguments `dtype`.", "question_id": 1544},
{"snippet": "Tensor.sum(dim=None, keepdim=False)", "intent": "See torch.sum ( ) With arguments `dim`, `keepdim`.", "question_id": 1545},
{"snippet": "Tensor.sum(dim=None, dtype=None)", "intent": "See torch.sum ( ) With arguments `dim`, `dtype`.", "question_id": 1546},
{"snippet": "Tensor.sum(keepdim=False, dtype=None)", "intent": "See torch.sum ( ) With arguments `keepdim`, `dtype`.", "question_id": 1547},
{"snippet": "Tensor.sum(dim=None, keepdim=False, dtype=None)", "intent": "See torch.sum ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 1548},
{"snippet": "torch.nn.quantized.ELU(scale, zero_point)", "intent": "This is the quantized equivalent of ELU . With arguments `scale`, `zero_point`.", "question_id": 1549},
{"snippet": "torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)", "intent": "This is the quantized equivalent of ELU . With arguments `scale`, `zero_point`, `alpha`.", "question_id": 1550},
{"snippet": "torch.hstack(tensors)", "intent": "Stack `tensors` in sequence horizontally ( column wise ) .", "question_id": 1551},
{"snippet": "torch.hstack(tensors, out=None)", "intent": "Stack `tensors` in sequence horizontally ( column wise ) . With arguments `out`.", "question_id": 1552},
{"snippet": "Tensor.mul(value)", "intent": "See torch.mul ( ) . With arguments `value`.", "question_id": 1553},
{"snippet": "torch.nn.functional.hardshrink(input)", "intent": "Applies the hard shrinkage function element-wise With arguments `input`.", "question_id": 1554},
{"snippet": "torch.nn.functional.hardshrink(input, lambd=0.5)", "intent": "Applies the hard shrinkage function element-wise With arguments `input`, `lambd`.", "question_id": 1555},
{"snippet": "torch.nn.functional.gumbel_softmax(logits)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`.", "question_id": 1556},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`.", "question_id": 1557},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`.", "question_id": 1558},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `eps`.", "question_id": 1559},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `dim`.", "question_id": 1560},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `hard`.", "question_id": 1561},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `eps`.", "question_id": 1562},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `dim`.", "question_id": 1563},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`, `eps`.", "question_id": 1564},
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`, `dim`.", "question_id": 1565},
{"snippet": "torch.bitwise_xor(input, other)", "intent": "Computes the bitwise XOR of `input` and `other` .", "question_id": 1566},
{"snippet": "torch.bitwise_xor(input, other, out=None)", "intent": "Computes the bitwise XOR of `input` and `other` . With arguments `out`.", "question_id": 1567},
{"snippet": "torch.imag(input)", "intent": "Returns a new tensor containing imaginary values of the self tensor . With arguments `input`.", "question_id": 1568},
{"snippet": "torch.trunc(input)", "intent": "Returns a new tensor with the truncated integer values of the elements of `input` .", "question_id": 1569},
{"snippet": "torch.trunc(input, out=None)", "intent": "Returns a new tensor with the truncated integer values of the elements of `input` . With arguments `out`.", "question_id": 1570},
{"snippet": "torch.jit.trace_module(mod, inputs)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`.", "question_id": 1571},
{"snippet": "torch.jit.trace_module(mod, inputs, optimize=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `optimize`.", "question_id": 1572},
{"snippet": "torch.jit.trace_module(mod, inputs, check_trace=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_trace`.", "question_id": 1573},
{"snippet": "torch.jit.trace_module(mod, inputs, check_inputs=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_inputs`.", "question_id": 1574},
{"snippet": "torch.jit.trace_module(mod, inputs, check_tolerance=1e-05)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_tolerance`.", "question_id": 1575},
{"snippet": "torch.jit.trace_module(mod, inputs, strict=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `strict`.", "question_id": 1576},
{"snippet": "torch.jit.trace_module(mod, inputs, _force_outplace=False)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_force_outplace`.", "question_id": 1577},
{"snippet": "torch.jit.trace_module(mod, inputs, _module_class=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_module_class`.", "question_id": 1578},
{"snippet": "torch.jit.trace_module(mod, inputs, _compilation_unit=<torch.jit.CompilationUnit object>)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_compilation_unit`.", "question_id": 1579},
{"snippet": "torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `optimize`, `check_trace`.", "question_id": 1580},
{"snippet": "torch.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers .", "question_id": 1581},
{"snippet": "Tensor.flatten(input)", "intent": "see torch.flatten ( ) With arguments `input`.", "question_id": 1582},
{"snippet": "Tensor.flatten(input, start_dim=0)", "intent": "see torch.flatten ( ) With arguments `input`, `start_dim`.", "question_id": 1583},
{"snippet": "Tensor.flatten(input, end_dim=- 1)", "intent": "see torch.flatten ( ) With arguments `input`, `end_dim`.", "question_id": 1584},
{"snippet": "Tensor.flatten(input, start_dim=0, end_dim=- 1)", "intent": "see torch.flatten ( ) With arguments `input`, `start_dim`, `end_dim`.", "question_id": 1585},
{"snippet": "Tensor.sort()", "intent": "See torch.sort ( )", "question_id": 1586},
{"snippet": "Tensor.sort(dim=- 1)", "intent": "See torch.sort ( ) With arguments `dim`.", "question_id": 1587},
{"snippet": "Tensor.sort(descending=False)", "intent": "See torch.sort ( ) With arguments `descending`.", "question_id": 1588},
{"snippet": "Tensor.sort(dim=- 1, descending=False)", "intent": "See torch.sort ( ) With arguments `dim`, `descending`.", "question_id": 1589},
{"snippet": "torch.nn.TransformerEncoder(encoder_layer, num_layers)", "intent": "TransformerEncoder is a stack of N encoder layers With arguments `encoder_layer`, `num_layers`.", "question_id": 1590},
{"snippet": "torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)", "intent": "TransformerEncoder is a stack of N encoder layers With arguments `encoder_layer`, `num_layers`, `norm`.", "question_id": 1591},
{"snippet": "transformer_encoder.forward(src)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`.", "question_id": 1592},
{"snippet": "transformer_encoder.forward(src, mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `mask`.", "question_id": 1593},
{"snippet": "transformer_encoder.forward(src, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `src_key_padding_mask`.", "question_id": 1594},
{"snippet": "transformer_encoder.forward(src, mask=None, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `mask`, `src_key_padding_mask`.", "question_id": 1595},
{"snippet": "torch.ormqr(input, tau, other)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) .", "question_id": 1596},
{"snippet": "torch.ormqr(input, tau, other, left=True)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) .", "question_id": 1597},
{"snippet": "torch.ormqr(input, tau, other, transpose=False)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op .", "question_id": 1598},
{"snippet": "torch.ormqr(input, tau, other, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . With arguments `out`.", "question_id": 1599},
{"snippet": "torch.ormqr(input, tau, other, left=True, transpose=False)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op .", "question_id": 1600},
{"snippet": "torch.ormqr(input, tau, other, left=True, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . With arguments `out`.", "question_id": 1601},
{"snippet": "torch.ormqr(input, tau, other, transpose=False, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op . With arguments `out`.", "question_id": 1602},
{"snippet": "torch.ormqr(input, tau, other, left=True, transpose=False, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op . With arguments `out`.", "question_id": 1603},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 1604},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 1605},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 1606},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 1607},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 1608},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 1609},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 1610},
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 1611},
{"snippet": "torch.fft.ihfft(input)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain .", "question_id": 1612},
{"snippet": "torch.fft.ihfft(input, n=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`.", "question_id": 1613},
{"snippet": "torch.fft.ihfft(input, dim=- 1)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`.", "question_id": 1614},
{"snippet": "torch.fft.ihfft(input, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `norm`.", "question_id": 1615},
{"snippet": "torch.fft.ihfft(input, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `out`.", "question_id": 1616},
{"snippet": "torch.fft.ihfft(input, n=None, dim=- 1)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `dim`.", "question_id": 1617},
{"snippet": "torch.fft.ihfft(input, n=None, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `norm`.", "question_id": 1618},
{"snippet": "torch.fft.ihfft(input, n=None, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `out`.", "question_id": 1619},
{"snippet": "torch.fft.ihfft(input, dim=- 1, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`, `norm`.", "question_id": 1620},
{"snippet": "torch.fft.ihfft(input, dim=- 1, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`, `out`.", "question_id": 1621},
{"snippet": "torch.ldexp(input, other)", "intent": "Multiplies `input` by 2 * * : attr : `other` .", "question_id": 1622},
{"snippet": "torch.ldexp(input, other, out=None)", "intent": "Multiplies `input` by 2 * * : attr : `other` . With arguments `out`.", "question_id": 1623},
{"snippet": "torch.negative(input)", "intent": "Alias for torch.neg ( ) With arguments `input`.", "question_id": 1624},
{"snippet": "torch.negative(input, out=None)", "intent": "Alias for torch.neg ( ) With arguments `input`, `out`.", "question_id": 1625},
{"snippet": "Tensor.where(condition, y)", "intent": "self.where ( `condition` , `y` ) is equivalent to torch.where ( condition , self , y ) .", "question_id": 1626},
{"snippet": "torch.hann_window(window_length)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 1627},
{"snippet": "torch.hann_window(window_length, periodic=True)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 1628},
{"snippet": "torch.hann_window(window_length, dtype=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 1629},
{"snippet": "torch.hann_window(window_length, layout=torch.strided)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 1630},
{"snippet": "torch.hann_window(window_length, device=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 1631},
{"snippet": "torch.hann_window(window_length, requires_grad=False)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 1632},
{"snippet": "torch.hann_window(window_length, periodic=True, dtype=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 1633},
{"snippet": "torch.hann_window(window_length, periodic=True, layout=torch.strided)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 1634},
{"snippet": "torch.hann_window(window_length, periodic=True, device=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 1635},
{"snippet": "torch.hann_window(window_length, periodic=True, requires_grad=False)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 1636},
{"snippet": "Tensor.addcdiv_(tensor1, tensor2)", "intent": "In-place version of addcdiv ( ) With arguments `tensor1`, `tensor2`.", "question_id": 1637},
{"snippet": "Tensor.addcdiv_(tensor1, tensor2, value=1)", "intent": "In-place version of addcdiv ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 1638},
{"snippet": "torch.nn.Hardshrink()", "intent": "Applies the hard shrinkage function element-wise :", "question_id": 1639},
{"snippet": "torch.nn.Hardshrink(lambd=0.5)", "intent": "Applies the hard shrinkage function element-wise : With arguments `lambd`.", "question_id": 1640},
{"snippet": "torch.matrix_power(input, n)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `input`, `n`.", "question_id": 1641},
{"snippet": "torch.matrix_power(input, n, out=None)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `input`, `n`, `out`.", "question_id": 1642},
{"snippet": "torch.stack(tensors)", "intent": "Concatenates a sequence of `tensors` along a new dimension .", "question_id": 1643},
{"snippet": "torch.stack(tensors, dim=0)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `dim`.", "question_id": 1644},
{"snippet": "torch.stack(tensors, out=None)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `out`.", "question_id": 1645},
{"snippet": "torch.stack(tensors, dim=0, out=None)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `dim`, `out`.", "question_id": 1646},
{"snippet": "torch.nn.UpsamplingBilinear2d()", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels .", "question_id": 1647},
{"snippet": "torch.nn.UpsamplingBilinear2d(size=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 1648},
{"snippet": "torch.nn.UpsamplingBilinear2d(scale_factor=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 1649},
{"snippet": "torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 1650},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss()", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) .", "question_id": 1651},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 1652},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 1653},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . With arguments `swap`.", "question_id": 1654},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 1655},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 1656},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . With arguments `swap`.", "question_id": 1657},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 1658},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0, swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . With arguments `swap`.", "question_id": 1659},
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0, reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 1660},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) .", "question_id": 1661},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`.", "question_id": 1662},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_pivots=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_pivots`.", "question_id": 1663},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `out`.", "question_id": 1664},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `unpack_pivots`.", "question_id": 1665},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `out`.", "question_id": 1666},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_pivots=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_pivots`, `out`.", "question_id": 1667},
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `unpack_pivots`, `out`.", "question_id": 1668},
{"snippet": "torch.nn.InstanceNorm2d(num_features)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 1669},
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 1670},
{"snippet": "torch.nn.InstanceNorm2d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 1671},
{"snippet": "torch.nn.InstanceNorm2d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 1672},
{"snippet": "torch.nn.InstanceNorm2d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 1673},
{"snippet": "torch.nn.InstanceNorm2d(num_features, device=None)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 1674},
{"snippet": "torch.nn.InstanceNorm2d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 1675},
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 1676},
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 1677},
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 1678},
{"snippet": "torch.fft.irfftn(input)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) .", "question_id": 1679},
{"snippet": "torch.fft.irfftn(input, s=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` .", "question_id": 1680},
{"snippet": "torch.fft.irfftn(input, dim=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`.", "question_id": 1681},
{"snippet": "torch.fft.irfftn(input, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `norm`.", "question_id": 1682},
{"snippet": "torch.fft.irfftn(input, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `out`.", "question_id": 1683},
{"snippet": "torch.fft.irfftn(input, s=None, dim=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `dim`.", "question_id": 1684},
{"snippet": "torch.fft.irfftn(input, s=None, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `norm`.", "question_id": 1685},
{"snippet": "torch.fft.irfftn(input, s=None, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `out`.", "question_id": 1686},
{"snippet": "torch.fft.irfftn(input, dim=None, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`, `norm`.", "question_id": 1687},
{"snippet": "torch.fft.irfftn(input, dim=None, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`, `out`.", "question_id": 1688},
{"snippet": "Tensor.resize_(*sizes)", "intent": "Resizes self tensor to the specified size . With arguments `*sizes`.", "question_id": 1689},
{"snippet": "Tensor.resize_(*sizes, memory_format=torch.contiguous_format)", "intent": "Resizes self tensor to the specified size . With arguments `*sizes`, `memory_format`.", "question_id": 1690},
{"snippet": "torch.cuda.max_memory_cached()", "intent": "Deprecated ; see max_memory_reserved ( ) .", "question_id": 1691},
{"snippet": "torch.cuda.max_memory_cached(device=None)", "intent": "Deprecated ; see max_memory_reserved ( ) . With arguments `device`.", "question_id": 1692},
{"snippet": "torch.nn.LazyBatchNorm2d()", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) .", "question_id": 1693},
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 1694},
{"snippet": "torch.nn.LazyBatchNorm2d(momentum=0.1)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 1695},
{"snippet": "torch.nn.LazyBatchNorm2d(affine=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 1696},
{"snippet": "torch.nn.LazyBatchNorm2d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 1697},
{"snippet": "torch.nn.LazyBatchNorm2d(device=None)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 1698},
{"snippet": "torch.nn.LazyBatchNorm2d(dtype=None)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 1699},
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 1700},
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 1701},
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 1702},
{"snippet": "lazy_batch_norm2d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm2d", "question_id": 1703},
{"snippet": "torch.xlogy(input, other)", "intent": "Computes `input` * log ( `other` ) with the following cases .", "question_id": 1704},
{"snippet": "torch.xlogy(input, other, out=None)", "intent": "Computes `input` * log ( `other` ) with the following cases . With arguments `out`.", "question_id": 1705},
{"snippet": "Tensor.log_()", "intent": "In-place version of log ( )", "question_id": 1706},
{"snippet": "torch.autograd.functional.jacobian(func, inputs)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`.", "question_id": 1707},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`.", "question_id": 1708},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, strict=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `strict`.", "question_id": 1709},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `vectorize`.", "question_id": 1710},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `strict`.", "question_id": 1711},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `vectorize`.", "question_id": 1712},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, strict=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `strict`, `vectorize`.", "question_id": 1713},
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `strict`, `vectorize`.", "question_id": 1714},
{"snippet": "torch.quantization.observer.get_observer_state_dict(mod)", "intent": "Returns the state dict corresponding to the observer stats . With arguments `mod`.", "question_id": 1715},
{"snippet": "torch.autograd.backward(tensors)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves .", "question_id": 1716},
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` .", "question_id": 1717},
{"snippet": "torch.autograd.backward(tensors, retain_graph=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `retain_graph`.", "question_id": 1718},
{"snippet": "torch.autograd.backward(tensors, create_graph=False)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `create_graph`.", "question_id": 1719},
{"snippet": "torch.autograd.backward(tensors, grad_variables=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `grad_variables`.", "question_id": 1720},
{"snippet": "torch.autograd.backward(tensors, inputs=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `inputs`.", "question_id": 1721},
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `retain_graph`.", "question_id": 1722},
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, create_graph=False)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `create_graph`.", "question_id": 1723},
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, grad_variables=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `grad_variables`.", "question_id": 1724},
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, inputs=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `inputs`.", "question_id": 1725},
{"snippet": "torch.polar(abs, angle)", "intent": "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value `abs` and `angle` angle .", "question_id": 1726},
{"snippet": "torch.polar(abs, angle, out=None)", "intent": "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value `abs` and `angle` angle . With arguments `out`.", "question_id": 1727},
{"snippet": "torch.nn.Transformer()", "intent": "A transformer model .", "question_id": 1728},
{"snippet": "torch.nn.Transformer(d_model=512)", "intent": "A transformer model . With arguments `d_model`.", "question_id": 1729},
{"snippet": "torch.nn.Transformer(nhead=8)", "intent": "A transformer model . With arguments `nhead`.", "question_id": 1730},
{"snippet": "torch.nn.Transformer(num_encoder_layers=6)", "intent": "A transformer model . With arguments `num_encoder_layers`.", "question_id": 1731},
{"snippet": "torch.nn.Transformer(num_decoder_layers=6)", "intent": "A transformer model . With arguments `num_decoder_layers`.", "question_id": 1732},
{"snippet": "torch.nn.Transformer(dim_feedforward=2048)", "intent": "A transformer model . With arguments `dim_feedforward`.", "question_id": 1733},
{"snippet": "torch.nn.Transformer(dropout=0.1)", "intent": "A transformer model . With arguments `dropout`.", "question_id": 1734},
{"snippet": "torch.nn.Transformer(activation='relu')", "intent": "A transformer model . With arguments `activation`.", "question_id": 1735},
{"snippet": "torch.nn.Transformer(custom_encoder=None)", "intent": "A transformer model . With arguments `custom_encoder`.", "question_id": 1736},
{"snippet": "torch.nn.Transformer(custom_decoder=None)", "intent": "A transformer model . With arguments `custom_decoder`.", "question_id": 1737},
{"snippet": "transformer.forward(src, tgt)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`.", "question_id": 1738},
{"snippet": "transformer.forward(src, tgt, src_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`.", "question_id": 1739},
{"snippet": "transformer.forward(src, tgt, tgt_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `tgt_mask`.", "question_id": 1740},
{"snippet": "transformer.forward(src, tgt, memory_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `memory_mask`.", "question_id": 1741},
{"snippet": "transformer.forward(src, tgt, src_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_key_padding_mask`.", "question_id": 1742},
{"snippet": "transformer.forward(src, tgt, tgt_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `tgt_key_padding_mask`.", "question_id": 1743},
{"snippet": "transformer.forward(src, tgt, memory_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `memory_key_padding_mask`.", "question_id": 1744},
{"snippet": "transformer.forward(src, tgt, src_mask=None, tgt_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `tgt_mask`.", "question_id": 1745},
{"snippet": "transformer.forward(src, tgt, src_mask=None, memory_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `memory_mask`.", "question_id": 1746},
{"snippet": "transformer.forward(src, tgt, src_mask=None, src_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `src_key_padding_mask`.", "question_id": 1747},
{"snippet": "transformer.generate_square_subsequent_mask(sz)", "intent": "Generate a square mask for the sequence . With arguments `sz`.", "question_id": 1748},
{"snippet": "Tensor.unsqueeze_(dim)", "intent": "In-place version of unsqueeze ( ) With arguments `dim`.", "question_id": 1749},
{"snippet": "torch.fmin(input, other)", "intent": "Computes the element-wise minimum of `input` and `other` .", "question_id": 1750},
{"snippet": "torch.fmin(input, other, out=None)", "intent": "Computes the element-wise minimum of `input` and `other` . With arguments `out`.", "question_id": 1751},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 1752},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 1753},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 1754},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 1755},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 1756},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 1757},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 1758},
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 1759},
{"snippet": "Tensor.flipud()", "intent": "See torch.flipud ( )", "question_id": 1760},
{"snippet": "torch.acos(input)", "intent": "Computes the inverse cosine of each element in `input` .", "question_id": 1761},
{"snippet": "torch.acos(input, out=None)", "intent": "Computes the inverse cosine of each element in `input` . With arguments `out`.", "question_id": 1762},
{"snippet": "torch.cuda.memory_snapshot()", "intent": "Returns a snapshot of the CUDA memory allocator state across all devices .", "question_id": 1763},
{"snippet": "torch.cuda.set_per_process_memory_fraction(fraction)", "intent": "Set memory `fraction` for a process .", "question_id": 1764},
{"snippet": "torch.cuda.set_per_process_memory_fraction(fraction, device=None)", "intent": "Set memory `fraction` for a process . The fraction is used to limit an caching allocator to allocated memory on a CUDA `device` .", "question_id": 1765},
{"snippet": "torch.cholesky_solve(input, input2)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . With arguments `input`, `input2`.", "question_id": 1766},
{"snippet": "torch.cholesky_solve(input, input2, upper=False)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . If `upper` is False , uuu is and lower triangular and c is returned such that : With arguments `input`, `input2`.", "question_id": 1767},
{"snippet": "torch.cholesky_solve(input, input2, out=None)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . With arguments `input`, `input2`, `out`.", "question_id": 1768},
{"snippet": "torch.cholesky_solve(input, input2, upper=False, out=None)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . If `upper` is False , uuu is and lower triangular and c is returned such that : With arguments `input`, `input2`, `out`.", "question_id": 1769},
{"snippet": "Tensor.allclose(other)", "intent": "See torch.allclose ( ) With arguments `other`.", "question_id": 1770},
{"snippet": "Tensor.allclose(other, rtol=1e-05)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`.", "question_id": 1771},
{"snippet": "Tensor.allclose(other, atol=1e-08)", "intent": "See torch.allclose ( ) With arguments `other`, `atol`.", "question_id": 1772},
{"snippet": "Tensor.allclose(other, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `equal_nan`.", "question_id": 1773},
{"snippet": "Tensor.allclose(other, rtol=1e-05, atol=1e-08)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `atol`.", "question_id": 1774},
{"snippet": "Tensor.allclose(other, rtol=1e-05, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `equal_nan`.", "question_id": 1775},
{"snippet": "Tensor.allclose(other, atol=1e-08, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `atol`, `equal_nan`.", "question_id": 1776},
{"snippet": "Tensor.allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `atol`, `equal_nan`.", "question_id": 1777},
{"snippet": "torch.linalg.det(A)", "intent": "Computes the determinant of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 1778},
{"snippet": "torch.linalg.det(A, out=None)", "intent": "Computes the determinant of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 1779},
{"snippet": "Tensor.logical_or_()", "intent": "In-place version of logical_or ( )", "question_id": 1780},
{"snippet": "torch.linalg.solve(A, B)", "intent": "Computes the solution of a square system of linear equations with a unique solution . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb { K } ^ { n \\times k } X\u2208Kn\u00d7k of the linear system associated to A\u2208Kn\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { n \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Kn\u00d7n , B\u2208Km\u00d7k , which is defined as With arguments `A`.", "question_id": 1781},
{"snippet": "torch.linalg.solve(A, B, out=None)", "intent": "Computes the solution of a square system of linear equations with a unique solution . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb { K } ^ { n \\times k } X\u2208Kn\u00d7k of the linear system associated to A\u2208Kn\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { n \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Kn\u00d7n , B\u2208Km\u00d7k , which is defined as With arguments `A`, `out`.", "question_id": 1782},
{"snippet": "torch.nn.SiLU()", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise .", "question_id": 1783},
{"snippet": "torch.nn.SiLU(inplace=False)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `inplace`.", "question_id": 1784},
{"snippet": "torch.nn.functional.alpha_dropout(input)", "intent": "Applies alpha dropout to the `input` .", "question_id": 1785},
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5)", "intent": "Applies alpha dropout to the `input` . With arguments `p`.", "question_id": 1786},
{"snippet": "torch.nn.functional.alpha_dropout(input, training=False)", "intent": "Applies alpha dropout to the `input` . With arguments `training`.", "question_id": 1787},
{"snippet": "torch.nn.functional.alpha_dropout(input, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `inplace`.", "question_id": 1788},
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, training=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `training`.", "question_id": 1789},
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `inplace`.", "question_id": 1790},
{"snippet": "torch.nn.functional.alpha_dropout(input, training=False, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `training`, `inplace`.", "question_id": 1791},
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `training`, `inplace`.", "question_id": 1792},
{"snippet": "torch.tril(input)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 1793},
{"snippet": "torch.tril(input, diagonal=0)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The lower triangular part of the matrix is defined as the elements on and below the `diagonal` .", "question_id": 1794},
{"snippet": "torch.tril(input, out=None)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 1795},
{"snippet": "torch.tril(input, diagonal=0, out=None)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The lower triangular part of the matrix is defined as the elements on and below the `diagonal` .", "question_id": 1796},
{"snippet": "torch.diag_embed(input)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 1797},
{"snippet": "torch.diag_embed(input, offset=0)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 1798},
{"snippet": "torch.diag_embed(input, dim1=- 2)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 1799},
{"snippet": "torch.diag_embed(input, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 1800},
{"snippet": "torch.diag_embed(input, offset=0, dim1=- 2)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 1801},
{"snippet": "torch.diag_embed(input, offset=0, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 1802},
{"snippet": "torch.diag_embed(input, dim1=- 2, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 1803},
{"snippet": "torch.diag_embed(input, offset=0, dim1=- 2, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 1804},
{"snippet": "Tensor.is_leaf", "intent": "All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "question_id": 1805},
{"snippet": "torch.flip(input, dims)", "intent": "Reverse the order of a n-D tensor along given axis in `dims` . With arguments `input`.", "question_id": 1806},
{"snippet": "Tensor.apply_(callable)", "intent": "Applies the function `callable` to each element in the tensor , replacing each element with the value returned by callable .", "question_id": 1807},
{"snippet": "torch.add(input, other)", "intent": "Adds the scalar `other` to each element of the `input` input and returns a new resulting tensor .", "question_id": 1808},
{"snippet": "torch.add(input, other, out=None)", "intent": "Adds the scalar `other` to each element of the `input` input and returns a new resulting tensor . With arguments `out`.", "question_id": 1809},
{"snippet": "torch.multiply(input, other)", "intent": "Alias for torch.mul ( ) . With arguments `input`, `other`.", "question_id": 1810},
{"snippet": "torch.multiply(input, other, out=None)", "intent": "Alias for torch.mul ( ) . With arguments `input`, `other`, `out`.", "question_id": 1811},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`.", "question_id": 1812},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`.", "question_id": 1813},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`.", "question_id": 1814},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `verbose`.", "question_id": 1815},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`.", "question_id": 1816},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `verbose`.", "question_id": 1817},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 1818},
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 1819},
{"snippet": "multi_step_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 1820},
{"snippet": "multi_step_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 1821},
{"snippet": "multi_step_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 1822},
{"snippet": "multi_step_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 1823},
{"snippet": "multi_step_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 1824},
{"snippet": "Tensor.scatter_add(dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_add_ ( ) With arguments `dim`, `index`, `src`.", "question_id": 1825},
{"snippet": "Tensor.movedim(source, destination)", "intent": "See torch.movedim ( ) With arguments `source`, `destination`.", "question_id": 1826},
{"snippet": "torch.cat(tensors)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension .", "question_id": 1827},
{"snippet": "torch.cat(tensors, dim=0)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `dim`.", "question_id": 1828},
{"snippet": "torch.cat(tensors, out=None)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `out`.", "question_id": 1829},
{"snippet": "torch.cat(tensors, dim=0, out=None)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `dim`, `out`.", "question_id": 1830},
{"snippet": "torch.cuda.current_blas_handle()", "intent": "Returns cublasHandle_t pointer to current cuBLAS handle", "question_id": 1831},
{"snippet": "Tensor.log10()", "intent": "See torch.log10 ( )", "question_id": 1832},
{"snippet": "torch.fft.ifftshift(input)", "intent": "Inverse of fftshift ( ) . With arguments `input`.", "question_id": 1833},
{"snippet": "torch.fft.ifftshift(input, dim=None)", "intent": "Inverse of fftshift ( ) . With arguments `input`, `dim`.", "question_id": 1834},
{"snippet": "Tensor.gt_(other)", "intent": "In-place version of gt ( ) . With arguments `other`.", "question_id": 1835},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 1836},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 1837},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 1838},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 1839},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 1840},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 1841},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 1842},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 1843},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 1844},
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 1845},
{"snippet": "torch.set_warn_always(b)", "intent": "When this flag is False ( default ) then some PyTorch warnings may only appear once per process . With arguments `b`.", "question_id": 1846},
{"snippet": "torch.exp(input)", "intent": "Returns a new tensor with the exponential of the elements of the `input` tensor input .", "question_id": 1847},
{"snippet": "torch.exp(input, out=None)", "intent": "Returns a new tensor with the exponential of the elements of the `input` tensor input . With arguments `out`.", "question_id": 1848},
{"snippet": "Tensor.lerp(end, weight)", "intent": "See torch.lerp ( ) With arguments `end`, `weight`.", "question_id": 1849},
{"snippet": "torch.nn.utils.prune.BasePruningMethod", "intent": "Abstract base class for creation of new pruning techniques.", "question_id": 1850},
{"snippet": "base_pruning_method.apply(module, name, *args, **kwargs)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`.", "question_id": 1851},
{"snippet": "base_pruning_method.apply(module, name, *args, **kwargs, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`, `importance_scores`.", "question_id": 1852},
{"snippet": "base_pruning_method.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 1853},
{"snippet": "base_pruning_method.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a random mask to apply on top of the default_mask according to the specific pruning method recipe . With arguments `t`.", "question_id": 1854},
{"snippet": "base_pruning_method.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 1855},
{"snippet": "base_pruning_method.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 1856},
{"snippet": "base_pruning_method.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 1857},
{"snippet": "base_pruning_method.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 1858},
{"snippet": "base_pruning_method.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 1859},
{"snippet": "torch.linalg.eigh(A)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 1860},
{"snippet": "torch.linalg.eigh(A, UPLO='L')", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`.", "question_id": 1861},
{"snippet": "torch.linalg.eigh(A, out=None)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 1862},
{"snippet": "torch.linalg.eigh(A, UPLO='L', out=None)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`, `out`.", "question_id": 1863},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`.", "question_id": 1864},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`.", "question_id": 1865},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`.", "question_id": 1866},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `swap`.", "question_id": 1867},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `reduction`.", "question_id": 1868},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, margin=1.0)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `margin`.", "question_id": 1869},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `swap`.", "question_id": 1870},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `reduction`.", "question_id": 1871},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`, `swap`.", "question_id": 1872},
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`, `reduction`.", "question_id": 1873},
{"snippet": "torch.take_along_dim(input, indices, dim)", "intent": "Selects values from `input` at the 1-dimensional `indices` from indices along the given `dim` .", "question_id": 1874},
{"snippet": "torch.take_along_dim(input, indices, dim, out=None)", "intent": "Selects values from `input` at the 1-dimensional `indices` from indices along the given `dim` . With arguments `out`.", "question_id": 1875},
{"snippet": "Tensor.ge(other)", "intent": "See torch.ge ( ) . With arguments `other`.", "question_id": 1876},
{"snippet": "Tensor.cumsum(dim)", "intent": "See torch.cumsum ( ) With arguments `dim`.", "question_id": 1877},
{"snippet": "Tensor.cumsum(dim, dtype=None)", "intent": "See torch.cumsum ( ) With arguments `dim`, `dtype`.", "question_id": 1878},
{"snippet": "Tensor.addmm(mat1, mat2)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`.", "question_id": 1879},
{"snippet": "Tensor.addmm(mat1, mat2, beta=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 1880},
{"snippet": "Tensor.addmm(mat1, mat2, alpha=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 1881},
{"snippet": "Tensor.addmm(mat1, mat2, beta=1, alpha=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 1882},
{"snippet": "Tensor.take_along_dim(indices, dim)", "intent": "See torch.take_along_dim ( ) With arguments `indices`, `dim`.", "question_id": 1883},
{"snippet": "torch.max(input)", "intent": "Returns the maximum value of all elements in the `input` tensor .", "question_id": 1884},
{"snippet": "Tensor.remainder_(divisor)", "intent": "In-place version of remainder ( ) With arguments `divisor`.", "question_id": 1885},
{"snippet": "torch.cuda.device_count()", "intent": "Returns the number of GPUs available .", "question_id": 1886},
{"snippet": "Tensor.repeat(*sizes)", "intent": "Repeats this tensor along the specified dimensions . With arguments `*sizes`.", "question_id": 1887},
{"snippet": "torch.randn(*size)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`.", "question_id": 1888},
{"snippet": "torch.randn(*size, out=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`.", "question_id": 1889},
{"snippet": "torch.randn(*size, dtype=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `dtype`.", "question_id": 1890},
{"snippet": "torch.randn(*size, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `layout`.", "question_id": 1891},
{"snippet": "torch.randn(*size, device=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `device`.", "question_id": 1892},
{"snippet": "torch.randn(*size, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `requires_grad`.", "question_id": 1893},
{"snippet": "torch.randn(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `dtype`.", "question_id": 1894},
{"snippet": "torch.randn(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `layout`.", "question_id": 1895},
{"snippet": "torch.randn(*size, out=None, device=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `device`.", "question_id": 1896},
{"snippet": "torch.randn(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `requires_grad`.", "question_id": 1897},
{"snippet": "torch.cos(input)", "intent": "Returns a new tensor with the cosine of the elements of `input` .", "question_id": 1898},
{"snippet": "torch.cos(input, out=None)", "intent": "Returns a new tensor with the cosine of the elements of `input` . With arguments `out`.", "question_id": 1899},
{"snippet": "Tensor.floor_divide_(value)", "intent": "In-place version of floor_divide ( ) With arguments `value`.", "question_id": 1900},
{"snippet": "torch.nn.functional.tanh(input)", "intent": "Applies element-wise , Tanh ( x ) =tanh\u2061 ( x ) =exp\u2061 ( x ) \u2212exp\u2061 ( \u2212x ) exp\u2061 ( x ) +exp\u2061 ( \u2212x ) \\text { Tanh } ( x ) = \\tanh ( x ) = \\frac { \\exp ( x ) - \\exp ( -x ) } { \\exp ( x ) + \\exp ( -x ) } Tanh ( x ) =tanh ( x ) =exp ( x ) +exp ( \u2212x ) exp ( x ) \u2212exp ( \u2212x ) \u200b With arguments `input`.", "question_id": 1901},
{"snippet": "torch.nn.quantized.functional.adaptive_avg_pool2d(input, output_size)", "intent": "Applies a 2D adaptive average pooling over a quantized `input` signal composed of several quantized input planes . With arguments `output_size`.", "question_id": 1902},
{"snippet": "torch.le(input, other)", "intent": "Computes input\u2264other\\text { `input` } \\leq \\text { `other` } input\u2264other element-wise .", "question_id": 1903},
{"snippet": "torch.le(input, other, out=None)", "intent": "Computes input\u2264other\\text { `input` } \\leq \\text { `other` } input\u2264other element-wise . With arguments `out`.", "question_id": 1904},
{"snippet": "Tensor.trace()", "intent": "See torch.trace ( )", "question_id": 1905},
{"snippet": "torch.atleast_1d(*tensors)", "intent": "Returns a 1-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 1906},
{"snippet": "torch.mvlgamma(input, p)", "intent": "Computes the multivariate log-gamma function ) with dimension ppp element-wise , given by where C=log\u2061 ( \u03c0 ) \u00d7p ( p\u22121 ) 4C = \\log ( \\pi ) \\times \\frac { `p` ( p - 1 ) } { 4 } C=log ( \u03c0 ) \u00d74p ( p\u22121 ) \u200b and \u0393 ( \u22c5 ) \\Gamma ( \\cdot ) \u0393 ( \u22c5 ) is the Gamma function . With arguments `input`.", "question_id": 1907},
{"snippet": "Tensor.set_()", "intent": "Sets the underlying storage , `size` , and strides .", "question_id": 1908},
{"snippet": "Tensor.set_(source=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source .", "question_id": 1909},
{"snippet": "Tensor.set_(storage_offset=0)", "intent": "Sets the underlying storage , `size` , and strides . With arguments `storage_offset`.", "question_id": 1910},
{"snippet": "Tensor.set_(size=None)", "intent": "Sets the underlying storage , `size` , and strides .", "question_id": 1911},
{"snippet": "Tensor.set_(stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` .", "question_id": 1912},
{"snippet": "Tensor.set_(source=None, storage_offset=0)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source . With arguments `storage_offset`.", "question_id": 1913},
{"snippet": "Tensor.set_(source=None, size=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source .", "question_id": 1914},
{"snippet": "Tensor.set_(source=None, stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` .", "question_id": 1915},
{"snippet": "Tensor.set_(storage_offset=0, size=None)", "intent": "Sets the underlying storage , `size` , and strides . With arguments `storage_offset`.", "question_id": 1916},
{"snippet": "Tensor.set_(storage_offset=0, stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` . With arguments `storage_offset`.", "question_id": 1917},
{"snippet": "torch.nn.SELU()", "intent": "Applied element-wise , as :", "question_id": 1918},
{"snippet": "torch.nn.SELU(inplace=False)", "intent": "Applied element-wise , as : With arguments `inplace`.", "question_id": 1919},
{"snippet": "Tensor.masked_select(mask)", "intent": "See torch.masked_select ( ) With arguments `mask`.", "question_id": 1920},
{"snippet": "Tensor.numel()", "intent": "See torch.numel ( )", "question_id": 1921},
{"snippet": "torch.nn.intrinsic.ConvBn2d(conv, bn)", "intent": "This is a sequential container which calls the Conv 2d and Batch Norm 2d modules . With arguments `conv`, `bn`.", "question_id": 1922},
{"snippet": "Tensor.isfinite()", "intent": "See torch.isfinite ( )", "question_id": 1923},
{"snippet": "torch.allclose(input, other)", "intent": "This function checks if all `input` and `other` satisfy the condition :", "question_id": 1924},
{"snippet": "torch.allclose(input, other, rtol=1e-05)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`.", "question_id": 1925},
{"snippet": "torch.allclose(input, other, atol=1e-08)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `atol`.", "question_id": 1926},
{"snippet": "torch.allclose(input, other, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `equal_nan`.", "question_id": 1927},
{"snippet": "torch.allclose(input, other, rtol=1e-05, atol=1e-08)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `atol`.", "question_id": 1928},
{"snippet": "torch.allclose(input, other, rtol=1e-05, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `equal_nan`.", "question_id": 1929},
{"snippet": "torch.allclose(input, other, atol=1e-08, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `atol`, `equal_nan`.", "question_id": 1930},
{"snippet": "torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `atol`, `equal_nan`.", "question_id": 1931},
{"snippet": "torch.nn.utils.weight_norm(module)", "intent": "Applies weight normalization to a parameter in the given `module` .", "question_id": 1932},
{"snippet": "torch.nn.utils.weight_norm(module, name='weight')", "intent": "Applies weight normalization to a parameter in the given `module` . This replaces the parameter specified by `name` ( e.g .", "question_id": 1933},
{"snippet": "torch.nn.utils.weight_norm(module, dim=0)", "intent": "Applies weight normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 1934},
{"snippet": "torch.nn.utils.weight_norm(module, name='weight', dim=0)", "intent": "Applies weight normalization to a parameter in the given `module` . This replaces the parameter specified by `name` ( e.g . With arguments `dim`.", "question_id": 1935},
{"snippet": "Tensor.matrix_power(n)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `n`.", "question_id": 1936},
{"snippet": "Tensor.q_per_channel_axis()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns the index of dimension on which per-channel quantization is applied .", "question_id": 1937},
{"snippet": "Tensor.maximum(other)", "intent": "See torch.maximum ( ) With arguments `other`.", "question_id": 1938},
{"snippet": "Tensor.to(*args, **kwargs)", "intent": "Performs Tensor dtype and/or device conversion . With arguments `*args`, `**kwargs`.", "question_id": 1939},
{"snippet": "to(dtype)", "intent": "Returns a Tensor with the specified `dtype`", "question_id": 1940},
{"snippet": "to(dtype, non_blocking=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`.", "question_id": 1941},
{"snippet": "to(dtype, copy=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `copy`.", "question_id": 1942},
{"snippet": "to(dtype, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `memory_format`.", "question_id": 1943},
{"snippet": "to(dtype, non_blocking=False, copy=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `copy`.", "question_id": 1944},
{"snippet": "to(dtype, non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `memory_format`.", "question_id": 1945},
{"snippet": "to(dtype, copy=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `copy`, `memory_format`.", "question_id": 1946},
{"snippet": "to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `copy`, `memory_format`.", "question_id": 1947},
{"snippet": "torch.to()", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 1948},
{"snippet": "torch.to(device=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 1949},
{"snippet": "torch.to(dtype=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 1950},
{"snippet": "torch.to(non_blocking=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 1951},
{"snippet": "torch.to(copy=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 1952},
{"snippet": "torch.to(memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . With arguments `memory_format`.", "question_id": 1953},
{"snippet": "torch.to(device=None, dtype=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 1954},
{"snippet": "torch.to(device=None, non_blocking=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 1955},
{"snippet": "torch.to(device=None, copy=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 1956},
{"snippet": "torch.to(device=None, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . With arguments `memory_format`.", "question_id": 1957},
{"snippet": "torch.to(other)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` .", "question_id": 1958},
{"snippet": "torch.to(other, non_blocking=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 1959},
{"snippet": "torch.to(other, copy=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 1960},
{"snippet": "torch.to(other, non_blocking=False, copy=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 1961},
{"snippet": "Tensor.sparse_mask(mask)", "intent": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor `mask` .", "question_id": 1962},
{"snippet": "Tensor.detach()", "intent": "Returns a new Tensor , detached from the current graph .", "question_id": 1963},
{"snippet": "Tensor.sinh_()", "intent": "In-place version of sinh ( )", "question_id": 1964},
{"snippet": "torch.vdot(input, other)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`.", "question_id": 1965},
{"snippet": "torch.vdot(input, other, out=None)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`, `out`.", "question_id": 1966},
{"snippet": "torch.Generator()", "intent": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers .", "question_id": 1967},
{"snippet": "torch.Generator(device='cpu')", "intent": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers . With arguments `device`.", "question_id": 1968},
{"snippet": "generator.device", "intent": "Generator.device -> device", "question_id": 1969},
{"snippet": "generator.get_state()", "intent": "Returns the Generator state as a torch.ByteTensor .", "question_id": 1970},
{"snippet": "generator.initial_seed()", "intent": "Returns the initial seed for generating random numbers .", "question_id": 1971},
{"snippet": "generator.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers .", "question_id": 1972},
{"snippet": "generator.seed()", "intent": "Gets a non-deterministic random number from std : :random_device or the current time and uses it to seed a Generator .", "question_id": 1973},
{"snippet": "generator.set_state(new_state)", "intent": "Sets the Generator state . With arguments `new_state`.", "question_id": 1974},
{"snippet": "Tensor.pow_(exponent)", "intent": "In-place version of pow ( ) With arguments `exponent`.", "question_id": 1975},
{"snippet": "Tensor.lt_(other)", "intent": "In-place version of lt ( ) . With arguments `other`.", "question_id": 1976},
{"snippet": "torch.cuda.reset_max_memory_allocated()", "intent": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given `device` .", "question_id": 1977},
{"snippet": "torch.cuda.reset_max_memory_allocated(device=None)", "intent": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given `device` .", "question_id": 1978},
{"snippet": "torch.nn.functional.threshold_(input, threshold, value)", "intent": "In-place version of `threshold` ( ) . With arguments `input`, `value`.", "question_id": 1979},
{"snippet": "torch.addmv(input, mat, vec)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result .", "question_id": 1980},
{"snippet": "torch.addmv(input, mat, vec, beta=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 1981},
{"snippet": "torch.addmv(input, mat, vec, alpha=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 1982},
{"snippet": "torch.addmv(input, mat, vec, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 1983},
{"snippet": "torch.addmv(input, mat, vec, beta=1, alpha=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 1984},
{"snippet": "torch.addmv(input, mat, vec, beta=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 1985},
{"snippet": "torch.addmv(input, mat, vec, alpha=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 1986},
{"snippet": "torch.addmv(input, mat, vec, beta=1, alpha=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 1987},
{"snippet": "Tensor.ldexp_(other)", "intent": "In-place version of ldexp ( ) With arguments `other`.", "question_id": 1988},
{"snippet": "torch.nanquantile(input, q)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist .", "question_id": 1989},
{"snippet": "torch.nanquantile(input, q, dim=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`.", "question_id": 1990},
{"snippet": "torch.nanquantile(input, q, keepdim=False)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `keepdim`.", "question_id": 1991},
{"snippet": "torch.nanquantile(input, q, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `out`.", "question_id": 1992},
{"snippet": "torch.nanquantile(input, q, dim=None, keepdim=False)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `keepdim`.", "question_id": 1993},
{"snippet": "torch.nanquantile(input, q, dim=None, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `out`.", "question_id": 1994},
{"snippet": "torch.nanquantile(input, q, keepdim=False, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `keepdim`, `out`.", "question_id": 1995},
{"snippet": "torch.nanquantile(input, q, dim=None, keepdim=False, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `keepdim`, `out`.", "question_id": 1996},
{"snippet": "torch.jit.Attribute(value, type)", "intent": "This method is a pass-through function that returns `value` , mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with `type` of type .", "question_id": 1997},
{"snippet": "attribute.count(value, /)", "intent": "Return number of occurrences of `value` . With arguments `/`.", "question_id": 1998},
{"snippet": "attribute.index(value, /)", "intent": "Return first index of `value` . With arguments `/`.", "question_id": 1999},
{"snippet": "attribute.index(value, /, start=0)", "intent": "Return first index of `value` . With arguments `/`, `start`.", "question_id": 2000},
{"snippet": "attribute.index(value, /, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `stop`.", "question_id": 2001},
{"snippet": "attribute.index(value, /, start=0, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `start`, `stop`.", "question_id": 2002},
{"snippet": "attribute.type", "intent": "Alias for field number 1", "question_id": 2003},
{"snippet": "attribute.value", "intent": "Alias for field number 0", "question_id": 2004},
{"snippet": "torch.nansum(input)", "intent": "Returns the sum of all elements , treating Not a Numbers ( NaNs ) as zero . Returns the sum of each row of the `input` tensor in the given dimension dim , treating Not a Numbers ( NaNs ) as zero .", "question_id": 2005},
{"snippet": "torch.nansum(input, dtype=None)", "intent": "Returns the sum of all elements , treating Not a Numbers ( NaNs ) as zero . Returns the sum of each row of the `input` tensor in the given dimension dim , treating Not a Numbers ( NaNs ) as zero . With arguments `dtype`.", "question_id": 2006},
{"snippet": "Tensor.acos_()", "intent": "In-place version of acos ( )", "question_id": 2007},
{"snippet": "torch.linalg.norm(A)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( )", "question_id": 2008},
{"snippet": "torch.linalg.norm(A, ord=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed .", "question_id": 2009},
{"snippet": "torch.linalg.norm(A, dim=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) Using the `dim` argument to compute vector norms :", "question_id": 2010},
{"snippet": "torch.linalg.norm(A, keepdim=False)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `keepdim`.", "question_id": 2011},
{"snippet": "torch.linalg.norm(A, out=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `out`.", "question_id": 2012},
{"snippet": "torch.linalg.norm(A, dtype=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dtype`.", "question_id": 2013},
{"snippet": "torch.linalg.norm(A, ord=None, dim=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . Using the `dim` argument to compute vector norms :", "question_id": 2014},
{"snippet": "torch.linalg.norm(A, ord=None, keepdim=False)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `keepdim`.", "question_id": 2015},
{"snippet": "torch.linalg.norm(A, ord=None, out=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `out`.", "question_id": 2016},
{"snippet": "torch.linalg.norm(A, ord=None, dtype=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `dtype`.", "question_id": 2017},
{"snippet": "Tensor.addr(vec1, vec2)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`.", "question_id": 2018},
{"snippet": "Tensor.addr(vec1, vec2, beta=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `beta`.", "question_id": 2019},
{"snippet": "Tensor.addr(vec1, vec2, alpha=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `alpha`.", "question_id": 2020},
{"snippet": "Tensor.addr(vec1, vec2, beta=1, alpha=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `beta`, `alpha`.", "question_id": 2021},
{"snippet": "torch.quantization.observer.HistogramObserver()", "intent": "The module records the running histogram of tensor values along with min/max values .", "question_id": 2022},
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`.", "question_id": 2023},
{"snippet": "torch.quantization.observer.HistogramObserver(upsample_rate=128)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `upsample_rate`.", "question_id": 2024},
{"snippet": "torch.quantization.observer.HistogramObserver(dtype=torch.quint8)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `dtype`.", "question_id": 2025},
{"snippet": "torch.quantization.observer.HistogramObserver(qscheme=torch.per_tensor_affine)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `qscheme`.", "question_id": 2026},
{"snippet": "torch.quantization.observer.HistogramObserver(reduce_range=False)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `reduce_range`.", "question_id": 2027},
{"snippet": "torch.quantization.observer.HistogramObserver(factory_kwargs=None)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `factory_kwargs`.", "question_id": 2028},
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, upsample_rate=128)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `upsample_rate`.", "question_id": 2029},
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, dtype=torch.quint8)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `dtype`.", "question_id": 2030},
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, qscheme=torch.per_tensor_affine)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `qscheme`.", "question_id": 2031},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 2032},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`.", "question_id": 2033},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`.", "question_id": 2034},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 2035},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `_random_samples`.", "question_id": 2036},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `output_ratio`.", "question_id": 2037},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `return_indices`.", "question_id": 2038},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `_random_samples`.", "question_id": 2039},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `return_indices`.", "question_id": 2040},
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `_random_samples`.", "question_id": 2041},
{"snippet": "torch.cuda.get_gencode_flags()", "intent": "Returns NVCC gencode flags this library was compiled with .", "question_id": 2042},
{"snippet": "Tensor.vdot(other)", "intent": "See torch.vdot ( ) With arguments `other`.", "question_id": 2043},
{"snippet": "torch.nn.functional.adaptive_max_pool3d(*args, **kwargs)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 2044},
{"snippet": "profile.total_average()", "intent": "Averages all events .", "question_id": 2045},
{"snippet": "torch.nn.quantized.Sigmoid(output_scale, output_zero_point)", "intent": "This is the quantized equivalent of Sigmoid . With arguments `output_scale`, `output_zero_point`.", "question_id": 2046},
{"snippet": "torch.logspace(start, end, steps)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base .", "question_id": 2047},
{"snippet": "torch.logspace(start, end, steps, base=10.0)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base .", "question_id": 2048},
{"snippet": "torch.logspace(start, end, steps, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `out`.", "question_id": 2049},
{"snippet": "torch.logspace(start, end, steps, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `dtype`.", "question_id": 2050},
{"snippet": "torch.logspace(start, end, steps, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `layout`.", "question_id": 2051},
{"snippet": "torch.logspace(start, end, steps, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `device`.", "question_id": 2052},
{"snippet": "torch.logspace(start, end, steps, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `requires_grad`.", "question_id": 2053},
{"snippet": "torch.logspace(start, end, steps, base=10.0, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `out`.", "question_id": 2054},
{"snippet": "torch.logspace(start, end, steps, base=10.0, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `dtype`.", "question_id": 2055},
{"snippet": "torch.logspace(start, end, steps, base=10.0, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `layout`.", "question_id": 2056},
{"snippet": "torch.nn.parameter.UninitializedParameter()", "intent": "A parameter that is not initialized .", "question_id": 2057},
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True)", "intent": "A parameter that is not initialized . With arguments `requires_grad`.", "question_id": 2058},
{"snippet": "torch.nn.parameter.UninitializedParameter(device=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter .", "question_id": 2059},
{"snippet": "torch.nn.parameter.UninitializedParameter(dtype=None)", "intent": "A parameter that is not initialized . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g .", "question_id": 2060},
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . With arguments `requires_grad`.", "question_id": 2061},
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, dtype=None)", "intent": "A parameter that is not initialized . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 2062},
{"snippet": "torch.nn.parameter.UninitializedParameter(device=None, dtype=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g .", "question_id": 2063},
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None, dtype=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 2064},
{"snippet": "uninitialized_parameter.cls_to_become", "intent": "alias of torch.nn.parameter.Parameter", "question_id": 2065},
{"snippet": "torch.nn.HingeEmbeddingLoss()", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) .", "question_id": 2066},
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`.", "question_id": 2067},
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 2068},
{"snippet": "torch.nn.HingeEmbeddingLoss(reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 2069},
{"snippet": "torch.nn.HingeEmbeddingLoss(reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 2070},
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `size_average`.", "question_id": 2071},
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduce`.", "question_id": 2072},
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduction`.", "question_id": 2073},
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None, reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 2074},
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None, reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 2075},
{"snippet": "Tensor.sparse_resize_and_clear_(size, sparse_dim, dense_dim)", "intent": "Removes all specified elements from a sparse tensor self and resizes self to the desired `size` and the number of sparse and dense dimensions . With arguments `sparse_dim`, `dense_dim`.", "question_id": 2076},
{"snippet": "torch.normal(mean, std)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution", "question_id": 2077},
{"snippet": "torch.normal(mean, std, generator=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `generator`.", "question_id": 2078},
{"snippet": "torch.normal(mean, std, out=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `out`.", "question_id": 2079},
{"snippet": "torch.normal(mean, std, generator=None, out=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `generator`, `out`.", "question_id": 2080},
{"snippet": "Tensor.sum_to_size(*size)", "intent": "Sum this tensor to size . With arguments `*size`.", "question_id": 2081},
{"snippet": "torch.nn.functional.relu(input)", "intent": "Applies the rectified linear unit function element-wise . With arguments `input`.", "question_id": 2082},
{"snippet": "torch.nn.functional.relu(input, inplace=False)", "intent": "Applies the rectified linear unit function element-wise . With arguments `input`, `inplace`.", "question_id": 2083},
{"snippet": "torch.linalg.eigvals(A)", "intent": "Computes the eigenvalues of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 2084},
{"snippet": "torch.linalg.eigvals(A, out=None)", "intent": "Computes the eigenvalues of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 2085},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 2086},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 2087},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 2088},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 2089},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 2090},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 2091},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 2092},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 2093},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 2094},
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 2095},
{"snippet": "conv1d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 2096},
{"snippet": "Tensor.isinf()", "intent": "See torch.isinf ( )", "question_id": 2097},
{"snippet": "Tensor.neg_()", "intent": "In-place version of neg ( )", "question_id": 2098},
{"snippet": "Tensor.matrix_exp()", "intent": "See torch.matrix_exp ( )", "question_id": 2099},
{"snippet": "torch.randint(high, size, \\*)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`.", "question_id": 2100},
{"snippet": "torch.randint(high, size, \\*, low=0)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`.", "question_id": 2101},
{"snippet": "torch.randint(high, size, \\*, generator=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `generator`.", "question_id": 2102},
{"snippet": "torch.randint(high, size, \\*, out=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `out`.", "question_id": 2103},
{"snippet": "torch.randint(high, size, \\*, dtype=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `dtype`.", "question_id": 2104},
{"snippet": "torch.randint(high, size, \\*, layout=torch.strided)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `layout`.", "question_id": 2105},
{"snippet": "torch.randint(high, size, \\*, device=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `device`.", "question_id": 2106},
{"snippet": "torch.randint(high, size, \\*, requires_grad=False)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `requires_grad`.", "question_id": 2107},
{"snippet": "torch.randint(high, size, \\*, low=0, generator=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `generator`.", "question_id": 2108},
{"snippet": "torch.randint(high, size, \\*, low=0, out=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `out`.", "question_id": 2109},
{"snippet": "torch.linalg.matrix_rank(A)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 2110},
{"snippet": "torch.linalg.matrix_rank(A, tol=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold .", "question_id": 2111},
{"snippet": "torch.linalg.matrix_rank(A, hermitian=False)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`.", "question_id": 2112},
{"snippet": "torch.linalg.matrix_rank(A, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 2113},
{"snippet": "torch.linalg.matrix_rank(A, tol=None, hermitian=False)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `hermitian`.", "question_id": 2114},
{"snippet": "torch.linalg.matrix_rank(A, tol=None, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `out`.", "question_id": 2115},
{"snippet": "torch.linalg.matrix_rank(A, hermitian=False, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`, `out`.", "question_id": 2116},
{"snippet": "torch.linalg.matrix_rank(A, tol=None, hermitian=False, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `hermitian`, `out`.", "question_id": 2117},
{"snippet": "torch.lobpcg(A)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 2118},
{"snippet": "torch.lobpcg(A, k=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 2119},
{"snippet": "torch.lobpcg(A, B=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `B`.", "question_id": 2120},
{"snippet": "torch.lobpcg(A, X=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `X`.", "question_id": 2121},
{"snippet": "torch.lobpcg(A, n=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `n`.", "question_id": 2122},
{"snippet": "torch.lobpcg(A, iK=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `iK`.", "question_id": 2123},
{"snippet": "torch.lobpcg(A, niter=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `niter`.", "question_id": 2124},
{"snippet": "torch.lobpcg(A, tol=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `tol`.", "question_id": 2125},
{"snippet": "torch.lobpcg(A, largest=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 2126},
{"snippet": "torch.lobpcg(A, method=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . This function is a front-end to the following LOBPCG algorithms selectable via `method` argument :", "question_id": 2127},
{"snippet": "Tensor.topk(k)", "intent": "See torch.topk ( ) With arguments `k`.", "question_id": 2128},
{"snippet": "Tensor.topk(k, dim=None)", "intent": "See torch.topk ( ) With arguments `k`, `dim`.", "question_id": 2129},
{"snippet": "Tensor.topk(k, largest=True)", "intent": "See torch.topk ( ) With arguments `k`, `largest`.", "question_id": 2130},
{"snippet": "Tensor.topk(k, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `sorted`.", "question_id": 2131},
{"snippet": "Tensor.topk(k, dim=None, largest=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `largest`.", "question_id": 2132},
{"snippet": "Tensor.topk(k, dim=None, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `sorted`.", "question_id": 2133},
{"snippet": "Tensor.topk(k, largest=True, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `largest`, `sorted`.", "question_id": 2134},
{"snippet": "Tensor.topk(k, dim=None, largest=True, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `largest`, `sorted`.", "question_id": 2135},
{"snippet": "torch.optim.LBFGS(params)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`.", "question_id": 2136},
{"snippet": "torch.optim.LBFGS(params, lr=1)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`.", "question_id": 2137},
{"snippet": "torch.optim.LBFGS(params, max_iter=20)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `max_iter`.", "question_id": 2138},
{"snippet": "torch.optim.LBFGS(params, max_eval=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `max_eval`.", "question_id": 2139},
{"snippet": "torch.optim.LBFGS(params, tolerance_grad=1e-07)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `tolerance_grad`.", "question_id": 2140},
{"snippet": "torch.optim.LBFGS(params, tolerance_change=1e-09)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `tolerance_change`.", "question_id": 2141},
{"snippet": "torch.optim.LBFGS(params, history_size=100)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `history_size`.", "question_id": 2142},
{"snippet": "torch.optim.LBFGS(params, line_search_fn=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `line_search_fn`.", "question_id": 2143},
{"snippet": "torch.optim.LBFGS(params, lr=1, max_iter=20)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`, `max_iter`.", "question_id": 2144},
{"snippet": "torch.optim.LBFGS(params, lr=1, max_eval=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`, `max_eval`.", "question_id": 2145},
{"snippet": "lbfgs.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 2146},
{"snippet": "lbfgs.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 2147},
{"snippet": "lbfgs.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 2148},
{"snippet": "lbfgs.step(closure)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 2149},
{"snippet": "lbfgs.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 2150},
{"snippet": "lbfgs.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 2151},
{"snippet": "torch.nn.Dropout()", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 2152},
{"snippet": "torch.nn.Dropout(p=0.5)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 2153},
{"snippet": "torch.nn.Dropout(inplace=False)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 2154},
{"snippet": "torch.nn.Dropout(p=0.5, inplace=False)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 2155},
{"snippet": "Tensor.reciprocal_()", "intent": "In-place version of reciprocal ( )", "question_id": 2156},
{"snippet": "torch.logdet(input)", "intent": "Calculates log determinant of a square matrix or batches of square matrices . With arguments `input`.", "question_id": 2157},
{"snippet": "Tensor.erfinv()", "intent": "See torch.erfinv ( )", "question_id": 2158},
{"snippet": "torch.cuda.comm.broadcast(tensor)", "intent": "Broadcasts a `tensor` to specified GPU `devices` .", "question_id": 2159},
{"snippet": "torch.cuda.comm.broadcast(tensor, devices=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` .", "question_id": 2160},
{"snippet": "torch.cuda.comm.broadcast(tensor, out=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` . With arguments `out`.", "question_id": 2161},
{"snippet": "torch.cuda.comm.broadcast(tensor, devices=None, out=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` . With arguments `out`.", "question_id": 2162},
{"snippet": "torch.outer(input, vec2)", "intent": "Outer product of `input` and `vec2` .", "question_id": 2163},
{"snippet": "torch.outer(input, vec2, out=None)", "intent": "Outer product of `input` and `vec2` . If input is a vector of size nnn and vec2 is a vector of size mmm , then `out` must be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 2164},
{"snippet": "torch.nn.functional.upsample(input)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 2165},
{"snippet": "torch.nn.functional.upsample(input, size=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 2166},
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 2167},
{"snippet": "torch.nn.functional.upsample(input, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 2168},
{"snippet": "torch.nn.functional.upsample(input, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 2169},
{"snippet": "torch.nn.functional.upsample(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 2170},
{"snippet": "torch.nn.functional.upsample(input, size=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 2171},
{"snippet": "torch.nn.functional.upsample(input, size=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 2172},
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 2173},
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 2174},
{"snippet": "Tensor.ger(vec2)", "intent": "See torch.ger ( ) With arguments `vec2`.", "question_id": 2175},
{"snippet": "torch.jit.script(obj)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`.", "question_id": 2176},
{"snippet": "torch.jit.script(obj, optimize=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`.", "question_id": 2177},
{"snippet": "torch.jit.script(obj, _frames_up=0)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_frames_up`.", "question_id": 2178},
{"snippet": "torch.jit.script(obj, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_rcb`.", "question_id": 2179},
{"snippet": "torch.jit.script(obj, optimize=None, _frames_up=0)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_frames_up`.", "question_id": 2180},
{"snippet": "torch.jit.script(obj, optimize=None, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_rcb`.", "question_id": 2181},
{"snippet": "torch.jit.script(obj, _frames_up=0, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_frames_up`, `_rcb`.", "question_id": 2182},
{"snippet": "torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_frames_up`, `_rcb`.", "question_id": 2183},
{"snippet": "torch.mv(input, vec)", "intent": "Performs a matrix-vector product of the matrix `input` and the vector `vec` .", "question_id": 2184},
{"snippet": "torch.mv(input, vec, out=None)", "intent": "Performs a matrix-vector product of the matrix `input` and the vector `vec` . If input is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size mmm , `out` will be 1-D of size nnn .", "question_id": 2185},
{"snippet": "torch.sparse.mm(mat1, mat2)", "intent": "Performs a matrix multiplication of the sparse matrix `mat1` and the ( sparse or strided ) matrix `mat2` .", "question_id": 2186},
{"snippet": "torch.mul(input, other)", "intent": "Multiplies each element of the `input` input with the scalar `other` and returns a new resulting tensor .", "question_id": 2187},
{"snippet": "torch.mul(input, other, out=None)", "intent": "Multiplies each element of the `input` input with the scalar `other` and returns a new resulting tensor . With arguments `out`.", "question_id": 2188},
{"snippet": "Tensor.index_copy(tensor1, dim, index, tensor2)", "intent": "Out-of-place version of torch.Tensor.index_copy_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_copy_ ( ) . With arguments `dim`, `index`, `tensor2`.", "question_id": 2189},
{"snippet": "Tensor.argmin()", "intent": "See torch.argmin ( )", "question_id": 2190},
{"snippet": "Tensor.argmin(dim=None)", "intent": "See torch.argmin ( ) With arguments `dim`.", "question_id": 2191},
{"snippet": "Tensor.argmin(keepdim=False)", "intent": "See torch.argmin ( ) With arguments `keepdim`.", "question_id": 2192},
{"snippet": "Tensor.argmin(dim=None, keepdim=False)", "intent": "See torch.argmin ( ) With arguments `dim`, `keepdim`.", "question_id": 2193},
{"snippet": "torch.fft.irfft2(input, - 1))", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`.", "question_id": 2194},
{"snippet": "torch.fft.irfft2(input, - 1), s=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`.", "question_id": 2195},
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`.", "question_id": 2196},
{"snippet": "torch.fft.irfft2(input, - 1), norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `norm`.", "question_id": 2197},
{"snippet": "torch.fft.irfft2(input, - 1), out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `out`.", "question_id": 2198},
{"snippet": "torch.fft.irfft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `dim`.", "question_id": 2199},
{"snippet": "torch.fft.irfft2(input, - 1), s=None, norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `norm`.", "question_id": 2200},
{"snippet": "torch.fft.irfft2(input, - 1), s=None, out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `out`.", "question_id": 2201},
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`, `norm`.", "question_id": 2202},
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`, `out`.", "question_id": 2203},
{"snippet": "Tensor.is_floating_point()", "intent": "Returns True if the data type of self is a floating point data type .", "question_id": 2204},
{"snippet": "Tensor.tolist()", "intent": "Returns the tensor as a ( nested ) list .", "question_id": 2205},
{"snippet": "Tensor.type(**kwargs)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`.", "question_id": 2206},
{"snippet": "Tensor.type(**kwargs, dtype=None)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`.", "question_id": 2207},
{"snippet": "Tensor.type(**kwargs, non_blocking=False)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`, `non_blocking`.", "question_id": 2208},
{"snippet": "Tensor.type(**kwargs, dtype=None, non_blocking=False)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`, `non_blocking`.", "question_id": 2209},
{"snippet": "Tensor.logical_or()", "intent": "See torch.logical_or ( )", "question_id": 2210},
{"snippet": "torch.argmax(input)", "intent": "Returns the indices of the maximum value of all elements in the `input` tensor .", "question_id": 2211},
{"snippet": "torch.scatter_add(input, dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_add_ ( ) With arguments `input`, `dim`, `index`, `src`.", "question_id": 2212},
{"snippet": "torch.nn.quantized.QFunctional", "intent": "Wrapper class for quantized operations.", "question_id": 2213},
{"snippet": "torch.quantization.observer.load_observer_state_dict(mod, obs_dict)", "intent": "Given input model and a state_dict containing model observer stats , load the stats back into the model . With arguments `mod`, `obs_dict`.", "question_id": 2214},
{"snippet": "torch.nn.MaxPool2d(kernel_size)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as :", "question_id": 2215},
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be :", "question_id": 2216},
{"snippet": "torch.nn.MaxPool2d(kernel_size, padding=0)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 2217},
{"snippet": "torch.nn.MaxPool2d(kernel_size, dilation=1)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : `dilation` controls the spacing between the kernel points .", "question_id": 2218},
{"snippet": "torch.nn.MaxPool2d(kernel_size, return_indices=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `return_indices`.", "question_id": 2219},
{"snippet": "torch.nn.MaxPool2d(kernel_size, ceil_mode=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 2220},
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 2221},
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : `dilation` controls the spacing between the kernel points .", "question_id": 2222},
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `return_indices`.", "question_id": 2223},
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `ceil_mode`.", "question_id": 2224},
{"snippet": "Tensor.exponential_()", "intent": "Fills self tensor with elements drawn from the exponential distribution :", "question_id": 2225},
{"snippet": "Tensor.exponential_(lambd=1)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `lambd`.", "question_id": 2226},
{"snippet": "Tensor.exponential_(generator=None)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `generator`.", "question_id": 2227},
{"snippet": "Tensor.exponential_(lambd=1, generator=None)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `lambd`, `generator`.", "question_id": 2228},
{"snippet": "torch.get_default_dtype()", "intent": "Get the current default floating point torch.dtype .", "question_id": 2229},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs)", "intent": "Simulate the quantize and dequantize operations in training time . With arguments `**observer_kwargs`.", "question_id": 2230},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>)", "intent": "Simulate the quantize and dequantize operations in training time . With arguments `**observer_kwargs`, `observer`.", "question_id": 2231},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_min=0)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 2232},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 2233},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 2234},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 2235},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_min=0, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 2236},
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 2237},
{"snippet": "torch.nn.intrinsic.ConvBnReLU3d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 3d , Batch Norm 3d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 2238},
{"snippet": "torch.nn.utils.parametrize.ParametrizationList(modules, original)", "intent": "A sequential container that holds and manages the `original` parameter or buffer of a parametrized torch.nn.Module . With arguments `modules`.", "question_id": 2239},
{"snippet": "parametrization_list.set_original_(value)", "intent": "This method is called when assigning to a parametrized tensor . With arguments `value`.", "question_id": 2240},
{"snippet": "torch.fmod(input, other)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the dividend `input` . With arguments `other`.", "question_id": 2241},
{"snippet": "torch.fmod(input, other, out=None)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the dividend `input` . With arguments `other`, `out`.", "question_id": 2242},
{"snippet": "Tensor.median()", "intent": "See torch.median ( )", "question_id": 2243},
{"snippet": "Tensor.median(dim=None)", "intent": "See torch.median ( ) With arguments `dim`.", "question_id": 2244},
{"snippet": "Tensor.median(keepdim=False)", "intent": "See torch.median ( ) With arguments `keepdim`.", "question_id": 2245},
{"snippet": "Tensor.median(dim=None, keepdim=False)", "intent": "See torch.median ( ) With arguments `dim`, `keepdim`.", "question_id": 2246},
{"snippet": "torch.quantization.observer.RecordingObserver(**kwargs)", "intent": "The module is mainly for debug and records the tensor values during runtime . With arguments `**kwargs`.", "question_id": 2247},
{"snippet": "torch.diagonal(input)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 2248},
{"snippet": "torch.diagonal(input, offset=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 2249},
{"snippet": "torch.diagonal(input, dim1=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 2250},
{"snippet": "torch.diagonal(input, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 2251},
{"snippet": "torch.diagonal(input, offset=0, dim1=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 2252},
{"snippet": "torch.diagonal(input, offset=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 2253},
{"snippet": "torch.diagonal(input, dim1=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 2254},
{"snippet": "torch.diagonal(input, offset=0, dim1=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 2255},
{"snippet": "torch.cuda.init()", "intent": "Initialize PyTorch \u2019 s CUDA state .", "question_id": 2256},
{"snippet": "torch.nn.functional.cross_entropy(input, target)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`.", "question_id": 2257},
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`.", "question_id": 2258},
{"snippet": "torch.nn.functional.cross_entropy(input, target, size_average=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `size_average`.", "question_id": 2259},
{"snippet": "torch.nn.functional.cross_entropy(input, target, ignore_index=- 100)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `ignore_index`.", "question_id": 2260},
{"snippet": "torch.nn.functional.cross_entropy(input, target, reduce=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `reduce`.", "question_id": 2261},
{"snippet": "torch.nn.functional.cross_entropy(input, target, reduction='mean')", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `reduction`.", "question_id": 2262},
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 2263},
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, ignore_index=- 100)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `ignore_index`.", "question_id": 2264},
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, reduce=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `reduce`.", "question_id": 2265},
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, reduction='mean')", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `reduction`.", "question_id": 2266},
{"snippet": "Tensor.arctan()", "intent": "See torch.arctan ( )", "question_id": 2267},
{"snippet": "torch.clip(input)", "intent": "Alias for torch.clamp ( ) . With arguments `input`.", "question_id": 2268},
{"snippet": "torch.clip(input, min=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`.", "question_id": 2269},
{"snippet": "torch.clip(input, max=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `max`.", "question_id": 2270},
{"snippet": "torch.clip(input, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `out`.", "question_id": 2271},
{"snippet": "torch.clip(input, min=None, max=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `max`.", "question_id": 2272},
{"snippet": "torch.clip(input, min=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `out`.", "question_id": 2273},
{"snippet": "torch.clip(input, max=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `max`, `out`.", "question_id": 2274},
{"snippet": "torch.clip(input, min=None, max=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `max`, `out`.", "question_id": 2275},
{"snippet": "torch.nn.functional.pixel_unshuffle(input, downscale_factor)", "intent": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) to a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) , where r is the `downscale_factor` . With arguments `input`.", "question_id": 2276},
{"snippet": "torch.ne(input, other)", "intent": "Computes input\u2260other\\text { `input` } \\neq \\text { `other` } input\ue020=other element-wise .", "question_id": 2277},
{"snippet": "torch.ne(input, other, out=None)", "intent": "Computes input\u2260other\\text { `input` } \\neq \\text { `other` } input\ue020=other element-wise . With arguments `out`.", "question_id": 2278},
{"snippet": "torch.sinc(input)", "intent": "Computes the normalized sinc of `input` .", "question_id": 2279},
{"snippet": "torch.sinc(input, out=None)", "intent": "Computes the normalized sinc of `input` . With arguments `out`.", "question_id": 2280},
{"snippet": "torch.rand(*size)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`.", "question_id": 2281},
{"snippet": "torch.rand(*size, out=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`.", "question_id": 2282},
{"snippet": "torch.rand(*size, dtype=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `dtype`.", "question_id": 2283},
{"snippet": "torch.rand(*size, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `layout`.", "question_id": 2284},
{"snippet": "torch.rand(*size, device=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `device`.", "question_id": 2285},
{"snippet": "torch.rand(*size, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `requires_grad`.", "question_id": 2286},
{"snippet": "torch.rand(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `dtype`.", "question_id": 2287},
{"snippet": "torch.rand(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `layout`.", "question_id": 2288},
{"snippet": "torch.rand(*size, out=None, device=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `device`.", "question_id": 2289},
{"snippet": "torch.rand(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `requires_grad`.", "question_id": 2290},
{"snippet": "torch.nonzero(input)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` .", "question_id": 2291},
{"snippet": "torch.nonzero(input, out=None)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` . If input has nnn dimensions , then the resulting indices tensor `out` is of size ( z\u00d7n ) ( z \\times n ) ( z\u00d7n ) , where zzz is the total number of non-zero elements in the input tensor .", "question_id": 2292},
{"snippet": "torch.nonzero(input, as_tuple=False)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` .", "question_id": 2293},
{"snippet": "torch.nonzero(input, out=None, as_tuple=False)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` . If input has nnn dimensions , then the resulting indices tensor `out` is of size ( z\u00d7n ) ( z \\times n ) ( z\u00d7n ) , where zzz is the total number of non-zero elements in the input tensor .", "question_id": 2294},
{"snippet": "torch.nn.ReplicationPad3d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 2295},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`.", "question_id": 2296},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`.", "question_id": 2297},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, stride=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `stride`.", "question_id": 2298},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `padding`.", "question_id": 2299},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `output_padding`.", "question_id": 2300},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, groups=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `groups`.", "question_id": 2301},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `dilation`.", "question_id": 2302},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `stride`.", "question_id": 2303},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `padding`.", "question_id": 2304},
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `output_padding`.", "question_id": 2305},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`.", "question_id": 2306},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`.", "question_id": 2307},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, padding=0)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `padding`.", "question_id": 2308},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, dilation=1)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `dilation`.", "question_id": 2309},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 2310},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, return_indices=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 2311},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 2312},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, dilation=1)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `dilation`.", "question_id": 2313},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 2314},
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, return_indices=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `return_indices`.", "question_id": 2315},
{"snippet": "Tensor.greater_equal(other)", "intent": "See torch.greater_equal ( ) . With arguments `other`.", "question_id": 2316},
{"snippet": "torch.transpose(input, dim0, dim1)", "intent": "Returns a tensor that is a transposed version of `input` . The given dimensions `dim0` and `dim1` are swapped .", "question_id": 2317},
{"snippet": "torch.get_num_threads()", "intent": "Returns the number of threads used for parallelizing CPU operations", "question_id": 2318},
{"snippet": "torch.nn.functional.pdist(input)", "intent": "Computes the p-norm distance between every pair of row vectors in the `input` .", "question_id": 2319},
{"snippet": "torch.nn.functional.pdist(input, p=2)", "intent": "Computes the p-norm distance between every pair of row vectors in the `input` . This function is equivalent to scipy.spatial.distance.pdist ( input , \u2018 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) .", "question_id": 2320},
{"snippet": "torch.prod(input)", "intent": "Returns the product of all elements in the `input` tensor .", "question_id": 2321},
{"snippet": "torch.prod(input, dtype=None)", "intent": "Returns the product of all elements in the `input` tensor . With arguments `dtype`.", "question_id": 2322},
{"snippet": "Tensor.broadcast_to(shape)", "intent": "See torch.broadcast_to ( ) . With arguments `shape`.", "question_id": 2323},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 2324},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 2325},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 2326},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 2327},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 2328},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 2329},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 2330},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 2331},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 2332},
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 2333},
{"snippet": "torch.as_strided(input, size, stride)", "intent": "Create a view of an existing torch.Tensor `input` with specified `size` , `stride` and `storage_offset` .", "question_id": 2334},
{"snippet": "torch.as_strided(input, size, stride, storage_offset=0)", "intent": "Create a view of an existing torch.Tensor `input` with specified `size` , `stride` and `storage_offset` .", "question_id": 2335},
{"snippet": "Tensor.std(dim)", "intent": "See torch.std ( ) With arguments `dim`.", "question_id": 2336},
{"snippet": "Tensor.std(dim, unbiased=True)", "intent": "See torch.std ( ) With arguments `dim`, `unbiased`.", "question_id": 2337},
{"snippet": "Tensor.std(dim, keepdim=False)", "intent": "See torch.std ( ) With arguments `dim`, `keepdim`.", "question_id": 2338},
{"snippet": "Tensor.std(dim, unbiased=True, keepdim=False)", "intent": "See torch.std ( ) With arguments `dim`, `unbiased`, `keepdim`.", "question_id": 2339},
{"snippet": "torch.logcumsumexp(input, dim)", "intent": "Returns the logarithm of the cumulative summation of the exponentiation of elements of `input` in the dimension `dim` .", "question_id": 2340},
{"snippet": "torch.logcumsumexp(input, dim, out=None)", "intent": "Returns the logarithm of the cumulative summation of the exponentiation of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 2341},
{"snippet": "Tensor.sign_()", "intent": "In-place version of sign ( )", "question_id": 2342},
{"snippet": "torch.row_stack(tensors)", "intent": "Alias of torch.vstack ( ) . With arguments `tensors`.", "question_id": 2343},
{"snippet": "torch.row_stack(tensors, out=None)", "intent": "Alias of torch.vstack ( ) . With arguments `tensors`, `out`.", "question_id": 2344},
{"snippet": "torch.sparse.addmm(mat, mat1, mat2)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`.", "question_id": 2345},
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, beta=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `beta`.", "question_id": 2346},
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, alpha=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `alpha`.", "question_id": 2347},
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, beta=1.0, alpha=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `beta`, `alpha`.", "question_id": 2348},
{"snippet": "torch.quantization.prepare_qat(model)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version .", "question_id": 2349},
{"snippet": "torch.quantization.prepare_qat(model, mapping=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `mapping`.", "question_id": 2350},
{"snippet": "torch.quantization.prepare_qat(model, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `inplace`.", "question_id": 2351},
{"snippet": "torch.quantization.prepare_qat(model, mapping=None, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `mapping`, `inplace`.", "question_id": 2352},
{"snippet": "torch.cuda.seed()", "intent": "Sets the seed for generating random numbers to a random number for the current GPU .", "question_id": 2353},
{"snippet": "torch.cuda.nvtx.mark(msg)", "intent": "Describe an instantaneous event that occurred at some point . With arguments `msg`.", "question_id": 2354},
{"snippet": "torch.repeat_interleave(input, repeats)", "intent": "Repeat elements of a tensor . If the `repeats` is tensor ( [ n1 , n2 , n3 , \u2026 ] ) , then the output will be tensor ( [ 0 , 0 , \u2026 , 1 , 1 , \u2026 , 2 , 2 , \u2026 , \u2026 ] ) where 0 appears n1 times , 1 appears n2 times , 2 appears n3 times , etc . With arguments `input`.", "question_id": 2355},
{"snippet": "torch.repeat_interleave(input, repeats, dim=None)", "intent": "Repeat elements of a tensor . If the `repeats` is tensor ( [ n1 , n2 , n3 , \u2026 ] ) , then the output will be tensor ( [ 0 , 0 , \u2026 , 1 , 1 , \u2026 , 2 , 2 , \u2026 , \u2026 ] ) where 0 appears n1 times , 1 appears n2 times , 2 appears n3 times , etc . With arguments `input`, `dim`.", "question_id": 2356},
{"snippet": "Tensor.atanh()", "intent": "See torch.atanh ( )", "question_id": 2357},
{"snippet": "Tensor.bitwise_xor_()", "intent": "In-place version of bitwise_xor ( )", "question_id": 2358},
{"snippet": "torch.meshgrid(*tensors)", "intent": "Take NNN tensors , each of which can be either scalar or 1-dimensional vector , and create NNN N-dimensional grids , where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs . With arguments `*tensors`.", "question_id": 2359},
{"snippet": "Tensor.atan2_(other)", "intent": "In-place version of atan2 ( ) With arguments `other`.", "question_id": 2360},
{"snippet": "torch.split(tensor, split_size_or_sections)", "intent": "Splits the `tensor` into chunks . If `split_size_or_sections` is an integer type , then tensor will be split into equally sized chunks ( if possible ) .", "question_id": 2361},
{"snippet": "torch.split(tensor, split_size_or_sections, dim=0)", "intent": "Splits the `tensor` into chunks . If `split_size_or_sections` is an integer type , then tensor will be split into equally sized chunks ( if possible ) . Last chunk will be smaller if the tensor size along the given dimension `dim` is not divisible by split_size .", "question_id": 2362},
{"snippet": "Tensor.flip(dims)", "intent": "See torch.flip ( ) With arguments `dims`.", "question_id": 2363},
{"snippet": "torch.quantization.qconfig.QConfigDynamic()", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights .", "question_id": 2364},
{"snippet": "torch.quantization.qconfig.QConfigDynamic(activation=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `activation`.", "question_id": 2365},
{"snippet": "torch.quantization.qconfig.QConfigDynamic(weight=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `weight`.", "question_id": 2366},
{"snippet": "torch.quantization.qconfig.QConfigDynamic(activation=<class 'torch.nn.modules.linear.Identity'>, weight=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `activation`, `weight`.", "question_id": 2367},
{"snippet": "torch.nn.functional.normalize(input)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as", "question_id": 2368},
{"snippet": "torch.nn.functional.normalize(input, p=2.0)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`.", "question_id": 2369},
{"snippet": "torch.nn.functional.normalize(input, dim=1)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as", "question_id": 2370},
{"snippet": "torch.nn.functional.normalize(input, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `eps`.", "question_id": 2371},
{"snippet": "torch.nn.functional.normalize(input, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `out`.", "question_id": 2372},
{"snippet": "torch.nn.functional.normalize(input, p=2.0, dim=1)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`.", "question_id": 2373},
{"snippet": "torch.nn.functional.normalize(input, p=2.0, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`, `eps`.", "question_id": 2374},
{"snippet": "torch.nn.functional.normalize(input, p=2.0, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`, `out`.", "question_id": 2375},
{"snippet": "torch.nn.functional.normalize(input, dim=1, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `eps`.", "question_id": 2376},
{"snippet": "torch.nn.functional.normalize(input, dim=1, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `out`.", "question_id": 2377},
{"snippet": "Tensor.asinh_()", "intent": "In-place version of asinh ( )", "question_id": 2378},
{"snippet": "torch.kaiser_window(window_length)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` .", "question_id": 2379},
{"snippet": "torch.kaiser_window(window_length, periodic=True)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length .", "question_id": 2380},
{"snippet": "torch.kaiser_window(window_length, beta=12.0)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` .", "question_id": 2381},
{"snippet": "torch.kaiser_window(window_length, dtype=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `dtype`.", "question_id": 2382},
{"snippet": "torch.kaiser_window(window_length, layout=torch.strided)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `layout`.", "question_id": 2383},
{"snippet": "torch.kaiser_window(window_length, device=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `device`.", "question_id": 2384},
{"snippet": "torch.kaiser_window(window_length, requires_grad=False)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `requires_grad`.", "question_id": 2385},
{"snippet": "torch.kaiser_window(window_length, periodic=True, beta=12.0)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length .", "question_id": 2386},
{"snippet": "torch.kaiser_window(window_length, periodic=True, dtype=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length . With arguments `dtype`.", "question_id": 2387},
{"snippet": "torch.kaiser_window(window_length, periodic=True, layout=torch.strided)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length . With arguments `layout`.", "question_id": 2388},
{"snippet": "Tensor.view_as(other)", "intent": "View this tensor as the same size as `other` .", "question_id": 2389},
{"snippet": "torch.nn.LSTM(*args, **kwargs)", "intent": "Applies a multi-layer long short-term memory ( LSTM ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 2390},
{"snippet": "Tensor.acosh()", "intent": "See torch.acosh ( )", "question_id": 2391},
{"snippet": "Tensor.hsplit(split_size_or_sections)", "intent": "See torch.hsplit ( ) With arguments `split_size_or_sections`.", "question_id": 2392},
{"snippet": "torch.quantization.add_observer_(module)", "intent": "Add observer for the leaf child of the `module` .", "question_id": 2393},
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`.", "question_id": 2394},
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`.", "question_id": 2395},
{"snippet": "torch.quantization.add_observer_(module, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `device`.", "question_id": 2396},
{"snippet": "torch.quantization.add_observer_(module, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `custom_module_class_mapping`.", "question_id": 2397},
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `non_leaf_module_list`.", "question_id": 2398},
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `device`.", "question_id": 2399},
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `custom_module_class_mapping`.", "question_id": 2400},
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`, `device`.", "question_id": 2401},
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`, `custom_module_class_mapping`.", "question_id": 2402},
{"snippet": "torch.less_equal(input, other)", "intent": "Alias for torch.le ( ) . With arguments `input`, `other`.", "question_id": 2403},
{"snippet": "torch.less_equal(input, other, out=None)", "intent": "Alias for torch.le ( ) . With arguments `input`, `other`, `out`.", "question_id": 2404},
{"snippet": "torch.jit.trace(func, example_inputs)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`.", "question_id": 2405},
{"snippet": "torch.jit.trace(func, example_inputs, optimize=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `optimize`.", "question_id": 2406},
{"snippet": "torch.jit.trace(func, example_inputs, check_trace=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_trace`.", "question_id": 2407},
{"snippet": "torch.jit.trace(func, example_inputs, check_inputs=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_inputs`.", "question_id": 2408},
{"snippet": "torch.jit.trace(func, example_inputs, check_tolerance=1e-05)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_tolerance`.", "question_id": 2409},
{"snippet": "torch.jit.trace(func, example_inputs, strict=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `strict`.", "question_id": 2410},
{"snippet": "torch.jit.trace(func, example_inputs, _force_outplace=False)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_force_outplace`.", "question_id": 2411},
{"snippet": "torch.jit.trace(func, example_inputs, _module_class=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_module_class`.", "question_id": 2412},
{"snippet": "torch.jit.trace(func, example_inputs, _compilation_unit=<torch.jit.CompilationUnit object>)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_compilation_unit`.", "question_id": 2413},
{"snippet": "torch.jit.trace(func, example_inputs, optimize=None, check_trace=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `optimize`, `check_trace`.", "question_id": 2414},
{"snippet": "torch.sparse.log_softmax(input, dim)", "intent": "Applies a softmax function followed by logarithm . With arguments `input`, `dim`.", "question_id": 2415},
{"snippet": "torch.sparse.log_softmax(input, dim, dtype=None)", "intent": "Applies a softmax function followed by logarithm . With arguments `input`, `dim`, `dtype`.", "question_id": 2416},
{"snippet": "Tensor.scatter_add_(dim, index, src)", "intent": "Adds all values from the tensor other into self at the indices specified in the `index` tensor in a similar fashion as scatter_ ( ) . For each value in `src` , it is added to an index in self which is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim .", "question_id": 2417},
{"snippet": "torch.quantize_per_channel(input, scales, zero_points, axis, dtype)", "intent": "Converts a float tensor to a per-channel quantized tensor with given `scales` and zero points . With arguments `input`, `zero_points`, `axis`, `dtype`.", "question_id": 2418},
{"snippet": "torch.nn.functional.logsigmoid(input)", "intent": "Applies element-wise LogSigmoid ( xi ) =log\u2061 ( 11+exp\u2061 ( \u2212xi ) ) \\text { LogSigmoid } ( x_i ) = \\log \\left ( \\frac { 1 } { 1 + \\exp ( -x_i ) } \\right ) LogSigmoid ( xi\u200b ) =log ( 1+exp ( \u2212xi\u200b ) 1\u200b ) With arguments `input`.", "question_id": 2419},
{"snippet": "Tensor.char()", "intent": "self.char ( ) is equivalent to self.to ( torch.int8 ) .", "question_id": 2420},
{"snippet": "Tensor.char(memory_format=torch.preserve_format)", "intent": "self.char ( ) is equivalent to self.to ( torch.int8 ) . With arguments `memory_format`.", "question_id": 2421},
{"snippet": "torch.cuda.nvtx.range_push(msg)", "intent": "Pushes a range onto a stack of nested range span . With arguments `msg`.", "question_id": 2422},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 2423},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 2424},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 2425},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 2426},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 2427},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 2428},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 2429},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 2430},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 2431},
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 2432},
{"snippet": "Tensor.index_select(dim, index)", "intent": "See torch.index_select ( ) With arguments `dim`, `index`.", "question_id": 2433},
{"snippet": "torch.nn.intrinsic.BNReLU2d(batch_norm, relu)", "intent": "This is a sequential container which calls the BatchNorm 2d and ReLU modules . With arguments `batch_norm`, `relu`.", "question_id": 2434},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`.", "question_id": 2435},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 2436},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, device=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 2437},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 2438},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, device=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 2439},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 2440},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, device=None, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `device`, `dtype`.", "question_id": 2441},
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, device=None, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `device`, `dtype`.", "question_id": 2442},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 2443},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`.", "question_id": 2444},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, max_norm=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 2445},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 2446},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 2447},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, sparse=False)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 2448},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, _weight=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 2449},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, dtype=torch.quint8)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 2450},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `max_norm`.", "question_id": 2451},
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, norm_type=2.0)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `norm_type`.", "question_id": 2452},
{"snippet": "embedding.from_float(mod)", "intent": "Create a quantized embedding module from a float module With arguments `mod`.", "question_id": 2453},
{"snippet": "Tensor.stride(dim)", "intent": "Returns the stride of self tensor . Stride is the jump necessary to go from one element to the next one in the specified dimension `dim` .", "question_id": 2454},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`.", "question_id": 2455},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`.", "question_id": 2456},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `reduction`.", "question_id": 2457},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `zero_infinity`.", "question_id": 2458},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `reduction`.", "question_id": 2459},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `zero_infinity`.", "question_id": 2460},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `reduction`, `zero_infinity`.", "question_id": 2461},
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `reduction`, `zero_infinity`.", "question_id": 2462},
{"snippet": "Tensor.cosh()", "intent": "See torch.cosh ( )", "question_id": 2463},
{"snippet": "torch.nn.utils.prune.random_structured(module, name, amount, dim)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` selected at random .", "question_id": 2464},
{"snippet": "torch.nn.CrossEntropyLoss()", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class .", "question_id": 2465},
{"snippet": "torch.nn.CrossEntropyLoss(weight=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes .", "question_id": 2466},
{"snippet": "torch.nn.CrossEntropyLoss(size_average=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `size_average`.", "question_id": 2467},
{"snippet": "torch.nn.CrossEntropyLoss(ignore_index=- 100)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . This criterion expects a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] as the target for each value of a 1D tensor of size minibatch ; if `ignore_index` is specified , this criterion also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 2468},
{"snippet": "torch.nn.CrossEntropyLoss(reduce=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `reduce`.", "question_id": 2469},
{"snippet": "torch.nn.CrossEntropyLoss(reduction='mean')", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `reduction`.", "question_id": 2470},
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, size_average=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `size_average`.", "question_id": 2471},
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, ignore_index=- 100)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . This criterion expects a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] as the target for each value of a 1D tensor of size minibatch ; if `ignore_index` is specified , this criterion also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 2472},
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, reduce=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduce`.", "question_id": 2473},
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, reduction='mean')", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduction`.", "question_id": 2474},
{"snippet": "torch.jit.isinstance(obj, target_type)", "intent": "This function provides for conatiner type refinement in TorchScript . With arguments `obj`, `target_type`.", "question_id": 2475},
{"snippet": "Tensor.sinc_()", "intent": "In-place version of sinc ( )", "question_id": 2476},
{"snippet": "torch.median(input)", "intent": "Returns the median of the values in `input` .", "question_id": 2477},
{"snippet": "Tensor.inner(other)", "intent": "See torch.inner ( ) . With arguments `other`.", "question_id": 2478},
{"snippet": "Tensor.prod()", "intent": "See torch.prod ( )", "question_id": 2479},
{"snippet": "Tensor.prod(dim=None)", "intent": "See torch.prod ( ) With arguments `dim`.", "question_id": 2480},
{"snippet": "Tensor.prod(keepdim=False)", "intent": "See torch.prod ( ) With arguments `keepdim`.", "question_id": 2481},
{"snippet": "Tensor.prod(dtype=None)", "intent": "See torch.prod ( ) With arguments `dtype`.", "question_id": 2482},
{"snippet": "Tensor.prod(dim=None, keepdim=False)", "intent": "See torch.prod ( ) With arguments `dim`, `keepdim`.", "question_id": 2483},
{"snippet": "Tensor.prod(dim=None, dtype=None)", "intent": "See torch.prod ( ) With arguments `dim`, `dtype`.", "question_id": 2484},
{"snippet": "Tensor.prod(keepdim=False, dtype=None)", "intent": "See torch.prod ( ) With arguments `keepdim`, `dtype`.", "question_id": 2485},
{"snippet": "Tensor.prod(dim=None, keepdim=False, dtype=None)", "intent": "See torch.prod ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 2486},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 2487},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `dropout`.", "question_id": 2488},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, bias=True)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `bias`.", "question_id": 2489},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, add_bias_kv=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `add_bias_kv`.", "question_id": 2490},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, add_zero_attn=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `add_zero_attn`.", "question_id": 2491},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, kdim=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 2492},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, vdim=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 2493},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `batch_first`.", "question_id": 2494},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, device=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `device`.", "question_id": 2495},
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, dtype=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `dtype`.", "question_id": 2496},
{"snippet": "torch.cuda.default_stream()", "intent": "Returns the default Stream for a given `device` .", "question_id": 2497},
{"snippet": "torch.cuda.default_stream(device=None)", "intent": "Returns the default Stream for a given `device` .", "question_id": 2498},
{"snippet": "torch.erf(input)", "intent": "Alias for torch.special.erf ( ) . With arguments `input`.", "question_id": 2499},
{"snippet": "torch.erf(input, out=None)", "intent": "Alias for torch.special.erf ( ) . With arguments `input`, `out`.", "question_id": 2500},
{"snippet": "torch.nn.functional.hardtanh_(input)", "intent": "In-place version of hardtanh ( ) . With arguments `input`.", "question_id": 2501},
{"snippet": "torch.nn.functional.hardtanh_(input, min_val=- 1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `min_val`.", "question_id": 2502},
{"snippet": "torch.nn.functional.hardtanh_(input, max_val=1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `max_val`.", "question_id": 2503},
{"snippet": "torch.nn.functional.hardtanh_(input, min_val=- 1., max_val=1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`.", "question_id": 2504},
{"snippet": "torch.nn.quantized.FXFloatFunctional", "intent": "module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly", "question_id": 2505},
{"snippet": "Optimizer.step(closure)", "intent": "Performs a single optimization step ( parameter update ) . With arguments `closure`.", "question_id": 2506},
{"snippet": "torch.broadcast_shapes(*shapes)", "intent": "Similar to broadcast_tensors ( ) but for shapes . With arguments `*shapes`.", "question_id": 2507},
{"snippet": "torch.nn.Identity(*args, **kwargs)", "intent": "A placeholder identity operator that is argument-insensitive . With arguments `*args`, `**kwargs`.", "question_id": 2508},
{"snippet": "torch.cuda.nvtx.range_pop()", "intent": "Pops a range off of a stack of nested range spans .", "question_id": 2509},
{"snippet": "torch.unique(*args, **kwargs)", "intent": "Returns the unique elements of the input tensor . With arguments `*args`, `**kwargs`.", "question_id": 2510},
{"snippet": "torch.floor_divide(input, other)", "intent": "Computes `input` divided by `other` , elementwise , and rounds each quotient towards zero .", "question_id": 2511},
{"snippet": "torch.floor_divide(input, other, out=None)", "intent": "Computes `input` divided by `other` , elementwise , and rounds each quotient towards zero . With arguments `out`.", "question_id": 2512},
{"snippet": "torch.svd(input)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` .", "question_id": 2513},
{"snippet": "torch.svd(input, some=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition .", "question_id": 2514},
{"snippet": "torch.svd(input, compute_uv=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input .", "question_id": 2515},
{"snippet": "torch.svd(input, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . With arguments `out`.", "question_id": 2516},
{"snippet": "torch.svd(input, some=True, compute_uv=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input .", "question_id": 2517},
{"snippet": "torch.svd(input, some=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . With arguments `out`.", "question_id": 2518},
{"snippet": "torch.svd(input, compute_uv=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input . With arguments `out`.", "question_id": 2519},
{"snippet": "torch.svd(input, some=True, compute_uv=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input . With arguments `out`.", "question_id": 2520},
{"snippet": "torch.any(input)", "intent": "Tests if any element in `input` evaluates to True .", "question_id": 2521},
{"snippet": "torch.nn.Module", "intent": "Base class for all neural network modules.", "question_id": 2522},
{"snippet": "module.add_module(name, module)", "intent": "Adds a child `module` to the current module . The module can be accessed as an attribute using the given `name` .", "question_id": 2523},
{"snippet": "module.apply(fn)", "intent": "Applies `fn` recursively to every submodule ( as returned by .children ( ) ) as well as self .", "question_id": 2524},
{"snippet": "module.bfloat16()", "intent": "Casts all floating point parameters and buffers to bfloat16 datatype .", "question_id": 2525},
{"snippet": "module.buffers()", "intent": "Returns an iterator over module buffers .", "question_id": 2526},
{"snippet": "module.buffers(recurse=True)", "intent": "Returns an iterator over module buffers . With arguments `recurse`.", "question_id": 2527},
{"snippet": "module.children()", "intent": "Returns an iterator over immediate children modules .", "question_id": 2528},
{"snippet": "module.cpu()", "intent": "Moves all model parameters and buffers to the CPU .", "question_id": 2529},
{"snippet": "module.cuda()", "intent": "Moves all model parameters and buffers to the GPU .", "question_id": 2530},
{"snippet": "module.cuda(device=None)", "intent": "Moves all model parameters and buffers to the GPU . With arguments `device`.", "question_id": 2531},
{"snippet": "module.double()", "intent": "Casts all floating point parameters and buffers to double datatype .", "question_id": 2532},
{"snippet": "module.dump_patches", "intent": "This allows better BC support for load_state_dict().", "question_id": 2533},
{"snippet": "module.eval()", "intent": "Sets the module in evaluation mode .", "question_id": 2534},
{"snippet": "module.extra_repr()", "intent": "Set the extra representation of the module", "question_id": 2535},
{"snippet": "module.float()", "intent": "Casts all floating point parameters and buffers to float datatype .", "question_id": 2536},
{"snippet": "module.forward(*input)", "intent": "Defines the computation performed at every call . With arguments `*input`.", "question_id": 2537},
{"snippet": "module.get_buffer(target)", "intent": "Returns the buffer given by `target` if it exists , otherwise throws an error .", "question_id": 2538},
{"snippet": "module.get_parameter(target)", "intent": "Returns the parameter given by `target` if it exists , otherwise throws an error .", "question_id": 2539},
{"snippet": "module.get_submodule(target)", "intent": "Returns the submodule given by `target` if it exists , otherwise throws an error .", "question_id": 2540},
{"snippet": "module.half()", "intent": "Casts all floating point parameters and buffers to half datatype .", "question_id": 2541},
{"snippet": "module.load_state_dict(state_dict)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants .", "question_id": 2542},
{"snippet": "module.load_state_dict(state_dict, strict=True)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants . If `strict` is True , then the keys of state_dict must exactly match the keys returned by this module \u2019 s state_dict ( ) function .", "question_id": 2543},
{"snippet": "module.modules()", "intent": "Returns an iterator over all modules in the network .", "question_id": 2544},
{"snippet": "module.named_buffers()", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself .", "question_id": 2545},
{"snippet": "module.named_buffers(prefix='')", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`.", "question_id": 2546},
{"snippet": "module.named_buffers(recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `recurse`.", "question_id": 2547},
{"snippet": "module.named_buffers(prefix='', recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`, `recurse`.", "question_id": 2548},
{"snippet": "module.named_children()", "intent": "Returns an iterator over immediate children modules , yielding both the name of the module as well as the module itself .", "question_id": 2549},
{"snippet": "module.named_modules()", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself .", "question_id": 2550},
{"snippet": "module.named_modules(memo=None)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`.", "question_id": 2551},
{"snippet": "module.named_modules(prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`.", "question_id": 2552},
{"snippet": "module.named_modules(remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `remove_duplicate`.", "question_id": 2553},
{"snippet": "module.named_modules(memo=None, prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`.", "question_id": 2554},
{"snippet": "module.named_modules(memo=None, remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `remove_duplicate`.", "question_id": 2555},
{"snippet": "module.named_modules(prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`, `remove_duplicate`.", "question_id": 2556},
{"snippet": "module.named_modules(memo=None, prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`, `remove_duplicate`.", "question_id": 2557},
{"snippet": "module.named_parameters()", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself .", "question_id": 2558},
{"snippet": "module.named_parameters(prefix='')", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`.", "question_id": 2559},
{"snippet": "module.named_parameters(recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `recurse`.", "question_id": 2560},
{"snippet": "module.named_parameters(prefix='', recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`, `recurse`.", "question_id": 2561},
{"snippet": "module.parameters()", "intent": "Returns an iterator over module parameters .", "question_id": 2562},
{"snippet": "module.parameters(recurse=True)", "intent": "Returns an iterator over module parameters . With arguments `recurse`.", "question_id": 2563},
{"snippet": "module.register_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 2564},
{"snippet": "module.register_buffer(name, tensor)", "intent": "Adds a buffer to the module . With arguments `name`, `tensor`.", "question_id": 2565},
{"snippet": "module.register_buffer(name, tensor, persistent=True)", "intent": "Adds a buffer to the module . Buffers , by default , are `persistent` and will be saved alongside parameters . With arguments `name`, `tensor`.", "question_id": 2566},
{"snippet": "module.register_forward_hook(hook)", "intent": "Registers a forward `hook` on the module .", "question_id": 2567},
{"snippet": "module.register_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook on the module . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 2568},
{"snippet": "module.register_full_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 2569},
{"snippet": "module.register_parameter(name, param)", "intent": "Adds a parameter to the module . The parameter can be accessed as an attribute using given `name` . With arguments `param`.", "question_id": 2570},
{"snippet": "module.requires_grad_()", "intent": "Change if autograd should record operations on parameters in this module .", "question_id": 2571},
{"snippet": "module.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on parameters in this module . This method sets the parameters \u2019 `requires_grad` attributes in-place .", "question_id": 2572},
{"snippet": "module.share_memory()", "intent": "See torch.Tensor.share_memory_ ( )", "question_id": 2573},
{"snippet": "module.state_dict()", "intent": "Returns a dictionary containing a whole state of the module .", "question_id": 2574},
{"snippet": "module.state_dict(destination=None)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`.", "question_id": 2575},
{"snippet": "module.state_dict(prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`.", "question_id": 2576},
{"snippet": "module.state_dict(keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `keep_vars`.", "question_id": 2577},
{"snippet": "module.state_dict(destination=None, prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`.", "question_id": 2578},
{"snippet": "module.state_dict(destination=None, keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `keep_vars`.", "question_id": 2579},
{"snippet": "module.state_dict(prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`, `keep_vars`.", "question_id": 2580},
{"snippet": "module.state_dict(destination=None, prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`, `keep_vars`.", "question_id": 2581},
{"snippet": "module.to(*args, **kwargs)", "intent": "Moves and/or casts the parameters and buffers . With arguments `*args`, `**kwargs`.", "question_id": 2582},
{"snippet": "module.to_empty(device)", "intent": "Moves the parameters and buffers to the specified `device` without copying storage .", "question_id": 2583},
{"snippet": "module.train()", "intent": "Sets the module in training `mode` .", "question_id": 2584},
{"snippet": "module.train(mode=True)", "intent": "Sets the module in training `mode` .", "question_id": 2585},
{"snippet": "module.type(dst_type)", "intent": "Casts all parameters and buffers to `dst_type` .", "question_id": 2586},
{"snippet": "module.xpu()", "intent": "Moves all model parameters and buffers to the XPU .", "question_id": 2587},
{"snippet": "module.xpu(device=None)", "intent": "Moves all model parameters and buffers to the XPU . With arguments `device`.", "question_id": 2588},
{"snippet": "module.zero_grad()", "intent": "Sets gradients of all model parameters to zero .", "question_id": 2589},
{"snippet": "module.zero_grad(set_to_none=False)", "intent": "Sets gradients of all model parameters to zero . With arguments `set_to_none`.", "question_id": 2590},
{"snippet": "torch.nn.functional.huber_loss(input, target)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`.", "question_id": 2591},
{"snippet": "torch.nn.functional.huber_loss(input, target, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 2592},
{"snippet": "torch.nn.functional.huber_loss(input, target, delta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`.", "question_id": 2593},
{"snippet": "torch.nn.functional.huber_loss(input, target, reduction='mean', delta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 2594},
{"snippet": "Tensor.bitwise_and()", "intent": "See torch.bitwise_and ( )", "question_id": 2595},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`.", "question_id": 2596},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`.", "question_id": 2597},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `ch_axis`.", "question_id": 2598},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `dtype`.", "question_id": 2599},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `qscheme`.", "question_id": 2600},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `reduce_range`.", "question_id": 2601},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `quant_min`.", "question_id": 2602},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `quant_max`.", "question_id": 2603},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01, ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`, `ch_axis`.", "question_id": 2604},
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`, `dtype`.", "question_id": 2605},
{"snippet": "torch.is_tensor(obj)", "intent": "Returns True if `obj` is a PyTorch tensor .", "question_id": 2606},
{"snippet": "Tensor.argmax()", "intent": "See torch.argmax ( )", "question_id": 2607},
{"snippet": "Tensor.argmax(dim=None)", "intent": "See torch.argmax ( ) With arguments `dim`.", "question_id": 2608},
{"snippet": "Tensor.argmax(keepdim=False)", "intent": "See torch.argmax ( ) With arguments `keepdim`.", "question_id": 2609},
{"snippet": "Tensor.argmax(dim=None, keepdim=False)", "intent": "See torch.argmax ( ) With arguments `dim`, `keepdim`.", "question_id": 2610},
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`.", "question_id": 2611},
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- 1)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `last_epoch`.", "question_id": 2612},
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, verbose=False)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `verbose`.", "question_id": 2613},
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- 1, verbose=False)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `last_epoch`, `verbose`.", "question_id": 2614},
{"snippet": "lambda_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 2615},
{"snippet": "lambda_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 2616},
{"snippet": "lambda_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 2617},
{"snippet": "lambda_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 2618},
{"snippet": "lambda_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 2619},
{"snippet": "torch.linalg.householder_product(A, tau)", "intent": "Computes the first n columns of a product of Householder matrices . With arguments `A`, `tau`.", "question_id": 2620},
{"snippet": "torch.linalg.householder_product(A, tau, out=None)", "intent": "Computes the first n columns of a product of Householder matrices . With arguments `A`, `tau`, `out`.", "question_id": 2621},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 2622},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 2623},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 2624},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 2625},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 2626},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 2627},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 2628},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 2629},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 2630},
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 2631},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 2632},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 2633},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config_dict=None)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 2634},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 2635},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, convert_custom_config_dict=None)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 2636},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 2637},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config_dict=None, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 2638},
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, convert_custom_config_dict=None, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 2639},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`.", "question_id": 2640},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`.", "question_id": 2641},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dropout=0.1)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dropout`.", "question_id": 2642},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, activation='relu')", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `activation`.", "question_id": 2643},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, layer_norm_eps=1e-05)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `layer_norm_eps`.", "question_id": 2644},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, batch_first=False)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `batch_first`.", "question_id": 2645},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, device=None)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `device`.", "question_id": 2646},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dtype=None)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dtype`.", "question_id": 2647},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `dropout`.", "question_id": 2648},
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, activation='relu')", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `activation`.", "question_id": 2649},
{"snippet": "transformer_decoder_layer.forward(tgt, memory)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`.", "question_id": 2650},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`.", "question_id": 2651},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`.", "question_id": 2652},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_key_padding_mask`.", "question_id": 2653},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_key_padding_mask`.", "question_id": 2654},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `memory_mask`.", "question_id": 2655},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `tgt_key_padding_mask`.", "question_id": 2656},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `memory_key_padding_mask`.", "question_id": 2657},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`, `tgt_key_padding_mask`.", "question_id": 2658},
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`, `memory_key_padding_mask`.", "question_id": 2659},
{"snippet": "torch.nn.GLU()", "intent": "Applies the gated linear unit function GLU ( a , b ) =a\u2297\u03c3 ( b ) { GLU } ( a , b ) = a \\otimes \\sigma ( b ) GLU ( a , b ) =a\u2297\u03c3 ( b ) where aaa is the first half of the input matrices and bbb is the second half .", "question_id": 2660},
{"snippet": "torch.nn.GLU(dim=- 1)", "intent": "Applies the gated linear unit function GLU ( a , b ) =a\u2297\u03c3 ( b ) { GLU } ( a , b ) = a \\otimes \\sigma ( b ) GLU ( a , b ) =a\u2297\u03c3 ( b ) where aaa is the first half of the input matrices and bbb is the second half . With arguments `dim`.", "question_id": 2661},
{"snippet": "Tensor.reshape_as(other)", "intent": "Returns this tensor as the same shape as `other` .", "question_id": 2662},
{"snippet": "torch.nextafter(input, other)", "intent": "Return the next floating-point value after `input` towards `other` , elementwise .", "question_id": 2663},
{"snippet": "torch.nextafter(input, other, out=None)", "intent": "Return the next floating-point value after `input` towards `other` , elementwise . With arguments `out`.", "question_id": 2664},
{"snippet": "Tensor.addmv_(mat, vec)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`.", "question_id": 2665},
{"snippet": "Tensor.addmv_(mat, vec, beta=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `beta`.", "question_id": 2666},
{"snippet": "Tensor.addmv_(mat, vec, alpha=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `alpha`.", "question_id": 2667},
{"snippet": "Tensor.addmv_(mat, vec, beta=1, alpha=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `beta`, `alpha`.", "question_id": 2668},
{"snippet": "torch.msort(input)", "intent": "Sorts the elements of the `input` tensor along its first dimension in ascending order by value .", "question_id": 2669},
{"snippet": "torch.msort(input, out=None)", "intent": "Sorts the elements of the `input` tensor along its first dimension in ascending order by value . With arguments `out`.", "question_id": 2670},
{"snippet": "torch.cuda.get_rng_state_all()", "intent": "Returns a list of ByteTensor representing the random number states of all devices .", "question_id": 2671},
{"snippet": "Tensor.new_tensor(data)", "intent": "Returns a new Tensor with `data` as the tensor data .", "question_id": 2672},
{"snippet": "Tensor.new_tensor(data, dtype=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`.", "question_id": 2673},
{"snippet": "Tensor.new_tensor(data, device=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `device`.", "question_id": 2674},
{"snippet": "Tensor.new_tensor(data, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `requires_grad`.", "question_id": 2675},
{"snippet": "Tensor.new_tensor(data, dtype=None, device=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `device`.", "question_id": 2676},
{"snippet": "Tensor.new_tensor(data, dtype=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `requires_grad`.", "question_id": 2677},
{"snippet": "Tensor.new_tensor(data, device=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `device`, `requires_grad`.", "question_id": 2678},
{"snippet": "Tensor.new_tensor(data, dtype=None, device=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 2679},
{"snippet": "Tensor.qr()", "intent": "See torch.qr ( )", "question_id": 2680},
{"snippet": "Tensor.qr(some=True)", "intent": "See torch.qr ( ) With arguments `some`.", "question_id": 2681},
{"snippet": "torch.as_tensor(data)", "intent": "Convert the `data` into a torch.Tensor .", "question_id": 2682},
{"snippet": "torch.as_tensor(data, dtype=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 2683},
{"snippet": "torch.as_tensor(data, device=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 2684},
{"snippet": "torch.as_tensor(data, dtype=None, device=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 2685},
{"snippet": "torch.ravel(input)", "intent": "Return a contiguous flattened tensor . With arguments `input`.", "question_id": 2686},
{"snippet": "Tensor.fmod(divisor)", "intent": "See torch.fmod ( ) With arguments `divisor`.", "question_id": 2687},
{"snippet": "torch.nn.modules.lazy.LazyModuleMixin(*args, **kwargs)", "intent": "A mixin for modules that lazily initialize parameters , also known as \u201c lazy modules . \u201d With arguments `*args`, `**kwargs`.", "question_id": 2688},
{"snippet": "lazy_module_mixin.has_uninitialized_params()", "intent": "Check if a module has parameters that are not initialized", "question_id": 2689},
{"snippet": "lazy_module_mixin.initialize_parameters(*args, **kwargs)", "intent": "Initialize parameters according to the input batch properties . With arguments `*args`, `**kwargs`.", "question_id": 2690},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`.", "question_id": 2691},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`.", "question_id": 2692},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`.", "question_id": 2693},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 2694},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`.", "question_id": 2695},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 2696},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 2697},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`.", "question_id": 2698},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 2699},
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`.", "question_id": 2700},
{"snippet": "Tensor.hypot_(other)", "intent": "In-place version of hypot ( ) With arguments `other`.", "question_id": 2701},
{"snippet": "torch.nn.RReLU()", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper :", "question_id": 2702},
{"snippet": "torch.nn.RReLU(lower=0.125)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 2703},
{"snippet": "torch.nn.RReLU(upper=0.3333333333333333)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 2704},
{"snippet": "torch.nn.RReLU(inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : With arguments `inplace`.", "question_id": 2705},
{"snippet": "torch.nn.RReLU(lower=0.125, upper=0.3333333333333333)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 2706},
{"snippet": "torch.nn.RReLU(lower=0.125, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 2707},
{"snippet": "torch.nn.RReLU(upper=0.3333333333333333, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 2708},
{"snippet": "torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 2709},
{"snippet": "Tensor.as_subclass(cls)", "intent": "Makes a `cls` instance with the same data pointer as self .", "question_id": 2710},
{"snippet": "torch.nn.ELU()", "intent": "Applies the element-wise function :", "question_id": 2711},
{"snippet": "torch.nn.ELU(alpha=1.0)", "intent": "Applies the element-wise function : With arguments `alpha`.", "question_id": 2712},
{"snippet": "torch.nn.ELU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 2713},
{"snippet": "torch.nn.ELU(alpha=1.0, inplace=False)", "intent": "Applies the element-wise function : With arguments `alpha`, `inplace`.", "question_id": 2714},
{"snippet": "torch.broadcast_to(input, shape)", "intent": "Broadcasts `input` to the `shape` shape .", "question_id": 2715},
{"snippet": "Tensor.greater(other)", "intent": "See torch.greater ( ) . With arguments `other`.", "question_id": 2716},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 2717},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 2718},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, padding=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 2719},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, dilation=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 2720},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, groups=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 2721},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 2722},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, scale=1.0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 2723},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, zero_point=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 2724},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 2725},
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 2726},
{"snippet": "Tensor.squeeze()", "intent": "See torch.squeeze ( )", "question_id": 2727},
{"snippet": "Tensor.squeeze(dim=None)", "intent": "See torch.squeeze ( ) With arguments `dim`.", "question_id": 2728},
{"snippet": "torch.norm(input)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`.", "question_id": 2729},
{"snippet": "torch.norm(input, p='fro')", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`.", "question_id": 2730},
{"snippet": "torch.norm(input, dim=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `dim`.", "question_id": 2731},
{"snippet": "torch.norm(input, keepdim=False)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `keepdim`.", "question_id": 2732},
{"snippet": "torch.norm(input, out=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `out`.", "question_id": 2733},
{"snippet": "torch.norm(input, dtype=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `dtype`.", "question_id": 2734},
{"snippet": "torch.norm(input, p='fro', dim=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `dim`.", "question_id": 2735},
{"snippet": "torch.norm(input, p='fro', keepdim=False)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `keepdim`.", "question_id": 2736},
{"snippet": "torch.norm(input, p='fro', out=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `out`.", "question_id": 2737},
{"snippet": "torch.norm(input, p='fro', dtype=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `dtype`.", "question_id": 2738},
{"snippet": "torch.heaviside(input, values)", "intent": "Computes the Heaviside step function for each element in `input` . With arguments `values`.", "question_id": 2739},
{"snippet": "torch.heaviside(input, values, out=None)", "intent": "Computes the Heaviside step function for each element in `input` . With arguments `values`, `out`.", "question_id": 2740},
{"snippet": "torch.nn.Dropout2d()", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) .", "question_id": 2741},
{"snippet": "torch.nn.Dropout2d(p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 2742},
{"snippet": "torch.nn.Dropout2d(inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . With arguments `inplace`.", "question_id": 2743},
{"snippet": "torch.nn.Dropout2d(p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 2744},
{"snippet": "Tensor.as_strided(size, stride)", "intent": "See torch.as_strided ( ) With arguments `size`, `stride`.", "question_id": 2745},
{"snippet": "Tensor.as_strided(size, stride, storage_offset=0)", "intent": "See torch.as_strided ( ) With arguments `size`, `stride`, `storage_offset`.", "question_id": 2746},
{"snippet": "Tensor.get_device() -> Device ordinal (Integer)", "intent": "For CUDA tensors , this function returns the device ordinal of the GPU on which the tensor resides . With arguments `) -> Device ordinal (Integer`.", "question_id": 2747},
{"snippet": "torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)", "intent": "Globally prunes tensors corresponding to all `parameters` in parameters by applying the specified `pruning_method` . With arguments `**kwargs`.", "question_id": 2748},
{"snippet": "torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs, importance_scores=None)", "intent": "Globally prunes tensors corresponding to all `parameters` in parameters by applying the specified `pruning_method` . With arguments `**kwargs`, `importance_scores`.", "question_id": 2749},
{"snippet": "Tensor.polygamma(n)", "intent": "See torch.polygamma ( ) With arguments `n`.", "question_id": 2750},
{"snippet": "Tensor.arctanh_(other)", "intent": "In-place version of arctanh ( ) With arguments `other`.", "question_id": 2751},
{"snippet": "Tensor.less_equal_(other)", "intent": "In-place version of less_equal ( ) . With arguments `other`.", "question_id": 2752},
{"snippet": "Tensor.bitwise_or_()", "intent": "In-place version of bitwise_or ( )", "question_id": 2753},
{"snippet": "torch.nn.UpsamplingNearest2d()", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels .", "question_id": 2754},
{"snippet": "torch.nn.UpsamplingNearest2d(size=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 2755},
{"snippet": "torch.nn.UpsamplingNearest2d(scale_factor=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 2756},
{"snippet": "torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 2757},
{"snippet": "Tensor.divide_(value)", "intent": "In-place version of divide ( ) With arguments `value`.", "question_id": 2758},
{"snippet": "Tensor.divide_(value, rounding_mode=None)", "intent": "In-place version of divide ( ) With arguments `value`, `rounding_mode`.", "question_id": 2759},
{"snippet": "torch.nn.BatchNorm3d(num_features)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 2760},
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 2761},
{"snippet": "torch.nn.BatchNorm3d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 2762},
{"snippet": "torch.nn.BatchNorm3d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 2763},
{"snippet": "torch.nn.BatchNorm3d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 2764},
{"snippet": "torch.nn.BatchNorm3d(num_features, device=None)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 2765},
{"snippet": "torch.nn.BatchNorm3d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 2766},
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 2767},
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 2768},
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 2769},
{"snippet": "Tensor.xlogy_(other)", "intent": "In-place version of xlogy ( ) With arguments `other`.", "question_id": 2770},
{"snippet": "torch.triangular_solve(b, A)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices .", "question_id": 2771},
{"snippet": "torch.triangular_solve(b, A, upper=True)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`.", "question_id": 2772},
{"snippet": "torch.triangular_solve(b, A, transpose=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `transpose`.", "question_id": 2773},
{"snippet": "torch.triangular_solve(b, A, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `unitriangular`.", "question_id": 2774},
{"snippet": "torch.triangular_solve(b, A, upper=True, transpose=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `transpose`.", "question_id": 2775},
{"snippet": "torch.triangular_solve(b, A, upper=True, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `unitriangular`.", "question_id": 2776},
{"snippet": "torch.triangular_solve(b, A, transpose=False, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `transpose`, `unitriangular`.", "question_id": 2777},
{"snippet": "torch.triangular_solve(b, A, upper=True, transpose=False, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `transpose`, `unitriangular`.", "question_id": 2778},
{"snippet": "torch.sin(input)", "intent": "Returns a new tensor with the sine of the elements of `input` .", "question_id": 2779},
{"snippet": "torch.sin(input, out=None)", "intent": "Returns a new tensor with the sine of the elements of `input` . With arguments `out`.", "question_id": 2780},
{"snippet": "Tensor.cummin(dim)", "intent": "See torch.cummin ( ) With arguments `dim`.", "question_id": 2781},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 2782},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 2783},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 2784},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 2785},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 2786},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 2787},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 2788},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 2789},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 2790},
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 2791},
{"snippet": "conv2d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 2792},
{"snippet": "torch.autograd.no_grad", "intent": "Context-manager that disabled gradient calculation.", "question_id": 2793},
{"snippet": "torch.linalg.matrix_norm(A, - 1))", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`.", "question_id": 2794},
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro')", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`.", "question_id": 2795},
{"snippet": "torch.linalg.matrix_norm(A, - 1), dim=(- 2)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) Also supports batches of matrices : the norm will be computed over the dimensions specified by the 2-tuple `dim` and the other dimensions will be treated as batch dimensions . With arguments `- 1)`.", "question_id": 2796},
{"snippet": "torch.linalg.matrix_norm(A, - 1), keepdim=False)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `keepdim`.", "question_id": 2797},
{"snippet": "torch.linalg.matrix_norm(A, - 1), dtype=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `dtype`.", "question_id": 2798},
{"snippet": "torch.linalg.matrix_norm(A, - 1), out=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `out`.", "question_id": 2799},
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', dim=(- 2)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . Also supports batches of matrices : the norm will be computed over the dimensions specified by the 2-tuple `dim` and the other dimensions will be treated as batch dimensions . With arguments `- 1)`.", "question_id": 2800},
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', keepdim=False)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `keepdim`.", "question_id": 2801},
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', dtype=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `dtype`.", "question_id": 2802},
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', out=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `out`.", "question_id": 2803},
{"snippet": "Tensor.positive()", "intent": "See torch.positive ( )", "question_id": 2804},
{"snippet": "torch.nn.intrinsic.LinearReLU(linear, relu)", "intent": "This is a sequential container which calls the Linear and ReLU modules . With arguments `linear`, `relu`.", "question_id": 2805},
{"snippet": "torch.fliplr(input)", "intent": "Flip tensor in the left/right direction , returning a new tensor . With arguments `input`.", "question_id": 2806},
{"snippet": "torch.randn_like(input)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) .", "question_id": 2807},
{"snippet": "torch.randn_like(input, dtype=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`.", "question_id": 2808},
{"snippet": "torch.randn_like(input, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `layout`.", "question_id": 2809},
{"snippet": "torch.randn_like(input, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `device`.", "question_id": 2810},
{"snippet": "torch.randn_like(input, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `requires_grad`.", "question_id": 2811},
{"snippet": "torch.randn_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `memory_format`.", "question_id": 2812},
{"snippet": "torch.randn_like(input, dtype=None, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `layout`.", "question_id": 2813},
{"snippet": "torch.randn_like(input, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `device`.", "question_id": 2814},
{"snippet": "torch.randn_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `requires_grad`.", "question_id": 2815},
{"snippet": "torch.randn_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `memory_format`.", "question_id": 2816},
{"snippet": "torch.nn.functional.max_pool2d(*args, **kwargs)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 2817},
{"snippet": "torch.cuda.can_device_access_peer(device, peer_device)", "intent": "Checks if peer access between two devices is possible . With arguments `device`, `peer_device`.", "question_id": 2818},
{"snippet": "torch.sinh(input)", "intent": "Returns a new tensor with the hyperbolic sine of the elements of `input` .", "question_id": 2819},
{"snippet": "torch.sinh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic sine of the elements of `input` . With arguments `out`.", "question_id": 2820},
{"snippet": "torch.nn.functional.adaptive_max_pool1d(*args, **kwargs)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 2821},
{"snippet": "Tensor.log10_()", "intent": "In-place version of log10 ( )", "question_id": 2822},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level .", "question_id": 2823},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, device_ids=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `device_ids`.", "question_id": 2824},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, output_device=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `output_device`.", "question_id": 2825},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, dim=0)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `dim`.", "question_id": 2826},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, broadcast_buffers=True)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `broadcast_buffers`.", "question_id": 2827},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, process_group=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `process_group`.", "question_id": 2828},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, bucket_cap_mb=25)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `bucket_cap_mb`.", "question_id": 2829},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, find_unused_parameters=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `find_unused_parameters`.", "question_id": 2830},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, check_reduction=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `check_reduction`.", "question_id": 2831},
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, gradient_as_bucket_view=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `gradient_as_bucket_view`.", "question_id": 2832},
{"snippet": "distributed_data_parallel.join()", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes .", "question_id": 2833},
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . With arguments `divide_by_initial_world_size`.", "question_id": 2834},
{"snippet": "distributed_data_parallel.join(enable=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop .", "question_id": 2835},
{"snippet": "distributed_data_parallel.join(throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic .", "question_id": 2836},
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, enable=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . With arguments `divide_by_initial_world_size`.", "question_id": 2837},
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic . With arguments `divide_by_initial_world_size`.", "question_id": 2838},
{"snippet": "distributed_data_parallel.join(enable=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic .", "question_id": 2839},
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, enable=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic . With arguments `divide_by_initial_world_size`.", "question_id": 2840},
{"snippet": "distributed_data_parallel.no_sync()", "intent": "A context manager to disable gradient synchronizations across DDP processes .", "question_id": 2841},
{"snippet": "distributed_data_parallel.register_comm_hook(state, hook)", "intent": "Registers a communication `hook` which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers . With arguments `state`.", "question_id": 2842},
{"snippet": "torch.set_flush_denormal(mode)", "intent": "Disables denormal floating numbers on CPU . Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal `mode` .", "question_id": 2843},
{"snippet": "torch.jit.annotate(the_type, the_value)", "intent": "This method is a pass-through function that returns `the_value` , used to hint TorchScript compiler the type of the_value . With arguments `the_type`.", "question_id": 2844},
{"snippet": "torch.sparse.sum(input)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` .", "question_id": 2845},
{"snippet": "torch.sparse.sum(input, dim=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` .", "question_id": 2846},
{"snippet": "torch.sparse.sum(input, dtype=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` . With arguments `dtype`.", "question_id": 2847},
{"snippet": "torch.sparse.sum(input, dim=None, dtype=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` . With arguments `dtype`.", "question_id": 2848},
{"snippet": "Tensor.pow(exponent)", "intent": "See torch.pow ( ) With arguments `exponent`.", "question_id": 2849},
{"snippet": "torch.ger(input, vec2)", "intent": "Alias of torch.outer ( ) . With arguments `input`, `vec2`.", "question_id": 2850},
{"snippet": "torch.ger(input, vec2, out=None)", "intent": "Alias of torch.outer ( ) . With arguments `input`, `vec2`, `out`.", "question_id": 2851},
{"snippet": "torch.nn.LeakyReLU()", "intent": "Applies the element-wise function :", "question_id": 2852},
{"snippet": "torch.nn.LeakyReLU(negative_slope=0.01)", "intent": "Applies the element-wise function : With arguments `negative_slope`.", "question_id": 2853},
{"snippet": "torch.nn.LeakyReLU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 2854},
{"snippet": "torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)", "intent": "Applies the element-wise function : With arguments `negative_slope`, `inplace`.", "question_id": 2855},
{"snippet": "torch.nn.quantized.ReLU6()", "intent": "Applies the element-wise function :", "question_id": 2856},
{"snippet": "torch.nn.quantized.ReLU6(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 2857},
{"snippet": "Tensor.cos()", "intent": "See torch.cos ( )", "question_id": 2858},
{"snippet": "torch.nn.ChannelShuffle(groups)", "intent": "Divide the channels in a tensor of shape ( \u2217 , C , H , W ) ( * , C , H , W ) ( \u2217 , C , H , W ) into g `groups` and rearrange them as ( \u2217 , Cg , g , H , W ) ( * , C \\frac g , g , H , W ) ( \u2217 , C , g\u200bg , H , W ) , while keeping the original tensor shape .", "question_id": 2859},
{"snippet": "torch.nn.LazyBatchNorm3d()", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) .", "question_id": 2860},
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 2861},
{"snippet": "torch.nn.LazyBatchNorm3d(momentum=0.1)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 2862},
{"snippet": "torch.nn.LazyBatchNorm3d(affine=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 2863},
{"snippet": "torch.nn.LazyBatchNorm3d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 2864},
{"snippet": "torch.nn.LazyBatchNorm3d(device=None)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 2865},
{"snippet": "torch.nn.LazyBatchNorm3d(dtype=None)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 2866},
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 2867},
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 2868},
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 2869},
{"snippet": "lazy_batch_norm3d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm3d", "question_id": 2870},
{"snippet": "torch.nn.quantized.functional.clamp(input, min_, max_)", "intent": "float ( `input` , `min_` , `max_` ) - > Tensor", "question_id": 2871},
{"snippet": "torch.arccos(input)", "intent": "Alias for torch.acos ( ) . With arguments `input`.", "question_id": 2872},
{"snippet": "torch.arccos(input, out=None)", "intent": "Alias for torch.acos ( ) . With arguments `input`, `out`.", "question_id": 2873},
{"snippet": "Tensor.round_()", "intent": "In-place version of round ( )", "question_id": 2874},
{"snippet": "torch.nn.Softplus()", "intent": "Applies the element-wise function :", "question_id": 2875},
{"snippet": "torch.nn.Softplus(beta=1)", "intent": "Applies the element-wise function : With arguments `beta`.", "question_id": 2876},
{"snippet": "torch.nn.Softplus(threshold=20)", "intent": "Applies the element-wise function : For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` .", "question_id": 2877},
{"snippet": "torch.nn.Softplus(beta=1, threshold=20)", "intent": "Applies the element-wise function : For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `beta`.", "question_id": 2878},
{"snippet": "Tensor.sinh()", "intent": "See torch.sinh ( )", "question_id": 2879},
{"snippet": "Optimizer.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 2880},
{"snippet": "Optimizer.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 2881},
{"snippet": "torch.fft.fft2(input, - 1))", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`.", "question_id": 2882},
{"snippet": "torch.fft.fft2(input, - 1), s=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`.", "question_id": 2883},
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`.", "question_id": 2884},
{"snippet": "torch.fft.fft2(input, - 1), norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `norm`.", "question_id": 2885},
{"snippet": "torch.fft.fft2(input, - 1), out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `out`.", "question_id": 2886},
{"snippet": "torch.fft.fft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 2887},
{"snippet": "torch.fft.fft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 2888},
{"snippet": "torch.fft.fft2(input, - 1), s=None, out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 2889},
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 2890},
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 2891},
{"snippet": "softplus.apply(module, name, mask)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning `mask` . With arguments `module`, `name`.", "question_id": 2892},
{"snippet": "softplus.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 2893},
{"snippet": "softplus.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 2894},
{"snippet": "softplus.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 2895},
{"snippet": "softplus.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 2896},
{"snippet": "softplus.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 2897},
{"snippet": "softplus.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 2898},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 2899},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 2900},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 2901},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 2902},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 2903},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 2904},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 2905},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 2906},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 2907},
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 2908},
{"snippet": "conv3d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 2909},
{"snippet": "torch.nn.functional.conv2d(input, weight)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`.", "question_id": 2910},
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`.", "question_id": 2911},
{"snippet": "torch.nn.functional.conv2d(input, weight, stride=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `stride`.", "question_id": 2912},
{"snippet": "torch.nn.functional.conv2d(input, weight, padding=0)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `padding`.", "question_id": 2913},
{"snippet": "torch.nn.functional.conv2d(input, weight, dilation=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 2914},
{"snippet": "torch.nn.functional.conv2d(input, weight, groups=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `groups`.", "question_id": 2915},
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, stride=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 2916},
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, padding=0)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 2917},
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, dilation=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 2918},
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, groups=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 2919},
{"snippet": "profile.self_cpu_time_total", "intent": "Returns total time spent on CPU obtained as a sum of all self times across all the events.", "question_id": 2920},
{"snippet": "torch.linalg.slogdet(A)", "intent": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix . For complex `A` , it returns the angle and the natural logarithm of the modulus of the determinant , that is , a logarithmic polar decomposition of the determinant .", "question_id": 2921},
{"snippet": "torch.linalg.slogdet(A, out=None)", "intent": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix . For complex `A` , it returns the angle and the natural logarithm of the modulus of the determinant , that is , a logarithmic polar decomposition of the determinant . With arguments `out`.", "question_id": 2922},
{"snippet": "Tensor.cholesky_inverse()", "intent": "See torch.cholesky_inverse ( )", "question_id": 2923},
{"snippet": "Tensor.cholesky_inverse(upper=False)", "intent": "See torch.cholesky_inverse ( ) With arguments `upper`.", "question_id": 2924},
{"snippet": "torch.rad2deg(input)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees .", "question_id": 2925},
{"snippet": "torch.rad2deg(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees . With arguments `out`.", "question_id": 2926},
{"snippet": "torch.load(f, **pickle_load_args)", "intent": "Loads an object saved with torch.save ( ) from a file . With arguments `f`, `**pickle_load_args`.", "question_id": 2927},
{"snippet": "torch.load(f, **pickle_load_args, map_location=None)", "intent": "Loads an object saved with torch.save ( ) from a file . However , storages can be dynamically remapped to an alternative set of devices using the `map_location` argument . With arguments `f`, `**pickle_load_args`.", "question_id": 2928},
{"snippet": "torch.load(f, **pickle_load_args, pickle_module=pickle)", "intent": "Loads an object saved with torch.save ( ) from a file . With arguments `f`, `**pickle_load_args`, `pickle_module`.", "question_id": 2929},
{"snippet": "torch.load(f, **pickle_load_args, map_location=None, pickle_module=pickle)", "intent": "Loads an object saved with torch.save ( ) from a file . However , storages can be dynamically remapped to an alternative set of devices using the `map_location` argument . With arguments `f`, `**pickle_load_args`, `pickle_module`.", "question_id": 2930},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`.", "question_id": 2931},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 2932},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `nonlinearity`.", "question_id": 2933},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 2934},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `nonlinearity`.", "question_id": 2935},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 2936},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, nonlinearity='tanh', dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `nonlinearity`, `dtype`.", "question_id": 2937},
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `nonlinearity`, `dtype`.", "question_id": 2938},
{"snippet": "torch.bernoulli(input)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number .", "question_id": 2939},
{"snippet": "torch.bernoulli(input, generator=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . With arguments `generator`.", "question_id": 2940},
{"snippet": "torch.bernoulli(input, out=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . The returned `out` tensor only has values 0 or 1 and is of the same shape as input .", "question_id": 2941},
{"snippet": "torch.bernoulli(input, generator=None, out=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . The returned `out` tensor only has values 0 or 1 and is of the same shape as input . With arguments `generator`.", "question_id": 2942},
{"snippet": "Tensor.rad2deg()", "intent": "See torch.rad2deg ( )", "question_id": 2943},
{"snippet": "torch.jit.ignore(**kwargs)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function . With arguments `**kwargs`.", "question_id": 2944},
{"snippet": "torch.jit.ignore(**kwargs, drop=False)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function . With arguments `**kwargs`, `drop`.", "question_id": 2945},
{"snippet": "torch.cuda.set_device(device)", "intent": "Sets the current `device` .", "question_id": 2946},
{"snippet": "torch.nn.quantized.functional.celu(input, scale, zero_point)", "intent": "Applies the quantized CELU function element-wise . With arguments `input`, `scale`, `zero_point`.", "question_id": 2947},
{"snippet": "torch.nn.quantized.functional.celu(input, scale, zero_point, alpha=1.)", "intent": "Applies the quantized CELU function element-wise . With arguments `input`, `scale`, `zero_point`, `alpha`.", "question_id": 2948},
{"snippet": "Tensor.int()", "intent": "self.int ( ) is equivalent to self.to ( torch.int32 ) .", "question_id": 2949},
{"snippet": "Tensor.int(memory_format=torch.preserve_format)", "intent": "self.int ( ) is equivalent to self.to ( torch.int32 ) . With arguments `memory_format`.", "question_id": 2950},
{"snippet": "torch.linalg.qr(A)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 2951},
{"snippet": "torch.linalg.qr(A, mode='reduced')", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `mode` chooses between the full and reduced QR decomposition .", "question_id": 2952},
{"snippet": "torch.linalg.qr(A, out=None)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 2953},
{"snippet": "torch.linalg.qr(A, mode='reduced', out=None)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `mode` chooses between the full and reduced QR decomposition . With arguments `out`.", "question_id": 2954},
{"snippet": "Tensor.less_(other)", "intent": "In-place version of less ( ) . With arguments `other`.", "question_id": 2955},
{"snippet": "Tensor.nan_to_num()", "intent": "See torch.nan_to_num ( ) .", "question_id": 2956},
{"snippet": "Tensor.nan_to_num(nan=0.0)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`.", "question_id": 2957},
{"snippet": "Tensor.nan_to_num(posinf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `posinf`.", "question_id": 2958},
{"snippet": "Tensor.nan_to_num(neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `neginf`.", "question_id": 2959},
{"snippet": "Tensor.nan_to_num(nan=0.0, posinf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `posinf`.", "question_id": 2960},
{"snippet": "Tensor.nan_to_num(nan=0.0, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `neginf`.", "question_id": 2961},
{"snippet": "Tensor.nan_to_num(posinf=None, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `posinf`, `neginf`.", "question_id": 2962},
{"snippet": "Tensor.nan_to_num(nan=0.0, posinf=None, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `posinf`, `neginf`.", "question_id": 2963},
{"snippet": "torch.dstack(tensors)", "intent": "Stack `tensors` in sequence depthwise ( along third axis ) .", "question_id": 2964},
{"snippet": "torch.dstack(tensors, out=None)", "intent": "Stack `tensors` in sequence depthwise ( along third axis ) . With arguments `out`.", "question_id": 2965},
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`.", "question_id": 2966},
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, stride=None)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`.", "question_id": 2967},
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`, `ceil_mode`.", "question_id": 2968},
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`, `ceil_mode`.", "question_id": 2969},
{"snippet": "Tensor.negative()", "intent": "See torch.negative ( )", "question_id": 2970},
{"snippet": "Function.backward(ctx, *grad_outputs)", "intent": "Defines a formula for differentiating the operation . It must accept a context `ctx` as the first argument , followed by as many outputs as the forward ( ) returned ( None will be passed in for non tensor outputs of the forward function ) , and it should return as many tensors , as there were inputs to forward ( ) . With arguments `*grad_outputs`.", "question_id": 2971},
{"snippet": "torch.cdist(x1, x2)", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R .", "question_id": 2972},
{"snippet": "torch.cdist(x1, x2, p=2.0)", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . This function is equivalent to scipy.spatial.distance.cdist ( input , \u2019 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) .", "question_id": 2973},
{"snippet": "torch.cdist(x1, x2, compute_mode='use_mm_for_euclid_dist_if_necessary')", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . With arguments `compute_mode`.", "question_id": 2974},
{"snippet": "torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . This function is equivalent to scipy.spatial.distance.cdist ( input , \u2019 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) . With arguments `compute_mode`.", "question_id": 2975},
{"snippet": "torch.nn.utils.prune.Identity", "intent": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "question_id": 2976},
{"snippet": "identity.apply(module, name)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`.", "question_id": 2977},
{"snippet": "identity.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 2978},
{"snippet": "identity.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 2979},
{"snippet": "identity.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 2980},
{"snippet": "identity.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 2981},
{"snippet": "identity.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 2982},
{"snippet": "identity.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 2983},
{"snippet": "torch.nn.functional.softmin(input)", "intent": "Applies a softmin function . With arguments `input`.", "question_id": 2984},
{"snippet": "torch.nn.functional.softmin(input, dim=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`.", "question_id": 2985},
{"snippet": "torch.nn.functional.softmin(input, _stacklevel=3)", "intent": "Applies a softmin function . With arguments `input`, `_stacklevel`.", "question_id": 2986},
{"snippet": "torch.nn.functional.softmin(input, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dtype`.", "question_id": 2987},
{"snippet": "torch.nn.functional.softmin(input, dim=None, _stacklevel=3)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `_stacklevel`.", "question_id": 2988},
{"snippet": "torch.nn.functional.softmin(input, dim=None, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `dtype`.", "question_id": 2989},
{"snippet": "torch.nn.functional.softmin(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 2990},
{"snippet": "torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `_stacklevel`, `dtype`.", "question_id": 2991},
{"snippet": "Tensor.split(split_size)", "intent": "See torch.split ( ) With arguments `split_size`.", "question_id": 2992},
{"snippet": "Tensor.split(split_size, dim=0)", "intent": "See torch.split ( ) With arguments `split_size`, `dim`.", "question_id": 2993},
{"snippet": "Tensor.logit()", "intent": "See torch.logit ( )", "question_id": 2994},
{"snippet": "Tensor.cholesky_solve(input2)", "intent": "See torch.cholesky_solve ( ) With arguments `input2`.", "question_id": 2995},
{"snippet": "Tensor.cholesky_solve(input2, upper=False)", "intent": "See torch.cholesky_solve ( ) With arguments `input2`, `upper`.", "question_id": 2996},
{"snippet": "Tensor.unique()", "intent": "Returns the unique elements of the input tensor .", "question_id": 2997},
{"snippet": "Tensor.unique(sorted=True)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`.", "question_id": 2998},
{"snippet": "Tensor.unique(return_inverse=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`.", "question_id": 2999},
{"snippet": "Tensor.unique(return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_counts`.", "question_id": 3000},
{"snippet": "Tensor.unique(dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `dim`.", "question_id": 3001},
{"snippet": "Tensor.unique(sorted=True, return_inverse=False)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `return_inverse`.", "question_id": 3002},
{"snippet": "Tensor.unique(sorted=True, return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `return_counts`.", "question_id": 3003},
{"snippet": "Tensor.unique(sorted=True, dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `dim`.", "question_id": 3004},
{"snippet": "Tensor.unique(return_inverse=False, return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`, `return_counts`.", "question_id": 3005},
{"snippet": "Tensor.unique(return_inverse=False, dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`, `dim`.", "question_id": 3006},
{"snippet": "Tensor.permute(*dims)", "intent": "See torch.permute ( ) With arguments `*dims`.", "question_id": 3007},
{"snippet": "torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` with the lowest L `` `n` `` -norm .", "question_id": 3008},
{"snippet": "torch.nn.utils.prune.ln_structured(module, name, amount, n, dim, importance_scores=None)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` with the lowest L `` `n` `` -norm . With arguments `importance_scores`.", "question_id": 3009},
{"snippet": "torch.tanh(input)", "intent": "Returns a new tensor with the hyperbolic tangent of the elements of `input` .", "question_id": 3010},
{"snippet": "torch.tanh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic tangent of the elements of `input` . With arguments `out`.", "question_id": 3011},
{"snippet": "Tensor.lcm_(other)", "intent": "In-place version of lcm ( ) With arguments `other`.", "question_id": 3012},
{"snippet": "Tensor.unbind()", "intent": "See torch.unbind ( )", "question_id": 3013},
{"snippet": "Tensor.unbind(dim=0)", "intent": "See torch.unbind ( ) With arguments `dim`.", "question_id": 3014},
{"snippet": "torch.optim.AdamW(params, 0.999))", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`.", "question_id": 3015},
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`.", "question_id": 3016},
{"snippet": "torch.optim.AdamW(params, 0.999), betas=(0.9)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `betas`.", "question_id": 3017},
{"snippet": "torch.optim.AdamW(params, 0.999), eps=1e-08)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `eps`.", "question_id": 3018},
{"snippet": "torch.optim.AdamW(params, 0.999), weight_decay=0.01)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 3019},
{"snippet": "torch.optim.AdamW(params, 0.999), amsgrad=False)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `amsgrad`.", "question_id": 3020},
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 3021},
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 3022},
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, weight_decay=0.01)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 3023},
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, amsgrad=False)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `amsgrad`.", "question_id": 3024},
{"snippet": "adam_w.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3025},
{"snippet": "adam_w.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3026},
{"snippet": "adam_w.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3027},
{"snippet": "adam_w.step()", "intent": "Performs a single optimization step .", "question_id": 3028},
{"snippet": "adam_w.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3029},
{"snippet": "adam_w.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3030},
{"snippet": "adam_w.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3031},
{"snippet": "Tensor.frac()", "intent": "See torch.frac ( )", "question_id": 3032},
{"snippet": "torch.fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max)", "intent": "Returns a new tensor with the data in `input` fake quantized per channel using `scale` , `zero_point` , `quant_min` and `quant_max` , across the channel specified by axis .", "question_id": 3033},
{"snippet": "torch.nn.Hardtanh()", "intent": "Applies the HardTanh function element-wise", "question_id": 3034},
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 3035},
{"snippet": "torch.nn.Hardtanh(max_val=1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 3036},
{"snippet": "torch.nn.Hardtanh(inplace=False)", "intent": "Applies the HardTanh function element-wise With arguments `inplace`.", "question_id": 3037},
{"snippet": "torch.nn.Hardtanh(min_value=None)", "intent": "Applies the HardTanh function element-wise Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 3038},
{"snippet": "torch.nn.Hardtanh(max_value=None)", "intent": "Applies the HardTanh function element-wise Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 3039},
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, max_val=1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 3040},
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, inplace=False)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . With arguments `inplace`.", "question_id": 3041},
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, min_value=None)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 3042},
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, max_value=None)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 3043},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 3044},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 3045},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 3046},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 3047},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 3048},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 3049},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 3050},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 3051},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 3052},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 3053},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 3054},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 3055},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 3056},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 3057},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 3058},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 3059},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 3060},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 3061},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 3062},
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 3063},
{"snippet": "lazy_conv_transpose1d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose1d", "question_id": 3064},
{"snippet": "torch.quantization.qconfig.QConfig(activation, weight)", "intent": "Describes how to quantize a layer or a part of the network by providing settings ( observer classes ) for activations and weights respectively . With arguments `activation`, `weight`.", "question_id": 3065},
{"snippet": "torch.optim.Adam(params, 0.999))", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`.", "question_id": 3066},
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`.", "question_id": 3067},
{"snippet": "torch.optim.Adam(params, 0.999), betas=(0.9)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `betas`.", "question_id": 3068},
{"snippet": "torch.optim.Adam(params, 0.999), eps=1e-08)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `eps`.", "question_id": 3069},
{"snippet": "torch.optim.Adam(params, 0.999), weight_decay=0)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 3070},
{"snippet": "torch.optim.Adam(params, 0.999), amsgrad=False)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `amsgrad`.", "question_id": 3071},
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 3072},
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 3073},
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, weight_decay=0)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 3074},
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, amsgrad=False)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `amsgrad`.", "question_id": 3075},
{"snippet": "adam.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3076},
{"snippet": "adam.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3077},
{"snippet": "adam.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3078},
{"snippet": "adam.step()", "intent": "Performs a single optimization step .", "question_id": 3079},
{"snippet": "adam.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3080},
{"snippet": "adam.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3081},
{"snippet": "adam.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3082},
{"snippet": "torch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max)", "intent": "Returns a new tensor with the data in `input` fake quantized using `scale` , `zero_point` , `quant_min` and `quant_max` .", "question_id": 3083},
{"snippet": "torch.diagflat(input)", "intent": "The argument `offset` controls which diagonal to consider : With arguments `input`.", "question_id": 3084},
{"snippet": "torch.diagflat(input, offset=0)", "intent": "The argument `offset` controls which diagonal to consider : With arguments `input`.", "question_id": 3085},
{"snippet": "Tensor.trunc()", "intent": "See torch.trunc ( )", "question_id": 3086},
{"snippet": "torch.linalg.matrix_power(A, n)", "intent": "Computes the n-th power of a square matrix for an integer `n` . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 3087},
{"snippet": "torch.linalg.matrix_power(A, n, out=None)", "intent": "Computes the n-th power of a square matrix for an integer `n` . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 3088},
{"snippet": "torch.nn.ParameterDict()", "intent": "Holds `parameters` in a dictionary .", "question_id": 3089},
{"snippet": "torch.nn.ParameterDict(parameters=None)", "intent": "Holds `parameters` in a dictionary .", "question_id": 3090},
{"snippet": "parameter_dict.clear()", "intent": "Remove all items from the ParameterDict .", "question_id": 3091},
{"snippet": "parameter_dict.items()", "intent": "Return an iterable of the ParameterDict key/value pairs .", "question_id": 3092},
{"snippet": "parameter_dict.keys()", "intent": "Return an iterable of the ParameterDict keys .", "question_id": 3093},
{"snippet": "parameter_dict.pop(key)", "intent": "Remove `key` from the ParameterDict and return its parameter .", "question_id": 3094},
{"snippet": "parameter_dict.update(parameters)", "intent": "Update the ParameterDict with the key-value pairs from a mapping or an iterable , overwriting existing keys . With arguments `parameters`.", "question_id": 3095},
{"snippet": "parameter_dict.values()", "intent": "Return an iterable of the ParameterDict values .", "question_id": 3096},
{"snippet": "Tensor.log_normal_()", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 .", "question_id": 3097},
{"snippet": "Tensor.log_normal_(mean=1)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 .", "question_id": 3098},
{"snippet": "Tensor.log_normal_(std=2)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution :", "question_id": 3099},
{"snippet": "Tensor.log_normal_(generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . With arguments `generator`.", "question_id": 3100},
{"snippet": "Tensor.log_normal_(mean=1, std=2)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution :", "question_id": 3101},
{"snippet": "Tensor.log_normal_(mean=1, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . With arguments `generator`.", "question_id": 3102},
{"snippet": "Tensor.log_normal_(std=2, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution : With arguments `generator`.", "question_id": 3103},
{"snippet": "Tensor.log_normal_(mean=1, std=2, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution : With arguments `generator`.", "question_id": 3104},
{"snippet": "torch.argmin(input)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`.", "question_id": 3105},
{"snippet": "torch.argmin(input, dim=None)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `dim`.", "question_id": 3106},
{"snippet": "torch.argmin(input, keepdim=False)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `keepdim`.", "question_id": 3107},
{"snippet": "torch.argmin(input, dim=None, keepdim=False)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `dim`, `keepdim`.", "question_id": 3108},
{"snippet": "torch.quantization.default_eval_fn(model, calib_data)", "intent": "Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the `model` on the dataset With arguments `calib_data`.", "question_id": 3109},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`.", "question_id": 3110},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`.", "question_id": 3111},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, bias=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `bias`.", "question_id": 3112},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `eps`.", "question_id": 3113},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `bias`.", "question_id": 3114},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `eps`.", "question_id": 3115},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, bias=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `bias`, `eps`.", "question_id": 3116},
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `bias`, `eps`.", "question_id": 3117},
{"snippet": "torch.nn.functional.conv1d(input, weight)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`.", "question_id": 3118},
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`.", "question_id": 3119},
{"snippet": "torch.nn.functional.conv1d(input, weight, stride=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `stride`.", "question_id": 3120},
{"snippet": "torch.nn.functional.conv1d(input, weight, padding=0)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `padding`.", "question_id": 3121},
{"snippet": "torch.nn.functional.conv1d(input, weight, dilation=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 3122},
{"snippet": "torch.nn.functional.conv1d(input, weight, groups=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `groups`.", "question_id": 3123},
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, stride=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 3124},
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, padding=0)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 3125},
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, dilation=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 3126},
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, groups=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 3127},
{"snippet": "Tensor.amin()", "intent": "See torch.amin ( )", "question_id": 3128},
{"snippet": "Tensor.amin(dim=None)", "intent": "See torch.amin ( ) With arguments `dim`.", "question_id": 3129},
{"snippet": "Tensor.amin(keepdim=False)", "intent": "See torch.amin ( ) With arguments `keepdim`.", "question_id": 3130},
{"snippet": "Tensor.amin(dim=None, keepdim=False)", "intent": "See torch.amin ( ) With arguments `dim`, `keepdim`.", "question_id": 3131},
{"snippet": "torch.cummax(input, dim)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative maximum of elements of `input` in the dimension `dim` .", "question_id": 3132},
{"snippet": "torch.cummax(input, dim, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative maximum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 3133},
{"snippet": "torch.nn.SyncBatchNorm(num_features)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 3134},
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 3135},
{"snippet": "torch.nn.SyncBatchNorm(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 3136},
{"snippet": "torch.nn.SyncBatchNorm(num_features, affine=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 3137},
{"snippet": "torch.nn.SyncBatchNorm(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 3138},
{"snippet": "torch.nn.SyncBatchNorm(num_features, process_group=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `process_group`.", "question_id": 3139},
{"snippet": "torch.nn.SyncBatchNorm(num_features, device=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 3140},
{"snippet": "torch.nn.SyncBatchNorm(num_features, dtype=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 3141},
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 3142},
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 3143},
{"snippet": "sync_batch_norm.convert_sync_batchnorm(module)", "intent": "Helper function to convert all BatchNorm * D layers in the model to torch.nn.SyncBatchNorm layers . With arguments `module`.", "question_id": 3144},
{"snippet": "sync_batch_norm.convert_sync_batchnorm(module, process_group=None)", "intent": "Helper function to convert all BatchNorm * D layers in the model to torch.nn.SyncBatchNorm layers . With arguments `module`, `process_group`.", "question_id": 3145},
{"snippet": "torch.svd_lowrank(A)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`.", "question_id": 3146},
{"snippet": "torch.svd_lowrank(A, q=6)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`.", "question_id": 3147},
{"snippet": "torch.svd_lowrank(A, niter=2)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `niter`.", "question_id": 3148},
{"snippet": "torch.svd_lowrank(A, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `M`.", "question_id": 3149},
{"snippet": "torch.svd_lowrank(A, q=6, niter=2)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `niter`.", "question_id": 3150},
{"snippet": "torch.svd_lowrank(A, q=6, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `M`.", "question_id": 3151},
{"snippet": "torch.svd_lowrank(A, niter=2, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `niter`, `M`.", "question_id": 3152},
{"snippet": "torch.svd_lowrank(A, q=6, niter=2, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `niter`, `M`.", "question_id": 3153},
{"snippet": "torch.where(condition, x, y)", "intent": "Return a tensor of elements selected from either `x` or `y` , depending on `condition` .", "question_id": 3154},
{"snippet": "torch.nn.LogSoftmax()", "intent": "Applies the log\u2061 ( Softmax ( x ) ) \\log ( \\text { Softmax } ( x ) ) log ( Softmax ( x ) ) function to an n-dimensional input Tensor .", "question_id": 3155},
{"snippet": "torch.nn.LogSoftmax(dim=None)", "intent": "Applies the log\u2061 ( Softmax ( x ) ) \\log ( \\text { Softmax } ( x ) ) log ( Softmax ( x ) ) function to an n-dimensional input Tensor . With arguments `dim`.", "question_id": 3156},
{"snippet": "torch.is_grad_enabled()", "intent": "Returns True if grad mode is currently enabled .", "question_id": 3157},
{"snippet": "torch.eq(input, other)", "intent": "Computes element-wise equality With arguments `input`, `other`.", "question_id": 3158},
{"snippet": "torch.eq(input, other, out=None)", "intent": "Computes element-wise equality With arguments `input`, `other`, `out`.", "question_id": 3159},
{"snippet": "Tensor.mean()", "intent": "See torch.mean ( )", "question_id": 3160},
{"snippet": "Tensor.mean(dim=None)", "intent": "See torch.mean ( ) With arguments `dim`.", "question_id": 3161},
{"snippet": "Tensor.mean(keepdim=False)", "intent": "See torch.mean ( ) With arguments `keepdim`.", "question_id": 3162},
{"snippet": "Tensor.mean(dim=None, keepdim=False)", "intent": "See torch.mean ( ) With arguments `dim`, `keepdim`.", "question_id": 3163},
{"snippet": "torch.cuda.Stream(**kwargs)", "intent": "Wrapper around a CUDA stream . With arguments `**kwargs`.", "question_id": 3164},
{"snippet": "torch.cuda.Stream(**kwargs, device=None)", "intent": "Wrapper around a CUDA stream . A CUDA stream is a linear sequence of execution that belongs to a specific `device` , independent from other streams . With arguments `**kwargs`.", "question_id": 3165},
{"snippet": "torch.cuda.Stream(**kwargs, priority=0)", "intent": "Wrapper around a CUDA stream . With arguments `**kwargs`, `priority`.", "question_id": 3166},
{"snippet": "torch.cuda.Stream(**kwargs, device=None, priority=0)", "intent": "Wrapper around a CUDA stream . A CUDA stream is a linear sequence of execution that belongs to a specific `device` , independent from other streams . With arguments `**kwargs`, `priority`.", "question_id": 3167},
{"snippet": "stream.query()", "intent": "Checks if all the work submitted has been completed .", "question_id": 3168},
{"snippet": "stream.record_event()", "intent": "Records an `event` .", "question_id": 3169},
{"snippet": "stream.record_event(event=None)", "intent": "Records an `event` .", "question_id": 3170},
{"snippet": "stream.synchronize()", "intent": "Wait for all the kernels in this stream to complete .", "question_id": 3171},
{"snippet": "stream.wait_event(event)", "intent": "Makes all future work submitted to the stream wait for an `event` .", "question_id": 3172},
{"snippet": "stream.wait_stream(stream)", "intent": "Synchronizes with another `stream` .", "question_id": 3173},
{"snippet": "torch.permute(input, dims)", "intent": "Returns a view of the original tensor `input` with its dimensions permuted . With arguments `dims`.", "question_id": 3174},
{"snippet": "torch.nn.functional.upsample_bilinear(input)", "intent": "Upsamples the `input` , using bilinear upsampling .", "question_id": 3175},
{"snippet": "torch.nn.functional.upsample_bilinear(input, size=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`.", "question_id": 3176},
{"snippet": "torch.nn.functional.upsample_bilinear(input, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `scale_factor`.", "question_id": 3177},
{"snippet": "torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`, `scale_factor`.", "question_id": 3178},
{"snippet": "Tensor.digamma_()", "intent": "In-place version of digamma ( )", "question_id": 3179},
{"snippet": "torch.bartlett_window(window_length)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 3180},
{"snippet": "torch.bartlett_window(window_length, periodic=True)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 3181},
{"snippet": "torch.bartlett_window(window_length, dtype=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 3182},
{"snippet": "torch.bartlett_window(window_length, layout=torch.strided)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 3183},
{"snippet": "torch.bartlett_window(window_length, device=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 3184},
{"snippet": "torch.bartlett_window(window_length, requires_grad=False)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 3185},
{"snippet": "torch.bartlett_window(window_length, periodic=True, dtype=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 3186},
{"snippet": "torch.bartlett_window(window_length, periodic=True, layout=torch.strided)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 3187},
{"snippet": "torch.bartlett_window(window_length, periodic=True, device=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 3188},
{"snippet": "torch.bartlett_window(window_length, periodic=True, requires_grad=False)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 3189},
{"snippet": "torch.cuda.get_arch_list()", "intent": "Returns list CUDA architectures this library was compiled for .", "question_id": 3190},
{"snippet": "torch.nn.LayerNorm(normalized_shape)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` .", "question_id": 3191},
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`.", "question_id": 3192},
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True .", "question_id": 3193},
{"snippet": "torch.nn.LayerNorm(normalized_shape, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `device`.", "question_id": 3194},
{"snippet": "torch.nn.LayerNorm(normalized_shape, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `dtype`.", "question_id": 3195},
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `eps`.", "question_id": 3196},
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`, `device`.", "question_id": 3197},
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`, `dtype`.", "question_id": 3198},
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `device`.", "question_id": 3199},
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `dtype`.", "question_id": 3200},
{"snippet": "torch.bincount(input)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 .", "question_id": 3201},
{"snippet": "torch.bincount(input, weights=None)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If n is the value at position i , out [ n ] += `weights` [ i ] if weights is specified else out [ n ] += 1 .", "question_id": 3202},
{"snippet": "torch.bincount(input, minlength=0)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If `minlength` is specified , the number of bins is at least minlength and if input is empty , then the result is tensor of size minlength filled with zeros .", "question_id": 3203},
{"snippet": "torch.bincount(input, weights=None, minlength=0)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If n is the value at position i , out [ n ] += `weights` [ i ] if weights is specified else out [ n ] += 1 . If `minlength` is specified , the number of bins is at least minlength and if input is empty , then the result is tensor of size minlength filled with zeros .", "question_id": 3204},
{"snippet": "torch.quantization.DeQuantStub", "intent": "Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.", "question_id": 3205},
{"snippet": "torch.nn.AvgPool1d(kernel_size)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as :", "question_id": 3206},
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple .", "question_id": 3207},
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 3208},
{"snippet": "torch.nn.AvgPool1d(kernel_size, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : With arguments `ceil_mode`.", "question_id": 3209},
{"snippet": "torch.nn.AvgPool1d(kernel_size, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : With arguments `count_include_pad`.", "question_id": 3210},
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, padding=0)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 3211},
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . With arguments `ceil_mode`.", "question_id": 3212},
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . With arguments `count_include_pad`.", "question_id": 3213},
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points . With arguments `ceil_mode`.", "question_id": 3214},
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points . With arguments `count_include_pad`.", "question_id": 3215},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 3216},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 3217},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 3218},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 3219},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 3220},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 3221},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 3222},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 3223},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 3224},
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 3225},
{"snippet": "lazy_conv_transpose2d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose2d", "question_id": 3226},
{"snippet": "torch.nn.Tanhshrink", "intent": "Applies the element-wise function:", "question_id": 3227},
{"snippet": "torch.nn.Fold(output_size, kernel_size)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks .", "question_id": 3228},
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3229},
{"snippet": "torch.nn.Fold(output_size, kernel_size, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3230},
{"snippet": "torch.nn.Fold(output_size, kernel_size, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3231},
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3232},
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3233},
{"snippet": "torch.nn.Fold(output_size, kernel_size, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3234},
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3235},
{"snippet": "torch.positive(input)", "intent": "Returns `input` .", "question_id": 3236},
{"snippet": "torch.lstsq(input, A)", "intent": "Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size ( m\u00d7n ) ( m \\times n ) ( m\u00d7n ) and a matrix BBB of size ( m\u00d7k ) ( m \\times k ) ( m\u00d7k ) . With arguments `input`, `A`.", "question_id": 3237},
{"snippet": "torch.lstsq(input, A, out=None)", "intent": "Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size ( m\u00d7n ) ( m \\times n ) ( m\u00d7n ) and a matrix BBB of size ( m\u00d7k ) ( m \\times k ) ( m\u00d7k ) . With arguments `input`, `A`, `out`.", "question_id": 3238},
{"snippet": "Tensor.asinh()", "intent": "See torch.asinh ( )", "question_id": 3239},
{"snippet": "torch.nn.FeatureAlphaDropout()", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g .", "question_id": 3240},
{"snippet": "torch.nn.FeatureAlphaDropout(p=0.5)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . Each element will be masked independently for each sample on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 3241},
{"snippet": "torch.nn.FeatureAlphaDropout(inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . With arguments `inplace`.", "question_id": 3242},
{"snippet": "torch.nn.FeatureAlphaDropout(p=0.5, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . Each element will be masked independently for each sample on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 3243},
{"snippet": "torch.exp2(input)", "intent": "Alias for torch.special.exp2 ( ) . With arguments `input`.", "question_id": 3244},
{"snippet": "torch.exp2(input, out=None)", "intent": "Alias for torch.special.exp2 ( ) . With arguments `input`, `out`.", "question_id": 3245},
{"snippet": "Tensor.copysign_(other)", "intent": "In-place version of copysign ( ) With arguments `other`.", "question_id": 3246},
{"snippet": "torch.nn.SoftMarginLoss()", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) .", "question_id": 3247},
{"snippet": "torch.nn.SoftMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 3248},
{"snippet": "torch.nn.SoftMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 3249},
{"snippet": "torch.nn.SoftMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 3250},
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 3251},
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 3252},
{"snippet": "torch.nn.SoftMarginLoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduce`, `reduction`.", "question_id": 3253},
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`, `reduction`.", "question_id": 3254},
{"snippet": "torch.tril_indices(row, col)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates .", "question_id": 3255},
{"snippet": "torch.tril_indices(row, col, offset=0)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider .", "question_id": 3256},
{"snippet": "torch.tril_indices(row, col, dtype=torch.long)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`.", "question_id": 3257},
{"snippet": "torch.tril_indices(row, col, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `device`.", "question_id": 3258},
{"snippet": "torch.tril_indices(row, col, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `layout`.", "question_id": 3259},
{"snippet": "torch.tril_indices(row, col, offset=0, dtype=torch.long)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `dtype`.", "question_id": 3260},
{"snippet": "torch.tril_indices(row, col, offset=0, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `device`.", "question_id": 3261},
{"snippet": "torch.tril_indices(row, col, offset=0, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `layout`.", "question_id": 3262},
{"snippet": "torch.tril_indices(row, col, dtype=torch.long, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `device`.", "question_id": 3263},
{"snippet": "torch.tril_indices(row, col, dtype=torch.long, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `layout`.", "question_id": 3264},
{"snippet": "torch.nn.AdaptiveMaxPool2d(output_size)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 3265},
{"snippet": "torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 3266},
{"snippet": "torch.linalg.cholesky(A)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 3267},
{"snippet": "torch.linalg.cholesky(A, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 3268},
{"snippet": "torch.jit.ScriptModule", "intent": "A wrapper around C++ torch::jit::Module.", "question_id": 3269},
{"snippet": "script_module.add_module(name, module)", "intent": "Adds a child `module` to the current module . The module can be accessed as an attribute using the given `name` .", "question_id": 3270},
{"snippet": "script_module.apply(fn)", "intent": "Applies `fn` recursively to every submodule ( as returned by .children ( ) ) as well as self .", "question_id": 3271},
{"snippet": "script_module.bfloat16()", "intent": "Casts all floating point parameters and buffers to bfloat16 datatype .", "question_id": 3272},
{"snippet": "script_module.buffers()", "intent": "Returns an iterator over module buffers .", "question_id": 3273},
{"snippet": "script_module.buffers(recurse=True)", "intent": "Returns an iterator over module buffers . With arguments `recurse`.", "question_id": 3274},
{"snippet": "script_module.children()", "intent": "Returns an iterator over immediate children modules .", "question_id": 3275},
{"snippet": "code", "intent": "Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method.", "question_id": 3276},
{"snippet": "code_with_constants", "intent": "Returns a tuple of:", "question_id": 3277},
{"snippet": "script_module.cpu()", "intent": "Moves all model parameters and buffers to the CPU .", "question_id": 3278},
{"snippet": "script_module.cuda()", "intent": "Moves all model parameters and buffers to the GPU .", "question_id": 3279},
{"snippet": "script_module.cuda(device=None)", "intent": "Moves all model parameters and buffers to the GPU . With arguments `device`.", "question_id": 3280},
{"snippet": "script_module.double()", "intent": "Casts all floating point parameters and buffers to double datatype .", "question_id": 3281},
{"snippet": "script_module.eval()", "intent": "Sets the module in evaluation mode .", "question_id": 3282},
{"snippet": "script_module.extra_repr()", "intent": "Set the extra representation of the module", "question_id": 3283},
{"snippet": "script_module.float()", "intent": "Casts all floating point parameters and buffers to float datatype .", "question_id": 3284},
{"snippet": "script_module.get_buffer(target)", "intent": "Returns the buffer given by `target` if it exists , otherwise throws an error .", "question_id": 3285},
{"snippet": "script_module.get_parameter(target)", "intent": "Returns the parameter given by `target` if it exists , otherwise throws an error .", "question_id": 3286},
{"snippet": "script_module.get_submodule(target)", "intent": "Returns the submodule given by `target` if it exists , otherwise throws an error .", "question_id": 3287},
{"snippet": "graph", "intent": "Returns a string representation of the internal graph for the forward method.", "question_id": 3288},
{"snippet": "script_module.half()", "intent": "Casts all floating point parameters and buffers to half datatype .", "question_id": 3289},
{"snippet": "inlined_graph", "intent": "Returns a string representation of the internal graph for the forward method.", "question_id": 3290},
{"snippet": "script_module.load_state_dict(state_dict)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants .", "question_id": 3291},
{"snippet": "script_module.load_state_dict(state_dict, strict=True)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants . If `strict` is True , then the keys of state_dict must exactly match the keys returned by this module \u2019 s state_dict ( ) function .", "question_id": 3292},
{"snippet": "script_module.modules()", "intent": "Returns an iterator over all modules in the network .", "question_id": 3293},
{"snippet": "script_module.named_buffers()", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself .", "question_id": 3294},
{"snippet": "script_module.named_buffers(prefix='')", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`.", "question_id": 3295},
{"snippet": "script_module.named_buffers(recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `recurse`.", "question_id": 3296},
{"snippet": "script_module.named_buffers(prefix='', recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`, `recurse`.", "question_id": 3297},
{"snippet": "script_module.named_children()", "intent": "Returns an iterator over immediate children modules , yielding both the name of the module as well as the module itself .", "question_id": 3298},
{"snippet": "script_module.named_modules()", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself .", "question_id": 3299},
{"snippet": "script_module.named_modules(memo=None)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`.", "question_id": 3300},
{"snippet": "script_module.named_modules(prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`.", "question_id": 3301},
{"snippet": "script_module.named_modules(remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `remove_duplicate`.", "question_id": 3302},
{"snippet": "script_module.named_modules(memo=None, prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`.", "question_id": 3303},
{"snippet": "script_module.named_modules(memo=None, remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `remove_duplicate`.", "question_id": 3304},
{"snippet": "script_module.named_modules(prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`, `remove_duplicate`.", "question_id": 3305},
{"snippet": "script_module.named_modules(memo=None, prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`, `remove_duplicate`.", "question_id": 3306},
{"snippet": "script_module.named_parameters()", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself .", "question_id": 3307},
{"snippet": "script_module.named_parameters(prefix='')", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`.", "question_id": 3308},
{"snippet": "script_module.named_parameters(recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `recurse`.", "question_id": 3309},
{"snippet": "script_module.named_parameters(prefix='', recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`, `recurse`.", "question_id": 3310},
{"snippet": "script_module.parameters()", "intent": "Returns an iterator over module parameters .", "question_id": 3311},
{"snippet": "script_module.parameters(recurse=True)", "intent": "Returns an iterator over module parameters . With arguments `recurse`.", "question_id": 3312},
{"snippet": "script_module.register_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 3313},
{"snippet": "script_module.register_buffer(name, tensor)", "intent": "Adds a buffer to the module . With arguments `name`, `tensor`.", "question_id": 3314},
{"snippet": "script_module.register_buffer(name, tensor, persistent=True)", "intent": "Adds a buffer to the module . Buffers , by default , are `persistent` and will be saved alongside parameters . With arguments `name`, `tensor`.", "question_id": 3315},
{"snippet": "script_module.register_forward_hook(hook)", "intent": "Registers a forward `hook` on the module .", "question_id": 3316},
{"snippet": "script_module.register_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook on the module . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 3317},
{"snippet": "script_module.register_full_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 3318},
{"snippet": "script_module.register_parameter(name, param)", "intent": "Adds a parameter to the module . The parameter can be accessed as an attribute using given `name` . With arguments `param`.", "question_id": 3319},
{"snippet": "script_module.requires_grad_()", "intent": "Change if autograd should record operations on parameters in this module .", "question_id": 3320},
{"snippet": "script_module.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on parameters in this module . This method sets the parameters \u2019 `requires_grad` attributes in-place .", "question_id": 3321},
{"snippet": "script_module.save(f)", "intent": "See torch.jit.save for details . With arguments `f`.", "question_id": 3322},
{"snippet": "script_module.save(f, _extra_files={})", "intent": "See torch.jit.save for details . With arguments `f`, `_extra_files`.", "question_id": 3323},
{"snippet": "script_module.share_memory()", "intent": "See torch.Tensor.share_memory_ ( )", "question_id": 3324},
{"snippet": "script_module.state_dict()", "intent": "Returns a dictionary containing a whole state of the module .", "question_id": 3325},
{"snippet": "script_module.state_dict(destination=None)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`.", "question_id": 3326},
{"snippet": "script_module.state_dict(prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`.", "question_id": 3327},
{"snippet": "script_module.state_dict(keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `keep_vars`.", "question_id": 3328},
{"snippet": "script_module.state_dict(destination=None, prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`.", "question_id": 3329},
{"snippet": "script_module.state_dict(destination=None, keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `keep_vars`.", "question_id": 3330},
{"snippet": "script_module.state_dict(prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`, `keep_vars`.", "question_id": 3331},
{"snippet": "script_module.state_dict(destination=None, prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`, `keep_vars`.", "question_id": 3332},
{"snippet": "script_module.to(*args, **kwargs)", "intent": "Moves and/or casts the parameters and buffers . With arguments `*args`, `**kwargs`.", "question_id": 3333},
{"snippet": "script_module.to_empty(device)", "intent": "Moves the parameters and buffers to the specified `device` without copying storage .", "question_id": 3334},
{"snippet": "script_module.train()", "intent": "Sets the module in training `mode` .", "question_id": 3335},
{"snippet": "script_module.train(mode=True)", "intent": "Sets the module in training `mode` .", "question_id": 3336},
{"snippet": "script_module.type(dst_type)", "intent": "Casts all parameters and buffers to `dst_type` .", "question_id": 3337},
{"snippet": "script_module.xpu()", "intent": "Moves all model parameters and buffers to the XPU .", "question_id": 3338},
{"snippet": "script_module.xpu(device=None)", "intent": "Moves all model parameters and buffers to the XPU . With arguments `device`.", "question_id": 3339},
{"snippet": "script_module.zero_grad()", "intent": "Sets gradients of all model parameters to zero .", "question_id": 3340},
{"snippet": "script_module.zero_grad(set_to_none=False)", "intent": "Sets gradients of all model parameters to zero . With arguments `set_to_none`.", "question_id": 3341},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`.", "question_id": 3342},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`.", "question_id": 3343},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`.", "question_id": 3344},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `dim`.", "question_id": 3345},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `module_kwargs`.", "question_id": 3346},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`.", "question_id": 3347},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `dim`.", "question_id": 3348},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `module_kwargs`.", "question_id": 3349},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`, `dim`.", "question_id": 3350},
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`, `module_kwargs`.", "question_id": 3351},
{"snippet": "Tensor.lu_solve(LU_data, LU_pivots)", "intent": "See torch.lu_solve ( ) With arguments `LU_data`, `LU_pivots`.", "question_id": 3352},
{"snippet": "torch.cuda.empty_cache()", "intent": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi .", "question_id": 3353},
{"snippet": "Tensor.sigmoid_()", "intent": "In-place version of sigmoid ( )", "question_id": 3354},
{"snippet": "torch.nn.functional.prelu(input, weight)", "intent": "Applies element-wise the function PReLU ( x ) =max\u2061 ( 0 , x ) +weight\u2217min\u2061 ( 0 , x ) \\text { PReLU } ( x ) = \\max ( 0 , x ) + \\text { `weight` } * \\min ( 0 , x ) PReLU ( x ) =max ( 0 , x ) +weight\u2217min ( 0 , x ) where weight is a learnable parameter . With arguments `input`.", "question_id": 3355},
{"snippet": "torch.logical_and(input, other)", "intent": "Computes the element-wise logical AND of the given `input` tensors . With arguments `other`.", "question_id": 3356},
{"snippet": "torch.logical_and(input, other, out=None)", "intent": "Computes the element-wise logical AND of the given `input` tensors . With arguments `other`, `out`.", "question_id": 3357},
{"snippet": "torch.nn.DataParallel(module)", "intent": "Implements data parallelism at the `module` level .", "question_id": 3358},
{"snippet": "torch.nn.DataParallel(module, device_ids=None)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module .", "question_id": 3359},
{"snippet": "torch.nn.DataParallel(module, output_device=None)", "intent": "Implements data parallelism at the `module` level . With arguments `output_device`.", "question_id": 3360},
{"snippet": "torch.nn.DataParallel(module, dim=0)", "intent": "Implements data parallelism at the `module` level . tensors will be scattered on `dim` specified ( default 0 ) .", "question_id": 3361},
{"snippet": "torch.nn.DataParallel(module, device_ids=None, output_device=None)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . With arguments `output_device`.", "question_id": 3362},
{"snippet": "torch.nn.DataParallel(module, device_ids=None, dim=0)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . tensors will be scattered on `dim` specified ( default 0 ) .", "question_id": 3363},
{"snippet": "torch.nn.DataParallel(module, output_device=None, dim=0)", "intent": "Implements data parallelism at the `module` level . tensors will be scattered on `dim` specified ( default 0 ) . With arguments `output_device`.", "question_id": 3364},
{"snippet": "torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . tensors will be scattered on `dim` specified ( default 0 ) . With arguments `output_device`.", "question_id": 3365},
{"snippet": "Tensor.logdet()", "intent": "See torch.logdet ( )", "question_id": 3366},
{"snippet": "Tensor.float()", "intent": "self.float ( ) is equivalent to self.to ( torch.float32 ) .", "question_id": 3367},
{"snippet": "Tensor.float(memory_format=torch.preserve_format)", "intent": "self.float ( ) is equivalent to self.to ( torch.float32 ) . With arguments `memory_format`.", "question_id": 3368},
{"snippet": "Tensor.imag", "intent": "Returns a new tensor containing imaginary values of the self tensor.", "question_id": 3369},
{"snippet": "torch.broadcast_tensors(*tensors)", "intent": "Broadcasts the given tensors according to Broadcasting semantics . With arguments `*tensors`.", "question_id": 3370},
{"snippet": "torch.cuda.get_device_capability()", "intent": "Gets the cuda capability of a `device` .", "question_id": 3371},
{"snippet": "torch.cuda.get_device_capability(device=None)", "intent": "Gets the cuda capability of a `device` .", "question_id": 3372},
{"snippet": "torch.cuda.max_memory_reserved()", "intent": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 3373},
{"snippet": "torch.cuda.max_memory_reserved(device=None)", "intent": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 3374},
{"snippet": "Tensor.all()", "intent": "See torch.all ( )", "question_id": 3375},
{"snippet": "Tensor.all(dim=None)", "intent": "See torch.all ( ) With arguments `dim`.", "question_id": 3376},
{"snippet": "Tensor.all(keepdim=False)", "intent": "See torch.all ( ) With arguments `keepdim`.", "question_id": 3377},
{"snippet": "Tensor.all(dim=None, keepdim=False)", "intent": "See torch.all ( ) With arguments `dim`, `keepdim`.", "question_id": 3378},
{"snippet": "Tensor.gcd_(other)", "intent": "In-place version of gcd ( ) With arguments `other`.", "question_id": 3379},
{"snippet": "torch.nan_to_num(input)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3380},
{"snippet": "torch.nan_to_num(input, nan=0.0)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3381},
{"snippet": "torch.nan_to_num(input, posinf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3382},
{"snippet": "torch.nan_to_num(input, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3383},
{"snippet": "torch.nan_to_num(input, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 3384},
{"snippet": "torch.nan_to_num(input, nan=0.0, posinf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3385},
{"snippet": "torch.nan_to_num(input, nan=0.0, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3386},
{"snippet": "torch.nan_to_num(input, nan=0.0, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 3387},
{"snippet": "torch.nan_to_num(input, posinf=None, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 3388},
{"snippet": "torch.nan_to_num(input, posinf=None, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 3389},
{"snippet": "torch.histc(input)", "intent": "Computes the histogram of a tensor . With arguments `input`.", "question_id": 3390},
{"snippet": "torch.histc(input, bins=100)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3391},
{"snippet": "torch.histc(input, min=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3392},
{"snippet": "torch.histc(input, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3393},
{"snippet": "torch.histc(input, out=None)", "intent": "Computes the histogram of a tensor . With arguments `input`, `out`.", "question_id": 3394},
{"snippet": "torch.histc(input, bins=100, min=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3395},
{"snippet": "torch.histc(input, bins=100, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3396},
{"snippet": "torch.histc(input, bins=100, out=None)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`, `out`.", "question_id": 3397},
{"snippet": "torch.histc(input, min=0, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 3398},
{"snippet": "torch.histc(input, min=0, out=None)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`, `out`.", "question_id": 3399},
{"snippet": "torch.cuda.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers for the current GPU .", "question_id": 3400},
{"snippet": "Tensor.masked_scatter_(mask, source)", "intent": "Copies elements from `source` into self tensor at positions where the `mask` is True .", "question_id": 3401},
{"snippet": "torch.nn.TripletMarginLoss()", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 .", "question_id": 3402},
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 .", "question_id": 3403},
{"snippet": "torch.nn.TripletMarginLoss(p=2.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . A triplet is composed by a , `p` and n ( i.e. , anchor , positive examples and negative examples respectively ) .", "question_id": 3404},
{"snippet": "torch.nn.TripletMarginLoss(eps=1e-06)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `eps`.", "question_id": 3405},
{"snippet": "torch.nn.TripletMarginLoss(swap=False)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . The distance `swap` is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas , E. Riba et al .", "question_id": 3406},
{"snippet": "torch.nn.TripletMarginLoss(size_average=None)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `size_average`.", "question_id": 3407},
{"snippet": "torch.nn.TripletMarginLoss(reduce=None)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `reduce`.", "question_id": 3408},
{"snippet": "torch.nn.TripletMarginLoss(reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `reduction`.", "question_id": 3409},
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0, p=2.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . A triplet is composed by a , `p` and n ( i.e. , anchor , positive examples and negative examples respectively ) .", "question_id": 3410},
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0, eps=1e-06)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `eps`.", "question_id": 3411},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`.", "question_id": 3412},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `eps`.", "question_id": 3413},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, momentum=0.1)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `momentum`.", "question_id": 3414},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05, momentum=0.1)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `eps`, `momentum`.", "question_id": 3415},
{"snippet": "torch.nn.MaxUnpool1d(kernel_size)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`.", "question_id": 3416},
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `stride`.", "question_id": 3417},
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `padding`.", "question_id": 3418},
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 3419},
{"snippet": "torch.nn.ReflectionPad1d(padding)", "intent": "Pads the input tensor using the reflection of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 3420},
{"snippet": "torch.nn.functional.pixel_shuffle(input, upscale_factor)", "intent": "Rearranges elements in a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) to a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) , where r is the `upscale_factor` . With arguments `input`.", "question_id": 3421},
{"snippet": "torch.neg(input)", "intent": "Returns a new tensor with the negative of the elements of `input` .", "question_id": 3422},
{"snippet": "torch.neg(input, out=None)", "intent": "Returns a new tensor with the negative of the elements of `input` . With arguments `out`.", "question_id": 3423},
{"snippet": "torch.nn.Softsign", "intent": "Applies the element-wise function:", "question_id": 3424},
{"snippet": "Tensor.addr_(vec1, vec2)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`.", "question_id": 3425},
{"snippet": "Tensor.addr_(vec1, vec2, beta=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `beta`.", "question_id": 3426},
{"snippet": "Tensor.addr_(vec1, vec2, alpha=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `alpha`.", "question_id": 3427},
{"snippet": "Tensor.addr_(vec1, vec2, beta=1, alpha=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `beta`, `alpha`.", "question_id": 3428},
{"snippet": "torch.logsumexp(input, dim)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` .", "question_id": 3429},
{"snippet": "torch.logsumexp(input, dim, keepdim=False)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , the output tensor is of the same size as input except in the dimension ( s ) dim where it is of size 1 .", "question_id": 3430},
{"snippet": "torch.logsumexp(input, dim, out=None)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 3431},
{"snippet": "torch.logsumexp(input, dim, keepdim=False, out=None)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , the output tensor is of the same size as input except in the dimension ( s ) dim where it is of size 1 . With arguments `out`.", "question_id": 3432},
{"snippet": "Tensor.erf_()", "intent": "In-place version of erf ( )", "question_id": 3433},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`.", "question_id": 3434},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`.", "question_id": 3435},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`.", "question_id": 3436},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `last_epoch`.", "question_id": 3437},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `verbose`.", "question_id": 3438},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `eta_min`.", "question_id": 3439},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `last_epoch`.", "question_id": 3440},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `verbose`.", "question_id": 3441},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`, `last_epoch`.", "question_id": 3442},
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`, `verbose`.", "question_id": 3443},
{"snippet": "cosine_annealing_warm_restarts.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 3444},
{"snippet": "cosine_annealing_warm_restarts.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 3445},
{"snippet": "cosine_annealing_warm_restarts.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 3446},
{"snippet": "cosine_annealing_warm_restarts.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 3447},
{"snippet": "cosine_annealing_warm_restarts.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 3448},
{"snippet": "cosine_annealing_warm_restarts.step()", "intent": "Step could be called after every batch update", "question_id": 3449},
{"snippet": "cosine_annealing_warm_restarts.step(epoch=None)", "intent": "Step could be called after every batch update With arguments `epoch`.", "question_id": 3450},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 3451},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 3452},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 3453},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 3454},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 3455},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 3456},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 3457},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 3458},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 3459},
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 3460},
{"snippet": "Tensor.lerp_(end, weight)", "intent": "In-place version of lerp ( ) With arguments `end`, `weight`.", "question_id": 3461},
{"snippet": "torch.nn.functional.unfold(input, kernel_size)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`.", "question_id": 3462},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`.", "question_id": 3463},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, padding=0)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `padding`.", "question_id": 3464},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `stride`.", "question_id": 3465},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `padding`.", "question_id": 3466},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `stride`.", "question_id": 3467},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `padding`, `stride`.", "question_id": 3468},
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `padding`, `stride`.", "question_id": 3469},
{"snippet": "torch.quantization.add_quant_dequant(module)", "intent": "Wrap the leaf child `module` in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well .", "question_id": 3470},
{"snippet": "Tensor.resize_as_(tensor)", "intent": "Resizes the self `tensor` to be the same size as the specified tensor .", "question_id": 3471},
{"snippet": "Tensor.resize_as_(tensor, memory_format=torch.contiguous_format)", "intent": "Resizes the self `tensor` to be the same size as the specified tensor . With arguments `memory_format`.", "question_id": 3472},
{"snippet": "Tensor.tan_()", "intent": "In-place version of tan ( )", "question_id": 3473},
{"snippet": "torch.acosh(input)", "intent": "Returns a new tensor with the inverse hyperbolic cosine of the elements of `input` .", "question_id": 3474},
{"snippet": "torch.acosh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic cosine of the elements of `input` . With arguments `out`.", "question_id": 3475},
{"snippet": "torch.polygamma(n, input)", "intent": "Computes the nthn^ { th } nth derivative of the digamma function on `input` . With arguments `n`.", "question_id": 3476},
{"snippet": "torch.polygamma(n, input, out=None)", "intent": "Computes the nthn^ { th } nth derivative of the digamma function on `input` . With arguments `n`, `out`.", "question_id": 3477},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 3478},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 3479},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 3480},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 3481},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 3482},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 3483},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 3484},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 3485},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 3486},
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 3487},
{"snippet": "lazy_conv_transpose3d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose3d", "question_id": 3488},
{"snippet": "torch.nn.quantized.functional.hardtanh(input)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`.", "question_id": 3489},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`.", "question_id": 3490},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, max_val=1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `max_val`.", "question_id": 3491},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `inplace`.", "question_id": 3492},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, max_val=1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`.", "question_id": 3493},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `inplace`.", "question_id": 3494},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, max_val=1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `max_val`, `inplace`.", "question_id": 3495},
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, max_val=1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`, `inplace`.", "question_id": 3496},
{"snippet": "Tensor.tanh()", "intent": "See torch.tanh ( )", "question_id": 3497},
{"snippet": "Tensor.logit_()", "intent": "In-place version of logit ( )", "question_id": 3498},
{"snippet": "Tensor.matmul(tensor2)", "intent": "See torch.matmul ( ) With arguments `tensor2`.", "question_id": 3499},
{"snippet": "torch.nn.functional.max_pool1d(*args, **kwargs)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 3500},
{"snippet": "torch.nn.MaxUnpool3d(kernel_size)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`.", "question_id": 3501},
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `stride`.", "question_id": 3502},
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `padding`.", "question_id": 3503},
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 3504},
{"snippet": "torch.nn.AdaptiveMaxPool1d(output_size)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 3505},
{"snippet": "torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 3506},
{"snippet": "torch.nn.GaussianNLLLoss()", "intent": "Gaussian negative log likelihood loss .", "question_id": 3507},
{"snippet": "torch.nn.GaussianNLLLoss(full=False)", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True .", "question_id": 3508},
{"snippet": "torch.nn.GaussianNLLLoss(eps=1e-06)", "intent": "Gaussian negative log likelihood loss . where `eps` is used for stability .", "question_id": 3509},
{"snippet": "torch.nn.GaussianNLLLoss(reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `reduction`.", "question_id": 3510},
{"snippet": "torch.nn.GaussianNLLLoss(full=False, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . where `eps` is used for stability .", "question_id": 3511},
{"snippet": "torch.nn.GaussianNLLLoss(full=False, reduction='mean')", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . With arguments `reduction`.", "question_id": 3512},
{"snippet": "torch.nn.GaussianNLLLoss(eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . where `eps` is used for stability . With arguments `reduction`.", "question_id": 3513},
{"snippet": "torch.nn.GaussianNLLLoss(full=False, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . where `eps` is used for stability . With arguments `reduction`.", "question_id": 3514},
{"snippet": "torch.nn.InstanceNorm3d(num_features)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 3515},
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 3516},
{"snippet": "torch.nn.InstanceNorm3d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 3517},
{"snippet": "torch.nn.InstanceNorm3d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 3518},
{"snippet": "torch.nn.InstanceNorm3d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 3519},
{"snippet": "torch.nn.InstanceNorm3d(num_features, device=None)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 3520},
{"snippet": "torch.nn.InstanceNorm3d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 3521},
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 3522},
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 3523},
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 3524},
{"snippet": "torch.jit.fork(func, *args, **kwargs)", "intent": "Creates an asynchronous task executing `func` and a reference to the value of the result of this execution . With arguments `*args`, `**kwargs`.", "question_id": 3525},
{"snippet": "torch.jit.optimize_for_inference(mod)", "intent": "Performs a set of optimization passes to optimize a model for the purposes of inference . With arguments `mod`.", "question_id": 3526},
{"snippet": "torch.nn.modules.module.register_module_backward_hook(hook)", "intent": "Registers a backward `hook` common to all the modules .", "question_id": 3527},
{"snippet": "torch.cuda.list_gpu_processes()", "intent": "Returns a human-readable printout of the running processes and their GPU memory use for a given `device` .", "question_id": 3528},
{"snippet": "torch.cuda.list_gpu_processes(device=None)", "intent": "Returns a human-readable printout of the running processes and their GPU memory use for a given `device` .", "question_id": 3529},
{"snippet": "torch.optim.Adadelta(params)", "intent": "Implements Adadelta algorithm . With arguments `params`.", "question_id": 3530},
{"snippet": "torch.optim.Adadelta(params, lr=1.0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`.", "question_id": 3531},
{"snippet": "torch.optim.Adadelta(params, rho=0.9)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`.", "question_id": 3532},
{"snippet": "torch.optim.Adadelta(params, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `eps`.", "question_id": 3533},
{"snippet": "torch.optim.Adadelta(params, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `weight_decay`.", "question_id": 3534},
{"snippet": "torch.optim.Adadelta(params, lr=1.0, rho=0.9)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `rho`.", "question_id": 3535},
{"snippet": "torch.optim.Adadelta(params, lr=1.0, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 3536},
{"snippet": "torch.optim.Adadelta(params, lr=1.0, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 3537},
{"snippet": "torch.optim.Adadelta(params, rho=0.9, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`, `eps`.", "question_id": 3538},
{"snippet": "torch.optim.Adadelta(params, rho=0.9, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`, `weight_decay`.", "question_id": 3539},
{"snippet": "adadelta.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3540},
{"snippet": "adadelta.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3541},
{"snippet": "adadelta.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3542},
{"snippet": "adadelta.step()", "intent": "Performs a single optimization step .", "question_id": 3543},
{"snippet": "adadelta.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3544},
{"snippet": "adadelta.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3545},
{"snippet": "adadelta.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3546},
{"snippet": "torch.signbit(input)", "intent": "Tests if each element of `input` has its sign bit set ( is less than zero ) or not .", "question_id": 3547},
{"snippet": "torch.signbit(input, out=None)", "intent": "Tests if each element of `input` has its sign bit set ( is less than zero ) or not . With arguments `out`.", "question_id": 3548},
{"snippet": "torch.stft(input, n_fft)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True .", "question_id": 3549},
{"snippet": "torch.stft(input, n_fft, hop_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `hop_length`.", "question_id": 3550},
{"snippet": "torch.stft(input, n_fft, win_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `win_length`.", "question_id": 3551},
{"snippet": "torch.stft(input, n_fft, window=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True .", "question_id": 3552},
{"snippet": "torch.stft(input, n_fft, center=True)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `center`.", "question_id": 3553},
{"snippet": "torch.stft(input, n_fft, pad_mode='reflect')", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `pad_mode`.", "question_id": 3554},
{"snippet": "torch.stft(input, n_fft, normalized=False)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `normalized`.", "question_id": 3555},
{"snippet": "torch.stft(input, n_fft, onesided=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `onesided`.", "question_id": 3556},
{"snippet": "torch.stft(input, n_fft, return_complex=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . Returns either a complex tensor of size ( \u2217\u00d7N\u00d7T ) ( * \\times N \\times T ) ( \u2217\u00d7N\u00d7T ) if `return_complex` is true , or a real tensor of size ( \u2217\u00d7N\u00d7T\u00d72 ) ( * \\times N \\times T \\times 2 ) ( \u2217\u00d7N\u00d7T\u00d72 ) .", "question_id": 3557},
{"snippet": "torch.stft(input, n_fft, hop_length=None, win_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `hop_length`, `win_length`.", "question_id": 3558},
{"snippet": "torch.cuda.stream(stream)", "intent": "Wrapper around the Context-manager StreamContext that selects a given `stream` .", "question_id": 3559},
{"snippet": "torch.nn.Unfold(kernel_size)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks :", "question_id": 3560},
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3561},
{"snippet": "torch.nn.Unfold(kernel_size, padding=0)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3562},
{"snippet": "torch.nn.Unfold(kernel_size, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3563},
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, padding=0)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3564},
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3565},
{"snippet": "torch.nn.Unfold(kernel_size, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3566},
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 3567},
{"snippet": "Tensor.isneginf()", "intent": "See torch.isneginf ( )", "question_id": 3568},
{"snippet": "torch.nn.quantized.functional.adaptive_avg_pool3d(input, output_size)", "intent": "Applies a 3D adaptive average pooling over a quantized `input` signal composed of several quantized input planes . With arguments `output_size`.", "question_id": 3569},
{"snippet": "Tensor.dequantize()", "intent": "Given a quantized Tensor , dequantize it and return the dequantized float Tensor .", "question_id": 3570},
{"snippet": "torch.nn.AvgPool2d(kernel_size)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as :", "question_id": 3571},
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be :", "question_id": 3572},
{"snippet": "torch.nn.AvgPool2d(kernel_size, padding=0)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 3573},
{"snippet": "torch.nn.AvgPool2d(kernel_size, ceil_mode=False)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 3574},
{"snippet": "torch.nn.AvgPool2d(kernel_size, count_include_pad=True)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `count_include_pad`.", "question_id": 3575},
{"snippet": "torch.nn.AvgPool2d(kernel_size, divisor_override=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `divisor_override`.", "question_id": 3576},
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, padding=0)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 3577},
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `ceil_mode`.", "question_id": 3578},
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `count_include_pad`.", "question_id": 3579},
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, divisor_override=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `divisor_override`.", "question_id": 3580},
{"snippet": "torch.nn.ReLU()", "intent": "Applies the rectified linear unit function element-wise :", "question_id": 3581},
{"snippet": "torch.nn.ReLU(inplace=False)", "intent": "Applies the rectified linear unit function element-wise : With arguments `inplace`.", "question_id": 3582},
{"snippet": "torch.logit(input)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`.", "question_id": 3583},
{"snippet": "torch.logit(input, eps=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `eps`.", "question_id": 3584},
{"snippet": "torch.logit(input, out=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `out`.", "question_id": 3585},
{"snippet": "torch.logit(input, eps=None, out=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `eps`, `out`.", "question_id": 3586},
{"snippet": "torch._assert(condition, message)", "intent": "A wrapper around Python \u2019 s assert which is symbolically traceable . With arguments `condition`, `message`.", "question_id": 3587},
{"snippet": "torch.logaddexp(input, other)", "intent": "Logarithm of the sum of exponentiations of the inputs . With arguments `input`, `other`.", "question_id": 3588},
{"snippet": "torch.logaddexp(input, other, out=None)", "intent": "Logarithm of the sum of exponentiations of the inputs . With arguments `input`, `other`, `out`.", "question_id": 3589},
{"snippet": "torch.cuda.initial_seed()", "intent": "Returns the current random seed of the current GPU .", "question_id": 3590},
{"snippet": "torch.nn.functional.fractional_max_pool2d(*args, **kwargs)", "intent": "Applies 2D fractional max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 3591},
{"snippet": "torch.ones_like(input)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` .", "question_id": 3592},
{"snippet": "torch.ones_like(input, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`.", "question_id": 3593},
{"snippet": "torch.ones_like(input, layout=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `layout`.", "question_id": 3594},
{"snippet": "torch.ones_like(input, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `device`.", "question_id": 3595},
{"snippet": "torch.ones_like(input, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `requires_grad`.", "question_id": 3596},
{"snippet": "torch.ones_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `memory_format`.", "question_id": 3597},
{"snippet": "torch.ones_like(input, dtype=None, layout=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 3598},
{"snippet": "torch.ones_like(input, dtype=None, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 3599},
{"snippet": "torch.ones_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 3600},
{"snippet": "torch.ones_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 3601},
{"snippet": "Tensor.bitwise_xor()", "intent": "See torch.bitwise_xor ( )", "question_id": 3602},
{"snippet": "torch.empty_strided(size, stride)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively .", "question_id": 3603},
{"snippet": "torch.empty_strided(size, stride, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`.", "question_id": 3604},
{"snippet": "torch.empty_strided(size, stride, layout=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `layout`.", "question_id": 3605},
{"snippet": "torch.empty_strided(size, stride, device=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `device`.", "question_id": 3606},
{"snippet": "torch.empty_strided(size, stride, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `requires_grad`.", "question_id": 3607},
{"snippet": "torch.empty_strided(size, stride, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `pin_memory`.", "question_id": 3608},
{"snippet": "torch.empty_strided(size, stride, dtype=None, layout=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `layout`.", "question_id": 3609},
{"snippet": "torch.empty_strided(size, stride, dtype=None, device=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `device`.", "question_id": 3610},
{"snippet": "torch.empty_strided(size, stride, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `requires_grad`.", "question_id": 3611},
{"snippet": "torch.empty_strided(size, stride, dtype=None, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `pin_memory`.", "question_id": 3612},
{"snippet": "torch.optim.Rprop(params, 1.2), 50))", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`.", "question_id": 3613},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`.", "question_id": 3614},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), etas=(0.5)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `etas`.", "question_id": 3615},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `step_sizes`.", "question_id": 3616},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, etas=(0.5)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `etas`.", "question_id": 3617},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `step_sizes`.", "question_id": 3618},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), etas=(0.5, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `etas`, `step_sizes`.", "question_id": 3619},
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, etas=(0.5, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `etas`, `step_sizes`.", "question_id": 3620},
{"snippet": "rprop.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3621},
{"snippet": "rprop.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3622},
{"snippet": "rprop.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3623},
{"snippet": "rprop.step()", "intent": "Performs a single optimization step .", "question_id": 3624},
{"snippet": "rprop.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3625},
{"snippet": "rprop.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3626},
{"snippet": "rprop.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3627},
{"snippet": "Tensor.heaviside(values)", "intent": "See torch.heaviside ( ) With arguments `values`.", "question_id": 3628},
{"snippet": "torch.nn.utils.remove_weight_norm(module)", "intent": "Removes the weight normalization reparameterization from a `module` .", "question_id": 3629},
{"snippet": "torch.nn.utils.remove_weight_norm(module, name='weight')", "intent": "Removes the weight normalization reparameterization from a `module` . With arguments `name`.", "question_id": 3630},
{"snippet": "Tensor.multiply(value)", "intent": "See torch.multiply ( ) . With arguments `value`.", "question_id": 3631},
{"snippet": "Tensor.is_shared()", "intent": "Checks if tensor is in shared memory .", "question_id": 3632},
{"snippet": "torch.gcd(input, other)", "intent": "Computes the element-wise greatest common divisor ( GCD ) of `input` and `other` .", "question_id": 3633},
{"snippet": "torch.gcd(input, other, out=None)", "intent": "Computes the element-wise greatest common divisor ( GCD ) of `input` and `other` . With arguments `out`.", "question_id": 3634},
{"snippet": "Tensor.frac_()", "intent": "In-place version of frac ( )", "question_id": 3635},
{"snippet": "torch.compiled_with_cxx11_abi()", "intent": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1", "question_id": 3636},
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`.", "question_id": 3637},
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `last_epoch`.", "question_id": 3638},
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `verbose`.", "question_id": 3639},
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 3640},
{"snippet": "exponential_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 3641},
{"snippet": "exponential_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 3642},
{"snippet": "exponential_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 3643},
{"snippet": "exponential_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 3644},
{"snippet": "exponential_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 3645},
{"snippet": "torch.nn.functional.embedding_bag(input, weight)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`.", "question_id": 3646},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, offsets=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `offsets`.", "question_id": 3647},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, max_norm=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `max_norm`.", "question_id": 3648},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, norm_type=2)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `norm_type`.", "question_id": 3649},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, scale_grad_by_freq=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `scale_grad_by_freq`.", "question_id": 3650},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, mode='mean')", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `mode`.", "question_id": 3651},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, sparse=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `sparse`.", "question_id": 3652},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, per_sample_weights=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `per_sample_weights`.", "question_id": 3653},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, include_last_offset=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `include_last_offset`.", "question_id": 3654},
{"snippet": "torch.nn.functional.embedding_bag(input, weight, padding_idx=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `padding_idx`.", "question_id": 3655},
{"snippet": "torch.quantization.convert(module)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class .", "question_id": 3656},
{"snippet": "torch.quantization.convert(module, mapping=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class .", "question_id": 3657},
{"snippet": "torch.quantization.convert(module, inplace=False)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`.", "question_id": 3658},
{"snippet": "torch.quantization.convert(module, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True .", "question_id": 3659},
{"snippet": "torch.quantization.convert(module, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `convert_custom_config_dict`.", "question_id": 3660},
{"snippet": "torch.quantization.convert(module, mapping=None, inplace=False)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`.", "question_id": 3661},
{"snippet": "torch.quantization.convert(module, mapping=None, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True .", "question_id": 3662},
{"snippet": "torch.quantization.convert(module, mapping=None, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `convert_custom_config_dict`.", "question_id": 3663},
{"snippet": "torch.quantization.convert(module, inplace=False, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True . With arguments `inplace`.", "question_id": 3664},
{"snippet": "torch.quantization.convert(module, inplace=False, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`, `convert_custom_config_dict`.", "question_id": 3665},
{"snippet": "torch.optim.Adagrad(params)", "intent": "Implements Adagrad algorithm . With arguments `params`.", "question_id": 3666},
{"snippet": "torch.optim.Adagrad(params, lr=0.01)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`.", "question_id": 3667},
{"snippet": "torch.optim.Adagrad(params, lr_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr_decay`.", "question_id": 3668},
{"snippet": "torch.optim.Adagrad(params, weight_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `weight_decay`.", "question_id": 3669},
{"snippet": "torch.optim.Adagrad(params, initial_accumulator_value=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `initial_accumulator_value`.", "question_id": 3670},
{"snippet": "torch.optim.Adagrad(params, eps=1e-10)", "intent": "Implements Adagrad algorithm . With arguments `params`, `eps`.", "question_id": 3671},
{"snippet": "torch.optim.Adagrad(params, lr=0.01, lr_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `lr_decay`.", "question_id": 3672},
{"snippet": "torch.optim.Adagrad(params, lr=0.01, weight_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 3673},
{"snippet": "torch.optim.Adagrad(params, lr=0.01, initial_accumulator_value=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `initial_accumulator_value`.", "question_id": 3674},
{"snippet": "torch.optim.Adagrad(params, lr=0.01, eps=1e-10)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 3675},
{"snippet": "adagrad.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3676},
{"snippet": "adagrad.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3677},
{"snippet": "adagrad.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3678},
{"snippet": "adagrad.step()", "intent": "Performs a single optimization step .", "question_id": 3679},
{"snippet": "adagrad.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3680},
{"snippet": "adagrad.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3681},
{"snippet": "adagrad.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3682},
{"snippet": "torch.set_rng_state(new_state)", "intent": "Sets the random number generator state . With arguments `new_state`.", "question_id": 3683},
{"snippet": "Tensor.argsort()", "intent": "See torch.argsort ( )", "question_id": 3684},
{"snippet": "Tensor.argsort(dim=- 1)", "intent": "See torch.argsort ( ) With arguments `dim`.", "question_id": 3685},
{"snippet": "Tensor.argsort(descending=False)", "intent": "See torch.argsort ( ) With arguments `descending`.", "question_id": 3686},
{"snippet": "Tensor.argsort(dim=- 1, descending=False)", "intent": "See torch.argsort ( ) With arguments `dim`, `descending`.", "question_id": 3687},
{"snippet": "Tensor.indices()", "intent": "Return the indices tensor of a sparse COO tensor .", "question_id": 3688},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`.", "question_id": 3689},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`.", "question_id": 3690},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, stride=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `stride`.", "question_id": 3691},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `padding`.", "question_id": 3692},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `output_padding`.", "question_id": 3693},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, groups=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `groups`.", "question_id": 3694},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `dilation`.", "question_id": 3695},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `stride`.", "question_id": 3696},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `padding`.", "question_id": 3697},
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `output_padding`.", "question_id": 3698},
{"snippet": "torch.nn.functional.instance_norm(input)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`.", "question_id": 3699},
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`.", "question_id": 3700},
{"snippet": "torch.nn.functional.instance_norm(input, running_var=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_var`.", "question_id": 3701},
{"snippet": "torch.nn.functional.instance_norm(input, weight=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `weight`.", "question_id": 3702},
{"snippet": "torch.nn.functional.instance_norm(input, bias=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `bias`.", "question_id": 3703},
{"snippet": "torch.nn.functional.instance_norm(input, use_input_stats=True)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `use_input_stats`.", "question_id": 3704},
{"snippet": "torch.nn.functional.instance_norm(input, momentum=0.1)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `momentum`.", "question_id": 3705},
{"snippet": "torch.nn.functional.instance_norm(input, eps=1e-05)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `eps`.", "question_id": 3706},
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None, running_var=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`, `running_var`.", "question_id": 3707},
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None, weight=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`, `weight`.", "question_id": 3708},
{"snippet": "torch.optim.RMSprop(params)", "intent": "Implements RMSprop algorithm . With arguments `params`.", "question_id": 3709},
{"snippet": "torch.optim.RMSprop(params, lr=0.01)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`.", "question_id": 3710},
{"snippet": "torch.optim.RMSprop(params, alpha=0.99)", "intent": "Implements RMSprop algorithm . With arguments `params`, `alpha`.", "question_id": 3711},
{"snippet": "torch.optim.RMSprop(params, eps=1e-08)", "intent": "Implements RMSprop algorithm . With arguments `params`, `eps`.", "question_id": 3712},
{"snippet": "torch.optim.RMSprop(params, weight_decay=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `weight_decay`.", "question_id": 3713},
{"snippet": "torch.optim.RMSprop(params, momentum=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `momentum`.", "question_id": 3714},
{"snippet": "torch.optim.RMSprop(params, centered=False)", "intent": "Implements RMSprop algorithm . The `centered` version first appears in Generating Sequences With Recurrent Neural Networks . With arguments `params`.", "question_id": 3715},
{"snippet": "torch.optim.RMSprop(params, lr=0.01, alpha=0.99)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `alpha`.", "question_id": 3716},
{"snippet": "torch.optim.RMSprop(params, lr=0.01, eps=1e-08)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 3717},
{"snippet": "torch.optim.RMSprop(params, lr=0.01, weight_decay=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 3718},
{"snippet": "rm_sprop.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3719},
{"snippet": "rm_sprop.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3720},
{"snippet": "rm_sprop.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3721},
{"snippet": "rm_sprop.step()", "intent": "Performs a single optimization step .", "question_id": 3722},
{"snippet": "rm_sprop.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3723},
{"snippet": "rm_sprop.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3724},
{"snippet": "rm_sprop.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3725},
{"snippet": "torch.nn.AlphaDropout()", "intent": "Applies Alpha Dropout over the input .", "question_id": 3726},
{"snippet": "torch.nn.AlphaDropout(p=0.5)", "intent": "Applies Alpha Dropout over the input . During training , it randomly masks some of the elements of the input tensor with probability `p` using samples from a bernoulli distribution .", "question_id": 3727},
{"snippet": "torch.nn.AlphaDropout(inplace=False)", "intent": "Applies Alpha Dropout over the input . With arguments `inplace`.", "question_id": 3728},
{"snippet": "torch.nn.AlphaDropout(p=0.5, inplace=False)", "intent": "Applies Alpha Dropout over the input . During training , it randomly masks some of the elements of the input tensor with probability `p` using samples from a bernoulli distribution . With arguments `inplace`.", "question_id": 3729},
{"snippet": "torch.are_deterministic_algorithms_enabled()", "intent": "Returns True if the global deterministic flag is turned on .", "question_id": 3730},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 3731},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 3732},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, training=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 3733},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 3734},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 3735},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 3736},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, training=False, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 3737},
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 3738},
{"snippet": "torch.matrix_rank(input)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues .", "question_id": 3739},
{"snippet": "torch.matrix_rank(input, tol=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 .", "question_id": 3740},
{"snippet": "torch.matrix_rank(input, symmetric=False)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues .", "question_id": 3741},
{"snippet": "torch.matrix_rank(input, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . With arguments `out`.", "question_id": 3742},
{"snippet": "torch.matrix_rank(input, tol=None, symmetric=False)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 .", "question_id": 3743},
{"snippet": "torch.matrix_rank(input, tol=None, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 . With arguments `out`.", "question_id": 3744},
{"snippet": "torch.matrix_rank(input, symmetric=False, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . With arguments `out`.", "question_id": 3745},
{"snippet": "torch.matrix_rank(input, tol=None, symmetric=False, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 . With arguments `out`.", "question_id": 3746},
{"snippet": "Tensor.to_mkldnn()", "intent": "Returns a copy of the tensor in torch.mkldnn layout .", "question_id": 3747},
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input)", "intent": "Upsamples the `input` , using bilinear upsampling .", "question_id": 3748},
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, size=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`.", "question_id": 3749},
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `scale_factor`.", "question_id": 3750},
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`, `scale_factor`.", "question_id": 3751},
{"snippet": "Tensor.clamp()", "intent": "See torch.clamp ( )", "question_id": 3752},
{"snippet": "Tensor.clamp(min=None)", "intent": "See torch.clamp ( ) With arguments `min`.", "question_id": 3753},
{"snippet": "Tensor.clamp(max=None)", "intent": "See torch.clamp ( ) With arguments `max`.", "question_id": 3754},
{"snippet": "Tensor.clamp(min=None, max=None)", "intent": "See torch.clamp ( ) With arguments `min`, `max`.", "question_id": 3755},
{"snippet": "Tensor.eq(other)", "intent": "See torch.eq ( ) With arguments `other`.", "question_id": 3756},
{"snippet": "Tensor.storage_type()", "intent": "Returns the type of the underlying storage .", "question_id": 3757},
{"snippet": "Tensor.unfold(dimension, size, step)", "intent": "Returns a view of the original tensor which contains all slices of `size` size from self tensor in the `dimension` dimension . Step between two slices is given by `step` .", "question_id": 3758},
{"snippet": "torch.nn.PairwiseDistance()", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm :", "question_id": 3759},
{"snippet": "torch.nn.PairwiseDistance(p=2.0)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`.", "question_id": 3760},
{"snippet": "torch.nn.PairwiseDistance(eps=1e-06)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `eps`.", "question_id": 3761},
{"snippet": "torch.nn.PairwiseDistance(keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `keepdim`.", "question_id": 3762},
{"snippet": "torch.nn.PairwiseDistance(p=2.0, eps=1e-06)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `eps`.", "question_id": 3763},
{"snippet": "torch.nn.PairwiseDistance(p=2.0, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `keepdim`.", "question_id": 3764},
{"snippet": "torch.nn.PairwiseDistance(eps=1e-06, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `eps`, `keepdim`.", "question_id": 3765},
{"snippet": "torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `eps`, `keepdim`.", "question_id": 3766},
{"snippet": "torch.arange(end)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 3767},
{"snippet": "torch.arange(end, start=0)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 3768},
{"snippet": "torch.arange(end, step=1)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 3769},
{"snippet": "torch.arange(end, out=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `out`.", "question_id": 3770},
{"snippet": "torch.arange(end, dtype=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `dtype`.", "question_id": 3771},
{"snippet": "torch.arange(end, layout=torch.strided)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `layout`.", "question_id": 3772},
{"snippet": "torch.arange(end, device=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `device`.", "question_id": 3773},
{"snippet": "torch.arange(end, requires_grad=False)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `requires_grad`.", "question_id": 3774},
{"snippet": "torch.arange(end, start=0, step=1)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 3775},
{"snippet": "torch.arange(end, start=0, out=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `out`.", "question_id": 3776},
{"snippet": "torch.poisson(input)", "intent": "Returns a tensor of the same size as `input` with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e. ,", "question_id": 3777},
{"snippet": "torch.poisson(input, generator=None)", "intent": "Returns a tensor of the same size as `input` with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e. , With arguments `generator`.", "question_id": 3778},
{"snippet": "torch.optim.SGD(params)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`.", "question_id": 3779},
{"snippet": "torch.optim.SGD(params, lr=<required parameter>)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`.", "question_id": 3780},
{"snippet": "torch.optim.SGD(params, momentum=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`.", "question_id": 3781},
{"snippet": "torch.optim.SGD(params, dampening=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `dampening`.", "question_id": 3782},
{"snippet": "torch.optim.SGD(params, weight_decay=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `weight_decay`.", "question_id": 3783},
{"snippet": "torch.optim.SGD(params, nesterov=False)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `nesterov`.", "question_id": 3784},
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, momentum=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`.", "question_id": 3785},
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, dampening=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `dampening`.", "question_id": 3786},
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, weight_decay=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `weight_decay`.", "question_id": 3787},
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, nesterov=False)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `nesterov`.", "question_id": 3788},
{"snippet": "sgd.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 3789},
{"snippet": "sgd.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 3790},
{"snippet": "sgd.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 3791},
{"snippet": "sgd.step()", "intent": "Performs a single optimization step .", "question_id": 3792},
{"snippet": "sgd.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 3793},
{"snippet": "sgd.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 3794},
{"snippet": "sgd.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 3795},
{"snippet": "torch.nn.utils.prune.identity(module, name)", "intent": "Applies pruning reparametrization to the tensor corresponding to the parameter called `name` in `module` without actually pruning any units .", "question_id": 3796},
{"snippet": "torch.dequantize(tensor)", "intent": "Returns an fp32 Tensor by dequantizing a quantized Tensor With arguments `tensor`.", "question_id": 3797},
{"snippet": "Tensor.true_divide(value)", "intent": "See torch.true_divide ( ) With arguments `value`.", "question_id": 3798},
{"snippet": "torch.digamma(input)", "intent": "Computes the logarithmic derivative of the gamma function on `input` .", "question_id": 3799},
{"snippet": "torch.digamma(input, out=None)", "intent": "Computes the logarithmic derivative of the gamma function on `input` . With arguments `out`.", "question_id": 3800},
{"snippet": "torch.linalg.lstsq(A, B)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as", "question_id": 3801},
{"snippet": "torch.linalg.lstsq(A, B, rcond=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as With arguments `rcond`.", "question_id": 3802},
{"snippet": "torch.linalg.lstsq(A, B, driver=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as `driver` chooses the LAPACK/MAGMA function that will be used .", "question_id": 3803},
{"snippet": "torch.linalg.lstsq(A, B, rcond=None, driver=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as `driver` chooses the LAPACK/MAGMA function that will be used . With arguments `rcond`.", "question_id": 3804},
{"snippet": "Tensor.equal(other)", "intent": "See torch.equal ( ) With arguments `other`.", "question_id": 3805},
{"snippet": "Tensor.storage_offset()", "intent": "Returns self tensor \u2019 s offset in the underlying storage in terms of number of storage elements ( not bytes ) .", "question_id": 3806},
{"snippet": "torch.logical_xor(input, other)", "intent": "Computes the element-wise logical XOR of the given `input` tensors . With arguments `other`.", "question_id": 3807},
{"snippet": "torch.logical_xor(input, other, out=None)", "intent": "Computes the element-wise logical XOR of the given `input` tensors . With arguments `other`, `out`.", "question_id": 3808},
{"snippet": "torch.mean(input)", "intent": "Returns the mean value of all elements in the `input` tensor .", "question_id": 3809},
{"snippet": "Tensor.storage()", "intent": "Returns the underlying storage .", "question_id": 3810},
{"snippet": "Tensor.var(dim)", "intent": "See torch.var ( ) With arguments `dim`.", "question_id": 3811},
{"snippet": "Tensor.var(dim, unbiased=True)", "intent": "See torch.var ( ) With arguments `dim`, `unbiased`.", "question_id": 3812},
{"snippet": "Tensor.var(dim, keepdim=False)", "intent": "See torch.var ( ) With arguments `dim`, `keepdim`.", "question_id": 3813},
{"snippet": "Tensor.var(dim, unbiased=True, keepdim=False)", "intent": "See torch.var ( ) With arguments `dim`, `unbiased`, `keepdim`.", "question_id": 3814},
{"snippet": "torch.greater(input, other)", "intent": "Alias for torch.gt ( ) . With arguments `input`, `other`.", "question_id": 3815},
{"snippet": "torch.greater(input, other, out=None)", "intent": "Alias for torch.gt ( ) . With arguments `input`, `other`, `out`.", "question_id": 3816},
{"snippet": "Tensor.rsqrt_()", "intent": "In-place version of rsqrt ( )", "question_id": 3817},
{"snippet": "torch.quantization.fake_quantize.FakeQuantizeBase", "intent": "Base fake quantize module Any fake quantize implementation should derive from this class.", "question_id": 3818},
{"snippet": "fake_quantize_base.with_args(**kwargs)", "intent": "Wrapper that allows creation of class factories . With arguments `**kwargs`.", "question_id": 3819},
{"snippet": "Tensor.multinomial(num_samples)", "intent": "See torch.multinomial ( ) With arguments `num_samples`.", "question_id": 3820},
{"snippet": "Tensor.multinomial(num_samples, replacement=False)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `replacement`.", "question_id": 3821},
{"snippet": "Tensor.multinomial(num_samples, generator=None)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `generator`.", "question_id": 3822},
{"snippet": "Tensor.multinomial(num_samples, replacement=False, generator=None)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `replacement`, `generator`.", "question_id": 3823},
{"snippet": "torch.nn.ConstantPad1d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 3824},
{"snippet": "torch.set_num_interop_threads(int)", "intent": "Sets the number of threads used for interop parallelism ( e.g . With arguments `int`.", "question_id": 3825},
{"snippet": "Tensor.diagonal()", "intent": "See torch.diagonal ( )", "question_id": 3826},
{"snippet": "Tensor.diagonal(offset=0)", "intent": "See torch.diagonal ( ) With arguments `offset`.", "question_id": 3827},
{"snippet": "Tensor.diagonal(dim1=0)", "intent": "See torch.diagonal ( ) With arguments `dim1`.", "question_id": 3828},
{"snippet": "Tensor.diagonal(dim2=1)", "intent": "See torch.diagonal ( ) With arguments `dim2`.", "question_id": 3829},
{"snippet": "Tensor.diagonal(offset=0, dim1=0)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim1`.", "question_id": 3830},
{"snippet": "Tensor.diagonal(offset=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim2`.", "question_id": 3831},
{"snippet": "Tensor.diagonal(dim1=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `dim1`, `dim2`.", "question_id": 3832},
{"snippet": "Tensor.diagonal(offset=0, dim1=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim1`, `dim2`.", "question_id": 3833},
{"snippet": "Tensor.is_contiguous()", "intent": "Returns True if self tensor is contiguous in memory in the order specified by memory format .", "question_id": 3834},
{"snippet": "Tensor.is_contiguous(memory_format=torch.contiguous_format)", "intent": "Returns True if self tensor is contiguous in memory in the order specified by memory format . With arguments `memory_format`.", "question_id": 3835},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`.", "question_id": 3836},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 3837},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 3838},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 3839},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, pct_start=0.3)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `pct_start`.", "question_id": 3840},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, anneal_strategy='cos')", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `anneal_strategy`.", "question_id": 3841},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, cycle_momentum=True)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `cycle_momentum`.", "question_id": 3842},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, base_momentum=0.85)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `base_momentum`.", "question_id": 3843},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, max_momentum=0.95)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `max_momentum`.", "question_id": 3844},
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, div_factor=25.0)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `div_factor`.", "question_id": 3845},
{"snippet": "one_cycle_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 3846},
{"snippet": "one_cycle_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 3847},
{"snippet": "one_cycle_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 3848},
{"snippet": "one_cycle_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 3849},
{"snippet": "one_cycle_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 3850},
{"snippet": "Tensor.dot(other)", "intent": "See torch.dot ( ) With arguments `other`.", "question_id": 3851},
{"snippet": "torch.nn.LazyBatchNorm1d()", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) .", "question_id": 3852},
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 3853},
{"snippet": "torch.nn.LazyBatchNorm1d(momentum=0.1)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 3854},
{"snippet": "torch.nn.LazyBatchNorm1d(affine=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 3855},
{"snippet": "torch.nn.LazyBatchNorm1d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 3856},
{"snippet": "torch.nn.LazyBatchNorm1d(device=None)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 3857},
{"snippet": "torch.nn.LazyBatchNorm1d(dtype=None)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 3858},
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 3859},
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 3860},
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 3861},
{"snippet": "lazy_batch_norm1d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm1d", "question_id": 3862},
{"snippet": "torch.nn.functional.softplus(input)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . With arguments `input`.", "question_id": 3863},
{"snippet": "torch.nn.functional.softplus(input, beta=1)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . With arguments `input`, `beta`.", "question_id": 3864},
{"snippet": "torch.nn.functional.softplus(input, threshold=20)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `input`.", "question_id": 3865},
{"snippet": "torch.nn.functional.softplus(input, beta=1, threshold=20)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `input`, `beta`.", "question_id": 3866},
{"snippet": "Tensor.ge_(other)", "intent": "In-place version of ge ( ) . With arguments `other`.", "question_id": 3867},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`.", "question_id": 3868},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 3869},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`.", "question_id": 3870},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 3871},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 3872},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 3873},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 3874},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 3875},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh', device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 3876},
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh', dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 3877},
{"snippet": "torch.quantization.propagate_qconfig_(module)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module", "question_id": 3878},
{"snippet": "torch.quantization.propagate_qconfig_(module, qconfig_dict=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `qconfig_dict`.", "question_id": 3879},
{"snippet": "torch.quantization.propagate_qconfig_(module, allow_list=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `allow_list`.", "question_id": 3880},
{"snippet": "torch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `qconfig_dict`, `allow_list`.", "question_id": 3881},
{"snippet": "torch.nn.functional.silu(input)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `input`.", "question_id": 3882},
{"snippet": "torch.nn.functional.silu(input, inplace=False)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `input`, `inplace`.", "question_id": 3883},
{"snippet": "torch.cuda.max_memory_allocated()", "intent": "Returns the maximum GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 3884},
{"snippet": "torch.cuda.max_memory_allocated(device=None)", "intent": "Returns the maximum GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 3885},
{"snippet": "torch.nn.functional.pad(input, pad)", "intent": "Pads tensor . With arguments `input`, `pad`.", "question_id": 3886},
{"snippet": "torch.nn.functional.pad(input, pad, mode='constant')", "intent": "Pads tensor . With arguments `input`, `pad`, `mode`.", "question_id": 3887},
{"snippet": "torch.nn.functional.pad(input, pad, value=0)", "intent": "Pads tensor . With arguments `input`, `pad`, `value`.", "question_id": 3888},
{"snippet": "torch.nn.functional.pad(input, pad, mode='constant', value=0)", "intent": "Pads tensor . With arguments `input`, `pad`, `mode`, `value`.", "question_id": 3889},
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) .", "question_id": 3890},
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . If `batch_first` is True , B x T x * input is expected .", "question_id": 3891},
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, enforce_sorted=True)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 3892},
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . If `batch_first` is True , B x T x * input is expected . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 3893},
{"snippet": "torch.lt(input, other)", "intent": "Computes `input` < other\\text { input } < \\text { `other` } input < other element-wise .", "question_id": 3894},
{"snippet": "torch.lt(input, other, out=None)", "intent": "Computes `input` < other\\text { input } < \\text { `other` } input < other element-wise . With arguments `out`.", "question_id": 3895},
{"snippet": "torch.cuda.memory_reserved()", "intent": "Returns the current GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 3896},
{"snippet": "torch.cuda.memory_reserved(device=None)", "intent": "Returns the current GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 3897},
{"snippet": "Tensor.fmod_(divisor)", "intent": "In-place version of fmod ( ) With arguments `divisor`.", "question_id": 3898},
{"snippet": "torch.frac(input)", "intent": "Computes the fractional portion of each element in `input` .", "question_id": 3899},
{"snippet": "torch.frac(input, out=None)", "intent": "Computes the fractional portion of each element in `input` . With arguments `out`.", "question_id": 3900},
{"snippet": "torch.mm(input, mat2)", "intent": "Performs a matrix multiplication of the matrices `input` and `mat2` .", "question_id": 3901},
{"snippet": "torch.mm(input, mat2, out=None)", "intent": "Performs a matrix multiplication of the matrices `input` and `mat2` . If input is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 3902},
{"snippet": "Tensor.erf()", "intent": "See torch.erf ( )", "question_id": 3903},
{"snippet": "Tensor.sin_()", "intent": "In-place version of sin ( )", "question_id": 3904},
{"snippet": "torch.nn.Unflatten(dim, unflattened_size)", "intent": "Unflattens a tensor `dim` expanding it to a desired shape . With arguments `unflattened_size`.", "question_id": 3905},
{"snippet": "torch.nn.qat.Linear(in_features, out_features)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`.", "question_id": 3906},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`.", "question_id": 3907},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`.", "question_id": 3908},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `device`.", "question_id": 3909},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 3910},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `qconfig`.", "question_id": 3911},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `device`.", "question_id": 3912},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 3913},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`, `device`.", "question_id": 3914},
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`, `dtype`.", "question_id": 3915},
{"snippet": "linear.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 3916},
{"snippet": "torch.jit.ScriptFunction", "intent": "Functionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters.", "question_id": 3917},
{"snippet": "torch.lu(*args, **kwargs)", "intent": "Computes the LU factorization of a matrix or batches of matrices A . With arguments `*args`, `**kwargs`.", "question_id": 3918},
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`.", "question_id": 3919},
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `bias`.", "question_id": 3920},
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, qconfig=None)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `qconfig`.", "question_id": 3921},
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `bias`, `qconfig`.", "question_id": 3922},
{"snippet": "torch.trapz(y, x)", "intent": "Estimate \u222by dx\\int y\\ , dx\u222bydx along `dim` , using the trapezoid rule . With arguments `y`, `x`.", "question_id": 3923},
{"snippet": "torch.trapz(y, x, dim=- 1)", "intent": "Estimate \u222by dx\\int y\\ , dx\u222bydx along `dim` , using the trapezoid rule . With arguments `y`, `x`.", "question_id": 3924},
{"snippet": "torch.nn.intrinsic.ConvBnReLU1d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 1d , Batch Norm 1d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 3925},
{"snippet": "torch.fft.fftn(input)", "intent": "Computes the N dimensional discrete Fourier transform of `input` .", "question_id": 3926},
{"snippet": "torch.fft.fftn(input, s=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`.", "question_id": 3927},
{"snippet": "torch.fft.fftn(input, dim=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 3928},
{"snippet": "torch.fft.fftn(input, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 3929},
{"snippet": "torch.fft.fftn(input, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `out`.", "question_id": 3930},
{"snippet": "torch.fft.fftn(input, s=None, dim=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `dim`.", "question_id": 3931},
{"snippet": "torch.fft.fftn(input, s=None, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `norm`.", "question_id": 3932},
{"snippet": "torch.fft.fftn(input, s=None, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `out`.", "question_id": 3933},
{"snippet": "torch.fft.fftn(input, dim=None, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 3934},
{"snippet": "torch.fft.fftn(input, dim=None, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 3935},
{"snippet": "torch.is_complex(input)", "intent": "Returns True if the data type of `input` is a complex data type i.e. , one of torch.complex64 , and torch.complex128 .", "question_id": 3936},
{"snippet": "torch.blackman_window(window_length)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 3937},
{"snippet": "torch.blackman_window(window_length, periodic=True)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 3938},
{"snippet": "torch.blackman_window(window_length, dtype=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 3939},
{"snippet": "torch.blackman_window(window_length, layout=torch.strided)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 3940},
{"snippet": "torch.blackman_window(window_length, device=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 3941},
{"snippet": "torch.blackman_window(window_length, requires_grad=False)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 3942},
{"snippet": "torch.blackman_window(window_length, periodic=True, dtype=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 3943},
{"snippet": "torch.blackman_window(window_length, periodic=True, layout=torch.strided)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 3944},
{"snippet": "torch.blackman_window(window_length, periodic=True, device=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 3945},
{"snippet": "torch.blackman_window(window_length, periodic=True, requires_grad=False)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 3946},
{"snippet": "Tensor.data_ptr()", "intent": "Returns the address of the first element of self tensor .", "question_id": 3947},
{"snippet": "Tensor.isclose(other)", "intent": "See torch.isclose ( ) With arguments `other`.", "question_id": 3948},
{"snippet": "Tensor.isclose(other, rtol=1e-05)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`.", "question_id": 3949},
{"snippet": "Tensor.isclose(other, atol=1e-08)", "intent": "See torch.isclose ( ) With arguments `other`, `atol`.", "question_id": 3950},
{"snippet": "Tensor.isclose(other, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `equal_nan`.", "question_id": 3951},
{"snippet": "Tensor.isclose(other, rtol=1e-05, atol=1e-08)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `atol`.", "question_id": 3952},
{"snippet": "Tensor.isclose(other, rtol=1e-05, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `equal_nan`.", "question_id": 3953},
{"snippet": "Tensor.isclose(other, atol=1e-08, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `atol`, `equal_nan`.", "question_id": 3954},
{"snippet": "Tensor.isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `atol`, `equal_nan`.", "question_id": 3955},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`.", "question_id": 3956},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`.", "question_id": 3957},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, margin=1.0)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `margin`.", "question_id": 3958},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, weight=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `weight`.", "question_id": 3959},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, size_average=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 3960},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, reduce=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 3961},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, reduction='mean')", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 3962},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `margin`.", "question_id": 3963},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, weight=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `weight`.", "question_id": 3964},
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, size_average=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `size_average`.", "question_id": 3965},
{"snippet": "Tensor.gt(other)", "intent": "See torch.gt ( ) . With arguments `other`.", "question_id": 3966},
{"snippet": "torch.randperm(n)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 .", "question_id": 3967},
{"snippet": "torch.randperm(n, generator=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`.", "question_id": 3968},
{"snippet": "torch.randperm(n, out=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `out`.", "question_id": 3969},
{"snippet": "torch.randperm(n, dtype=torch.int64)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `dtype`.", "question_id": 3970},
{"snippet": "torch.randperm(n, layout=torch.strided)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `layout`.", "question_id": 3971},
{"snippet": "torch.randperm(n, device=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `device`.", "question_id": 3972},
{"snippet": "torch.randperm(n, requires_grad=False)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `requires_grad`.", "question_id": 3973},
{"snippet": "torch.randperm(n, pin_memory=False)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `pin_memory`.", "question_id": 3974},
{"snippet": "torch.randperm(n, generator=None, out=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`, `out`.", "question_id": 3975},
{"snippet": "torch.randperm(n, generator=None, dtype=torch.int64)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`, `dtype`.", "question_id": 3976},
{"snippet": "torch.amax(input, dim)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` .", "question_id": 3977},
{"snippet": "torch.amax(input, dim, keepdim=False)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is `` True ` , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 .", "question_id": 3978},
{"snippet": "torch.amax(input, dim, out=None)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . With arguments `out`.", "question_id": 3979},
{"snippet": "torch.amax(input, dim, keepdim=False, out=None)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is `` True ` , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 . With arguments `out`.", "question_id": 3980},
{"snippet": "torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `*args`, `**kwargs`.", "question_id": 3981},
{"snippet": "torch.moveaxis(input, source, destination)", "intent": "Alias for torch.movedim ( ) . With arguments `input`, `source`, `destination`.", "question_id": 3982},
{"snippet": "torch.conj(input)", "intent": "Computes the element-wise conjugate of the given `input` tensor .", "question_id": 3983},
{"snippet": "torch.conj(input, out=None)", "intent": "Computes the element-wise conjugate of the given `input` tensor . With arguments `out`.", "question_id": 3984},
{"snippet": "torch.quantization.prepare(model)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training .", "question_id": 3985},
{"snippet": "torch.quantization.prepare(model, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`.", "question_id": 3986},
{"snippet": "torch.quantization.prepare(model, allow_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`.", "question_id": 3987},
{"snippet": "torch.quantization.prepare(model, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `observer_non_leaf_module_list`.", "question_id": 3988},
{"snippet": "torch.quantization.prepare(model, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `prepare_custom_config_dict`.", "question_id": 3989},
{"snippet": "torch.quantization.prepare(model, inplace=False, allow_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `allow_list`.", "question_id": 3990},
{"snippet": "torch.quantization.prepare(model, inplace=False, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `observer_non_leaf_module_list`.", "question_id": 3991},
{"snippet": "torch.quantization.prepare(model, inplace=False, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `prepare_custom_config_dict`.", "question_id": 3992},
{"snippet": "torch.quantization.prepare(model, allow_list=None, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`, `observer_non_leaf_module_list`.", "question_id": 3993},
{"snippet": "torch.quantization.prepare(model, allow_list=None, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`, `prepare_custom_config_dict`.", "question_id": 3994},
{"snippet": "torch.cuda.current_device()", "intent": "Returns the index of a currently selected device .", "question_id": 3995},
{"snippet": "Tensor.tril_()", "intent": "In-place version of tril ( )", "question_id": 3996},
{"snippet": "Tensor.tril_(k=0)", "intent": "In-place version of tril ( ) With arguments `k`.", "question_id": 3997},
{"snippet": "torch.no_grad", "intent": "Context-manager that disabled gradient calculation.", "question_id": 3998},
{"snippet": "Tensor.logical_not_()", "intent": "In-place version of logical_not ( )", "question_id": 3999},
{"snippet": "torch.arcsinh(input)", "intent": "Alias for torch.asinh ( ) . With arguments `input`.", "question_id": 4000},
{"snippet": "torch.arcsinh(input, out=None)", "intent": "Alias for torch.asinh ( ) . With arguments `input`, `out`.", "question_id": 4001},
{"snippet": "torch.atleast_3d(*tensors)", "intent": "Returns a 3-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 4002},
{"snippet": "profile.key_averages()", "intent": "Averages all function events over their keys .", "question_id": 4003},
{"snippet": "profile.key_averages(group_by_input_shape=False)", "intent": "Averages all function events over their keys . With arguments `group_by_input_shape`.", "question_id": 4004},
{"snippet": "profile.key_averages(group_by_stack_n=0)", "intent": "Averages all function events over their keys . With arguments `group_by_stack_n`.", "question_id": 4005},
{"snippet": "profile.key_averages(group_by_input_shape=False, group_by_stack_n=0)", "intent": "Averages all function events over their keys . With arguments `group_by_input_shape`, `group_by_stack_n`.", "question_id": 4006},
{"snippet": "torch.nn.quantized.FloatFunctional", "intent": "State collector class for float operations.", "question_id": 4007},
{"snippet": "torch.linalg.cond(A)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm .", "question_id": 4008},
{"snippet": "torch.linalg.cond(A, p=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . `p` defines the matrix norm that is computed .", "question_id": 4009},
{"snippet": "torch.linalg.cond(A, out=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . With arguments `out`.", "question_id": 4010},
{"snippet": "torch.linalg.cond(A, p=None, out=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . `p` defines the matrix norm that is computed . With arguments `out`.", "question_id": 4011},
{"snippet": "torch.min(input)", "intent": "Returns the minimum value of all elements in the `input` tensor .", "question_id": 4012},
{"snippet": "Tensor.sign()", "intent": "See torch.sign ( )", "question_id": 4013},
{"snippet": "torch.cuda.get_rng_state()", "intent": "Returns the random number generator state of the specified GPU as a ByteTensor .", "question_id": 4014},
{"snippet": "torch.cuda.get_rng_state(device='cuda')", "intent": "Returns the random number generator state of the specified GPU as a ByteTensor . With arguments `device`.", "question_id": 4015},
{"snippet": "Tensor.to_dense()", "intent": "Creates a strided copy of self .", "question_id": 4016},
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`.", "question_id": 4017},
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `bias`.", "question_id": 4018},
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, dtype=torch.qint8)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `dtype`.", "question_id": 4019},
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 4020},
{"snippet": "torch.matrix_exp(input)", "intent": "Computes the matrix exponential of a square matrix or of each square matrix in a batch . For a matrix `input` , the matrix exponential is defined as", "question_id": 4021},
{"snippet": "Tensor.fmax(other)", "intent": "See torch.fmax ( ) With arguments `other`.", "question_id": 4022},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4023},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4024},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4025},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4026},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4027},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4028},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4029},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 4030},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 4031},
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 4032},
{"snippet": "conv2d.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 4033},
{"snippet": "torch.full(size, fill_value)", "intent": "Creates a tensor of `size` size filled with `fill_value` .", "question_id": 4034},
{"snippet": "torch.full(size, fill_value, out=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`.", "question_id": 4035},
{"snippet": "torch.full(size, fill_value, dtype=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . The tensor \u2019 s `dtype` is inferred from fill_value .", "question_id": 4036},
{"snippet": "torch.full(size, fill_value, layout=torch.strided)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `layout`.", "question_id": 4037},
{"snippet": "torch.full(size, fill_value, device=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `device`.", "question_id": 4038},
{"snippet": "torch.full(size, fill_value, requires_grad=False)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `requires_grad`.", "question_id": 4039},
{"snippet": "torch.full(size, fill_value, out=None, dtype=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . The tensor \u2019 s `dtype` is inferred from fill_value . With arguments `out`.", "question_id": 4040},
{"snippet": "torch.full(size, fill_value, out=None, layout=torch.strided)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `layout`.", "question_id": 4041},
{"snippet": "torch.full(size, fill_value, out=None, device=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `device`.", "question_id": 4042},
{"snippet": "torch.full(size, fill_value, out=None, requires_grad=False)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `requires_grad`.", "question_id": 4043},
{"snippet": "Tensor.ne(other)", "intent": "See torch.ne ( ) . With arguments `other`.", "question_id": 4044},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4045},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4046},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4047},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 4048},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4049},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4050},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4051},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4052},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 4053},
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 4054},
{"snippet": "torch.nn.functional.glu(input)", "intent": "The gated linear unit . where `input` is split in half along `dim` to form a and b , \u03c3\\sigma\u03c3 is the sigmoid function and \u2297\\otimes\u2297 is the element-wise product between matrices .", "question_id": 4055},
{"snippet": "torch.nn.functional.glu(input, dim=- 1)", "intent": "The gated linear unit . where `input` is split in half along `dim` to form a and b , \u03c3\\sigma\u03c3 is the sigmoid function and \u2297\\otimes\u2297 is the element-wise product between matrices .", "question_id": 4056},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`.", "question_id": 4057},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 4058},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduce=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 4059},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 4060},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 4061},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 4062},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduce=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 4063},
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 4064},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 4065},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 4066},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 4067},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 4068},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 4069},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 4070},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 4071},
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 4072},
{"snippet": "torch.cummin(input, dim)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative minimum of elements of `input` in the dimension `dim` .", "question_id": 4073},
{"snippet": "torch.cummin(input, dim, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative minimum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 4074},
{"snippet": "torch.nn.functional.softmax(input)", "intent": "Applies a softmax function . With arguments `input`.", "question_id": 4075},
{"snippet": "torch.nn.functional.softmax(input, dim=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`.", "question_id": 4076},
{"snippet": "torch.nn.functional.softmax(input, _stacklevel=3)", "intent": "Applies a softmax function . With arguments `input`, `_stacklevel`.", "question_id": 4077},
{"snippet": "torch.nn.functional.softmax(input, dtype=None)", "intent": "Applies a softmax function . With arguments `input`, `dtype`.", "question_id": 4078},
{"snippet": "torch.nn.functional.softmax(input, dim=None, _stacklevel=3)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `_stacklevel`.", "question_id": 4079},
{"snippet": "torch.nn.functional.softmax(input, dim=None, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `dtype`.", "question_id": 4080},
{"snippet": "torch.nn.functional.softmax(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmax function . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 4081},
{"snippet": "torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 4082},
{"snippet": "Tensor.cummax(dim)", "intent": "See torch.cummax ( ) With arguments `dim`.", "question_id": 4083},
{"snippet": "torch.set_grad_enabled(mode)", "intent": "Context-manager that sets gradient calculation to on or off . set_grad_enabled will enable or disable grads based on its argument `mode` .", "question_id": 4084},
{"snippet": "torch.get_rng_state()", "intent": "Returns the random number generator state as a torch.ByteTensor .", "question_id": 4085},
{"snippet": "Tensor.nextafter(other)", "intent": "See torch.nextafter ( ) With arguments `other`.", "question_id": 4086},
{"snippet": "Tensor.sigmoid()", "intent": "See torch.sigmoid ( )", "question_id": 4087},
{"snippet": "torch.quantization.quantize_qat(model, run_fn, run_args)", "intent": "Do quantization aware training and output a quantized `model` With arguments `run_fn`, `run_args`.", "question_id": 4088},
{"snippet": "torch.quantization.quantize_qat(model, run_fn, run_args, inplace=False)", "intent": "Do quantization aware training and output a quantized `model` With arguments `run_fn`, `run_args`, `inplace`.", "question_id": 4089},
{"snippet": "torch.quantization.observer.default_placeholder_observer", "intent": "alias of torch.quantization.observer.PlaceholderObserver", "question_id": 4090},
{"snippet": "torch.sqrt(input)", "intent": "Returns a new tensor with the square-root of the elements of `input` .", "question_id": 4091},
{"snippet": "torch.sqrt(input, out=None)", "intent": "Returns a new tensor with the square-root of the elements of `input` . With arguments `out`.", "question_id": 4092},
{"snippet": "Tensor.scatter(dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_ ( ) With arguments `dim`, `index`, `src`.", "question_id": 4093},
{"snippet": "torch.quantization.observer.PlaceholderObserver()", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) .", "question_id": 4094},
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`.", "question_id": 4095},
{"snippet": "torch.quantization.observer.PlaceholderObserver(custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`.", "question_id": 4096},
{"snippet": "torch.quantization.observer.PlaceholderObserver(compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `compute_dtype`.", "question_id": 4097},
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`.", "question_id": 4098},
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `compute_dtype`.", "question_id": 4099},
{"snippet": "torch.quantization.observer.PlaceholderObserver(custom_op_name='', compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`, `compute_dtype`.", "question_id": 4100},
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='', compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`, `compute_dtype`.", "question_id": 4101},
{"snippet": "Tensor.tanh_()", "intent": "In-place version of tanh ( )", "question_id": 4102},
{"snippet": "torch.triu_indices(row, col)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates .", "question_id": 4103},
{"snippet": "torch.triu_indices(row, col, offset=0)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider .", "question_id": 4104},
{"snippet": "torch.triu_indices(row, col, dtype=torch.long)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`.", "question_id": 4105},
{"snippet": "torch.triu_indices(row, col, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `device`.", "question_id": 4106},
{"snippet": "torch.triu_indices(row, col, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `layout`.", "question_id": 4107},
{"snippet": "torch.triu_indices(row, col, offset=0, dtype=torch.long)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `dtype`.", "question_id": 4108},
{"snippet": "torch.triu_indices(row, col, offset=0, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `device`.", "question_id": 4109},
{"snippet": "torch.triu_indices(row, col, offset=0, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `layout`.", "question_id": 4110},
{"snippet": "torch.triu_indices(row, col, dtype=torch.long, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `device`.", "question_id": 4111},
{"snippet": "torch.triu_indices(row, col, dtype=torch.long, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `layout`.", "question_id": 4112},
{"snippet": "Tensor.arccos_()", "intent": "In-place version of arccos ( )", "question_id": 4113},
{"snippet": "torch.lcm(input, other)", "intent": "Computes the element-wise least common multiple ( LCM ) of `input` and `other` .", "question_id": 4114},
{"snippet": "torch.lcm(input, other, out=None)", "intent": "Computes the element-wise least common multiple ( LCM ) of `input` and `other` . With arguments `out`.", "question_id": 4115},
{"snippet": "torch.fft.irfft(input)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) .", "question_id": 4116},
{"snippet": "torch.fft.irfft(input, n=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` :", "question_id": 4117},
{"snippet": "torch.fft.irfft(input, dim=- 1)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`.", "question_id": 4118},
{"snippet": "torch.fft.irfft(input, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `norm`.", "question_id": 4119},
{"snippet": "torch.fft.irfft(input, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `out`.", "question_id": 4120},
{"snippet": "torch.fft.irfft(input, n=None, dim=- 1)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `dim`.", "question_id": 4121},
{"snippet": "torch.fft.irfft(input, n=None, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `norm`.", "question_id": 4122},
{"snippet": "torch.fft.irfft(input, n=None, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `out`.", "question_id": 4123},
{"snippet": "torch.fft.irfft(input, dim=- 1, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`, `norm`.", "question_id": 4124},
{"snippet": "torch.fft.irfft(input, dim=- 1, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`, `out`.", "question_id": 4125},
{"snippet": "Tensor.symeig()", "intent": "See torch.symeig ( )", "question_id": 4126},
{"snippet": "Tensor.symeig(eigenvectors=False)", "intent": "See torch.symeig ( ) With arguments `eigenvectors`.", "question_id": 4127},
{"snippet": "Tensor.symeig(upper=True)", "intent": "See torch.symeig ( ) With arguments `upper`.", "question_id": 4128},
{"snippet": "Tensor.symeig(eigenvectors=False, upper=True)", "intent": "See torch.symeig ( ) With arguments `eigenvectors`, `upper`.", "question_id": 4129},
{"snippet": "torch.igamma(input, other)", "intent": "Computes the regularized lower incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive .", "question_id": 4130},
{"snippet": "torch.igamma(input, other, out=None)", "intent": "Computes the regularized lower incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive . If both are zero or either is negative then outi=nan\\text { `out` } _i=\\text { nan } outi\u200b=nan .", "question_id": 4131},
{"snippet": "torch.lerp(input, end, weight)", "intent": "Does a linear interpolation of two tensors start ( given by `input` ) and `end` based on a scalar or tensor `weight` and returns the resulting `out` tensor .", "question_id": 4132},
{"snippet": "torch.lerp(input, end, weight, out=None)", "intent": "Does a linear interpolation of two tensors start ( given by `input` ) and `end` based on a scalar or tensor `weight` and returns the resulting `out` tensor .", "question_id": 4133},
{"snippet": "Tensor.new_full(size, fill_value)", "intent": "Returns a Tensor of `size` size filled with `fill_value` .", "question_id": 4134},
{"snippet": "Tensor.new_full(size, fill_value, dtype=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`.", "question_id": 4135},
{"snippet": "Tensor.new_full(size, fill_value, device=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `device`.", "question_id": 4136},
{"snippet": "Tensor.new_full(size, fill_value, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `requires_grad`.", "question_id": 4137},
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `device`.", "question_id": 4138},
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `requires_grad`.", "question_id": 4139},
{"snippet": "Tensor.new_full(size, fill_value, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `device`, `requires_grad`.", "question_id": 4140},
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 4141},
{"snippet": "torch.nn.AdaptiveAvgPool1d(output_size)", "intent": "Applies a 1D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 4142},
{"snippet": "Tensor.index_put_(indices, values)", "intent": "Puts `values` from the tensor values into the tensor self using the `indices` specified in indices ( which is a tuple of Tensors ) .", "question_id": 4143},
{"snippet": "Tensor.index_put_(indices, values, accumulate=False)", "intent": "Puts `values` from the tensor values into the tensor self using the `indices` specified in indices ( which is a tuple of Tensors ) . If `accumulate` is True , the elements in values are added to self .", "question_id": 4144},
{"snippet": "Tensor.transpose_(dim0, dim1)", "intent": "In-place version of transpose ( ) With arguments `dim0`, `dim1`.", "question_id": 4145},
{"snippet": "Tensor.is_meta", "intent": "Is True if the Tensor is a meta tensor, False otherwise.", "question_id": 4146},
{"snippet": "torch.movedim(input, source, destination)", "intent": "Moves the dimension ( s ) of `input` at the position ( s ) in `source` to the position ( s ) in `destination` .", "question_id": 4147},
{"snippet": "torch.hsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with one or more dimensions , into multiple tensors horizontally according to `indices_or_sections` .", "question_id": 4148},
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`.", "question_id": 4149},
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `weight`.", "question_id": 4150},
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, size_average=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 4151},
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 4152},
{"snippet": "torch.nn.functional.interpolate(input)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4153},
{"snippet": "torch.nn.functional.interpolate(input, size=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4154},
{"snippet": "torch.nn.functional.interpolate(input, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4155},
{"snippet": "torch.nn.functional.interpolate(input, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` The algorithm used for interpolation is determined by `mode` .", "question_id": 4156},
{"snippet": "torch.nn.functional.interpolate(input, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 4157},
{"snippet": "torch.nn.functional.interpolate(input, recompute_scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `recompute_scale_factor`.", "question_id": 4158},
{"snippet": "torch.nn.functional.interpolate(input, size=None, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4159},
{"snippet": "torch.nn.functional.interpolate(input, size=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` The algorithm used for interpolation is determined by `mode` .", "question_id": 4160},
{"snippet": "torch.nn.functional.interpolate(input, size=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 4161},
{"snippet": "torch.nn.functional.interpolate(input, size=None, recompute_scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `recompute_scale_factor`.", "question_id": 4162},
{"snippet": "Tensor.contiguous()", "intent": "Returns a contiguous in memory tensor containing the same data as self tensor .", "question_id": 4163},
{"snippet": "Tensor.contiguous(memory_format=torch.contiguous_format)", "intent": "Returns a contiguous in memory tensor containing the same data as self tensor . With arguments `memory_format`.", "question_id": 4164},
{"snippet": "torch.complex(real, imag)", "intent": "Constructs a complex tensor with its `real` part equal to real and its imaginary part equal to `imag` .", "question_id": 4165},
{"snippet": "torch.complex(real, imag, out=None)", "intent": "Constructs a complex tensor with its `real` part equal to real and its imaginary part equal to `imag` . With arguments `out`.", "question_id": 4166},
{"snippet": "torch.nn.functional.mse_loss(input, target)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`.", "question_id": 4167},
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`.", "question_id": 4168},
{"snippet": "torch.nn.functional.mse_loss(input, target, reduce=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduce`.", "question_id": 4169},
{"snippet": "torch.nn.functional.mse_loss(input, target, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduction`.", "question_id": 4170},
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 4171},
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 4172},
{"snippet": "torch.nn.functional.mse_loss(input, target, reduce=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 4173},
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 4174},
{"snippet": "torch.nn.Softmax2d", "intent": "Applies SoftMax over features to each spatial location.", "question_id": 4175},
{"snippet": "Tensor.mvlgamma_(p)", "intent": "In-place version of mvlgamma ( ) With arguments `p`.", "question_id": 4176},
{"snippet": "Tensor.ormqr(input2, input3)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`.", "question_id": 4177},
{"snippet": "Tensor.ormqr(input2, input3, left=True)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `left`.", "question_id": 4178},
{"snippet": "Tensor.ormqr(input2, input3, transpose=False)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `transpose`.", "question_id": 4179},
{"snippet": "Tensor.ormqr(input2, input3, left=True, transpose=False)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `left`, `transpose`.", "question_id": 4180},
{"snippet": "torch.isreal(input)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is real-valued or not .", "question_id": 4181},
{"snippet": "torch.nn.ConstantPad3d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 4182},
{"snippet": "Tensor.nextafter_(other)", "intent": "In-place version of nextafter ( ) With arguments `other`.", "question_id": 4183},
{"snippet": "Tensor.any()", "intent": "See torch.any ( )", "question_id": 4184},
{"snippet": "Tensor.any(dim=None)", "intent": "See torch.any ( ) With arguments `dim`.", "question_id": 4185},
{"snippet": "Tensor.any(keepdim=False)", "intent": "See torch.any ( ) With arguments `keepdim`.", "question_id": 4186},
{"snippet": "Tensor.any(dim=None, keepdim=False)", "intent": "See torch.any ( ) With arguments `dim`, `keepdim`.", "question_id": 4187},
{"snippet": "Tensor.sin()", "intent": "See torch.sin ( )", "question_id": 4188},
{"snippet": "torch.nn.quantized.functional.hardswish(input, scale, zero_point)", "intent": "This is the quantized version of hardswish ( ) . With arguments `input`, `scale`, `zero_point`.", "question_id": 4189},
{"snippet": "torch.fft.rfft(input)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` .", "question_id": 4190},
{"snippet": "torch.fft.rfft(input, n=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`.", "question_id": 4191},
{"snippet": "torch.fft.rfft(input, dim=- 1)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`.", "question_id": 4192},
{"snippet": "torch.fft.rfft(input, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `norm`.", "question_id": 4193},
{"snippet": "torch.fft.rfft(input, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `out`.", "question_id": 4194},
{"snippet": "torch.fft.rfft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `dim`.", "question_id": 4195},
{"snippet": "torch.fft.rfft(input, n=None, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `norm`.", "question_id": 4196},
{"snippet": "torch.fft.rfft(input, n=None, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `out`.", "question_id": 4197},
{"snippet": "torch.fft.rfft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`, `norm`.", "question_id": 4198},
{"snippet": "torch.fft.rfft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`, `out`.", "question_id": 4199},
{"snippet": "Tensor.ceil_()", "intent": "In-place version of ceil ( )", "question_id": 4200},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`.", "question_id": 4201},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`.", "question_id": 4202},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, stride=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `stride`.", "question_id": 4203},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `padding`.", "question_id": 4204},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `output_padding`.", "question_id": 4205},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, groups=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `groups`.", "question_id": 4206},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `dilation`.", "question_id": 4207},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `stride`.", "question_id": 4208},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `padding`.", "question_id": 4209},
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `output_padding`.", "question_id": 4210},
{"snippet": "Tensor.arccosh_()", "intent": "acosh_ ( ) - > Tensor", "question_id": 4211},
{"snippet": "torch.ge(input, other)", "intent": "Computes input\u2265other\\text { `input` } \\geq \\text { `other` } input\u2265other element-wise .", "question_id": 4212},
{"snippet": "torch.ge(input, other, out=None)", "intent": "Computes input\u2265other\\text { `input` } \\geq \\text { `other` } input\u2265other element-wise . With arguments `out`.", "question_id": 4213},
{"snippet": "torch.autograd.functional.jvp(func, inputs)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`.", "question_id": 4214},
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`.", "question_id": 4215},
{"snippet": "torch.autograd.functional.jvp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`.", "question_id": 4216},
{"snippet": "torch.autograd.functional.jvp(func, inputs, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `strict`.", "question_id": 4217},
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`.", "question_id": 4218},
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `strict`.", "question_id": 4219},
{"snippet": "torch.autograd.functional.jvp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`, `strict`.", "question_id": 4220},
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`, `strict`.", "question_id": 4221},
{"snippet": "torch.nn.modules.module.register_module_forward_hook(hook)", "intent": "Registers a global forward `hook` for all the modules", "question_id": 4222},
{"snippet": "torch.is_nonzero(input)", "intent": "Returns True if the `input` is a single element tensor which is not equal to zero after type conversions .", "question_id": 4223},
{"snippet": "torch.hspmm(mat1, mat2)", "intent": "Performs a matrix multiplication of a sparse COO matrix `mat1` and a strided matrix `mat2` .", "question_id": 4224},
{"snippet": "torch.hspmm(mat1, mat2, out=None)", "intent": "Performs a matrix multiplication of a sparse COO matrix `mat1` and a strided matrix `mat2` . With arguments `out`.", "question_id": 4225},
{"snippet": "Tensor.histc()", "intent": "See torch.histc ( )", "question_id": 4226},
{"snippet": "Tensor.histc(bins=100)", "intent": "See torch.histc ( ) With arguments `bins`.", "question_id": 4227},
{"snippet": "Tensor.histc(min=0)", "intent": "See torch.histc ( ) With arguments `min`.", "question_id": 4228},
{"snippet": "Tensor.histc(max=0)", "intent": "See torch.histc ( ) With arguments `max`.", "question_id": 4229},
{"snippet": "Tensor.histc(bins=100, min=0)", "intent": "See torch.histc ( ) With arguments `bins`, `min`.", "question_id": 4230},
{"snippet": "Tensor.histc(bins=100, max=0)", "intent": "See torch.histc ( ) With arguments `bins`, `max`.", "question_id": 4231},
{"snippet": "Tensor.histc(min=0, max=0)", "intent": "See torch.histc ( ) With arguments `min`, `max`.", "question_id": 4232},
{"snippet": "Tensor.histc(bins=100, min=0, max=0)", "intent": "See torch.histc ( ) With arguments `bins`, `min`, `max`.", "question_id": 4233},
{"snippet": "torch.nn.functional.linear(input, weight)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`.", "question_id": 4234},
{"snippet": "torch.nn.functional.linear(input, weight, bias=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`.", "question_id": 4235},
{"snippet": "Tensor.ldexp(other)", "intent": "See torch.ldexp ( ) With arguments `other`.", "question_id": 4236},
{"snippet": "torch.var(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`.", "question_id": 4237},
{"snippet": "torch.var(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 4238},
{"snippet": "torch.var(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 4239},
{"snippet": "torch.var(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 4240},
{"snippet": "torch.nn.utils.prune.L1Unstructured(amount)", "intent": "Prune ( currently unpruned ) units in a tensor by zeroing out the ones with the lowest L1-norm . With arguments `amount`.", "question_id": 4241},
{"snippet": "l1_unstructured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 4242},
{"snippet": "l1_unstructured.apply(module, name, amount, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `importance_scores`.", "question_id": 4243},
{"snippet": "l1_unstructured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 4244},
{"snippet": "l1_unstructured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 4245},
{"snippet": "l1_unstructured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 4246},
{"snippet": "l1_unstructured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 4247},
{"snippet": "l1_unstructured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 4248},
{"snippet": "l1_unstructured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 4249},
{"snippet": "Tensor.diagflat()", "intent": "See torch.diagflat ( )", "question_id": 4250},
{"snippet": "Tensor.diagflat(offset=0)", "intent": "See torch.diagflat ( ) With arguments `offset`.", "question_id": 4251},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 4252},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 4253},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 4254},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 4255},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 4256},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 4257},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4258},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 4259},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 4260},
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 4261},
{"snippet": "lazy_conv3d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv3d", "question_id": 4262},
{"snippet": "Tensor.q_per_channel_scales()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns a Tensor of scales of the underlying quantizer .", "question_id": 4263},
{"snippet": "torch.nn.MultiLabelMarginLoss()", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) .", "question_id": 4264},
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`.", "question_id": 4265},
{"snippet": "torch.nn.MultiLabelMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduce`.", "question_id": 4266},
{"snippet": "torch.nn.MultiLabelMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduction`.", "question_id": 4267},
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduce`.", "question_id": 4268},
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduction`.", "question_id": 4269},
{"snippet": "torch.nn.MultiLabelMarginLoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduce`, `reduction`.", "question_id": 4270},
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduce`, `reduction`.", "question_id": 4271},
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 4272},
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 4273},
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 4274},
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 4275},
{"snippet": "torch.nn.functional.fractional_max_pool3d(*args, **kwargs)", "intent": "Applies 3D fractional max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 4276},
{"snippet": "torch.initial_seed()", "intent": "Returns the initial seed for generating random numbers as a Python long .", "question_id": 4277},
{"snippet": "torch.nn.utils.spectral_norm(module)", "intent": "Applies spectral normalization to a parameter in the given `module` .", "question_id": 4278},
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight')", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`.", "question_id": 4279},
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`.", "question_id": 4280},
{"snippet": "torch.nn.utils.spectral_norm(module, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `eps`.", "question_id": 4281},
{"snippet": "torch.nn.utils.spectral_norm(module, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 4282},
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `n_power_iterations`.", "question_id": 4283},
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `eps`.", "question_id": 4284},
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `dim`.", "question_id": 4285},
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `eps`.", "question_id": 4286},
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `dim`.", "question_id": 4287},
{"snippet": "Tensor.le(other)", "intent": "See torch.le ( ) . With arguments `other`.", "question_id": 4288},
{"snippet": "Tensor.dist(other)", "intent": "See torch.dist ( ) With arguments `other`.", "question_id": 4289},
{"snippet": "Tensor.dist(other, p=2)", "intent": "See torch.dist ( ) With arguments `other`, `p`.", "question_id": 4290},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence .", "question_id": 4291},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence .", "question_id": 4292},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, sorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`.", "question_id": 4293},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `unsorted_indices`.", "question_id": 4294},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`.", "question_id": 4295},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `unsorted_indices`.", "question_id": 4296},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, sorted_indices=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`, `unsorted_indices`.", "question_id": 4297},
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`, `unsorted_indices`.", "question_id": 4298},
{"snippet": "packed_sequence.batch_sizes", "intent": "Alias for field number 1", "question_id": 4299},
{"snippet": "packed_sequence.count(value, /)", "intent": "Return number of occurrences of `value` . With arguments `/`.", "question_id": 4300},
{"snippet": "packed_sequence.data", "intent": "Alias for field number 0", "question_id": 4301},
{"snippet": "packed_sequence.index(value, /)", "intent": "Return first index of `value` . With arguments `/`.", "question_id": 4302},
{"snippet": "packed_sequence.index(value, /, start=0)", "intent": "Return first index of `value` . With arguments `/`, `start`.", "question_id": 4303},
{"snippet": "packed_sequence.index(value, /, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `stop`.", "question_id": 4304},
{"snippet": "packed_sequence.index(value, /, start=0, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `start`, `stop`.", "question_id": 4305},
{"snippet": "is_cuda", "intent": "Returns true if self.data stored on a gpu", "question_id": 4306},
{"snippet": "packed_sequence.is_pinned()", "intent": "Returns true if self.data stored on in pinned memory", "question_id": 4307},
{"snippet": "packed_sequence.sorted_indices", "intent": "Alias for field number 2", "question_id": 4308},
{"snippet": "packed_sequence.to(*args, **kwargs)", "intent": "Performs dtype and/or device conversion on self.data . With arguments `*args`, `**kwargs`.", "question_id": 4309},
{"snippet": "packed_sequence.unsorted_indices", "intent": "Alias for field number 3", "question_id": 4310},
{"snippet": "Tensor.rot90(k, dims)", "intent": "See torch.rot90 ( ) With arguments `k`, `dims`.", "question_id": 4311},
{"snippet": "Tensor.sspaddmm(mat1, mat2)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`.", "question_id": 4312},
{"snippet": "Tensor.sspaddmm(mat1, mat2, beta=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 4313},
{"snippet": "Tensor.sspaddmm(mat1, mat2, alpha=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 4314},
{"snippet": "Tensor.sspaddmm(mat1, mat2, beta=1, alpha=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 4315},
{"snippet": "Tensor.trunc_()", "intent": "In-place version of trunc ( )", "question_id": 4316},
{"snippet": "torch.quantile(input, q)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input .", "question_id": 4317},
{"snippet": "torch.quantile(input, q, dim=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input .", "question_id": 4318},
{"snippet": "torch.quantile(input, q, keepdim=False)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`.", "question_id": 4319},
{"snippet": "torch.quantile(input, q, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `out`.", "question_id": 4320},
{"snippet": "torch.quantile(input, q, dim=None, keepdim=False)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`.", "question_id": 4321},
{"snippet": "torch.quantile(input, q, dim=None, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `out`.", "question_id": 4322},
{"snippet": "torch.quantile(input, q, keepdim=False, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`, `out`.", "question_id": 4323},
{"snippet": "torch.quantile(input, q, dim=None, keepdim=False, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`, `out`.", "question_id": 4324},
{"snippet": "torch.autograd.functional.hessian(func, inputs)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`.", "question_id": 4325},
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`.", "question_id": 4326},
{"snippet": "torch.autograd.functional.hessian(func, inputs, strict=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `strict`.", "question_id": 4327},
{"snippet": "torch.autograd.functional.hessian(func, inputs, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `vectorize`.", "question_id": 4328},
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `strict`.", "question_id": 4329},
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `vectorize`.", "question_id": 4330},
{"snippet": "torch.autograd.functional.hessian(func, inputs, strict=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `strict`, `vectorize`.", "question_id": 4331},
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `strict`, `vectorize`.", "question_id": 4332},
{"snippet": "Tensor.bitwise_not_()", "intent": "In-place version of bitwise_not ( )", "question_id": 4333},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver()", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values .", "question_id": 4334},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`.", "question_id": 4335},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `dtype`.", "question_id": 4336},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `qscheme`.", "question_id": 4337},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `reduce_range`.", "question_id": 4338},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `quant_min`.", "question_id": 4339},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `quant_max`.", "question_id": 4340},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(factory_kwargs=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `factory_kwargs`.", "question_id": 4341},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`, `dtype`.", "question_id": 4342},
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`, `qscheme`.", "question_id": 4343},
{"snippet": "torch.std(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`.", "question_id": 4344},
{"snippet": "torch.std(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 4345},
{"snippet": "torch.std(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 4346},
{"snippet": "torch.std(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 4347},
{"snippet": "torch.nn.utils.prune.RandomUnstructured(amount)", "intent": "Prune ( currently unpruned ) units in a tensor at random . With arguments `amount`.", "question_id": 4348},
{"snippet": "random_unstructured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 4349},
{"snippet": "random_unstructured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 4350},
{"snippet": "random_unstructured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 4351},
{"snippet": "random_unstructured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 4352},
{"snippet": "random_unstructured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 4353},
{"snippet": "random_unstructured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 4354},
{"snippet": "random_unstructured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 4355},
{"snippet": "Tensor.repeat_interleave(repeats)", "intent": "See torch.repeat_interleave ( ) . With arguments `repeats`.", "question_id": 4356},
{"snippet": "Tensor.repeat_interleave(repeats, dim=None)", "intent": "See torch.repeat_interleave ( ) . With arguments `repeats`, `dim`.", "question_id": 4357},
{"snippet": "torch.bitwise_not(input)", "intent": "Computes the bitwise NOT of the given `input` tensor .", "question_id": 4358},
{"snippet": "torch.bitwise_not(input, out=None)", "intent": "Computes the bitwise NOT of the given `input` tensor . With arguments `out`.", "question_id": 4359},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`.", "question_id": 4360},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`.", "question_id": 4361},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`.", "question_id": 4362},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `verbose`.", "question_id": 4363},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`.", "question_id": 4364},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `verbose`.", "question_id": 4365},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 4366},
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 4367},
{"snippet": "step_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 4368},
{"snippet": "step_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 4369},
{"snippet": "step_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 4370},
{"snippet": "step_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 4371},
{"snippet": "step_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 4372},
{"snippet": "torch.nn.intrinsic.ConvBn1d(conv, bn)", "intent": "This is a sequential container which calls the Conv 1d and Batch Norm 1d modules . With arguments `conv`, `bn`.", "question_id": 4373},
{"snippet": "torch.nn.Upsample()", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data .", "question_id": 4374},
{"snippet": "torch.nn.Upsample(size=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 4375},
{"snippet": "torch.nn.Upsample(scale_factor=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 4376},
{"snippet": "torch.nn.Upsample(mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . With arguments `mode`.", "question_id": 4377},
{"snippet": "torch.nn.Upsample(align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . With arguments `align_corners`.", "question_id": 4378},
{"snippet": "torch.nn.Upsample(size=None, scale_factor=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 4379},
{"snippet": "torch.nn.Upsample(size=None, mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `mode`.", "question_id": 4380},
{"snippet": "torch.nn.Upsample(size=None, align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `align_corners`.", "question_id": 4381},
{"snippet": "torch.nn.Upsample(scale_factor=None, mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `mode`.", "question_id": 4382},
{"snippet": "torch.nn.Upsample(scale_factor=None, align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `align_corners`.", "question_id": 4383},
{"snippet": "torch.use_deterministic_algorithms(mode)", "intent": "Sets whether PyTorch operations must use \u201c deterministic \u201d algorithms . With arguments `mode`.", "question_id": 4384},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4385},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4386},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4387},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4388},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4389},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4390},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4391},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 4392},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 4393},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 4394},
{"snippet": "torch.cuda.synchronize()", "intent": "Waits for all kernels in all streams on a CUDA `device` to complete .", "question_id": 4395},
{"snippet": "torch.cuda.synchronize(device=None)", "intent": "Waits for all kernels in all streams on a CUDA `device` to complete .", "question_id": 4396},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`.", "question_id": 4397},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 4398},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, device=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `device`.", "question_id": 4399},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 4400},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 4401},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 4402},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, device=None, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `device`, `dtype`.", "question_id": 4403},
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `device`, `dtype`.", "question_id": 4404},
{"snippet": "torch.frexp(input, Tensor exponent)", "intent": "Decomposes `input` into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text { input } = \\text { mantissa } \\times 2^ { \\text { exponent } } input=mantissa\u00d72exponent . With arguments `Tensor exponent`.", "question_id": 4405},
{"snippet": "torch.frexp(input, Tensor exponent, out=None) -> (Tensor mantissa)", "intent": "Decomposes `input` into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text { input } = \\text { mantissa } \\times 2^ { \\text { exponent } } input=mantissa\u00d72exponent . With arguments `Tensor exponent`, `out`.", "question_id": 4406},
{"snippet": "torch.numel(input)", "intent": "Returns the total number of elements in the `input` tensor .", "question_id": 4407},
{"snippet": "Tensor.log2()", "intent": "See torch.log2 ( )", "question_id": 4408},
{"snippet": "Tensor.share_memory_()", "intent": "Moves the underlying storage to shared memory .", "question_id": 4409},
{"snippet": "torch.chain_matmul(*matrices)", "intent": "Returns the matrix product of the NNN 2-D tensors . With arguments `*matrices`.", "question_id": 4410},
{"snippet": "torch.chain_matmul(*matrices, out=None)", "intent": "Returns the matrix product of the NNN 2-D tensors . With arguments `*matrices`, `out`.", "question_id": 4411},
{"snippet": "Optimizer.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 4412},
{"snippet": "torch.nn.functional.rrelu_(input)", "intent": "In-place version of rrelu ( ) . With arguments `input`.", "question_id": 4413},
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`.", "question_id": 4414},
{"snippet": "torch.nn.functional.rrelu_(input, upper=1. / 3)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `upper`.", "question_id": 4415},
{"snippet": "torch.nn.functional.rrelu_(input, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `training`.", "question_id": 4416},
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, upper=1. / 3)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `upper`.", "question_id": 4417},
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `training`.", "question_id": 4418},
{"snippet": "torch.nn.functional.rrelu_(input, upper=1. / 3, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `upper`, `training`.", "question_id": 4419},
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, upper=1. / 3, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `upper`, `training`.", "question_id": 4420},
{"snippet": "Tensor.cholesky()", "intent": "See torch.cholesky ( )", "question_id": 4421},
{"snippet": "Tensor.cholesky(upper=False)", "intent": "See torch.cholesky ( ) With arguments `upper`.", "question_id": 4422},
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`.", "question_id": 4423},
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, negative_slope=0.01)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `negative_slope`.", "question_id": 4424},
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `inplace`.", "question_id": 4425},
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, negative_slope=0.01, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `negative_slope`, `inplace`.", "question_id": 4426},
{"snippet": "Tensor.acos()", "intent": "See torch.acos ( )", "question_id": 4427},
{"snippet": "torch.optim.ASGD(params)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`.", "question_id": 4428},
{"snippet": "torch.optim.ASGD(params, lr=0.01)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`.", "question_id": 4429},
{"snippet": "torch.optim.ASGD(params, lambd=0.0001)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lambd`.", "question_id": 4430},
{"snippet": "torch.optim.ASGD(params, alpha=0.75)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `alpha`.", "question_id": 4431},
{"snippet": "torch.optim.ASGD(params, t0=1000000.0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `t0`.", "question_id": 4432},
{"snippet": "torch.optim.ASGD(params, weight_decay=0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `weight_decay`.", "question_id": 4433},
{"snippet": "torch.optim.ASGD(params, lr=0.01, lambd=0.0001)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `lambd`.", "question_id": 4434},
{"snippet": "torch.optim.ASGD(params, lr=0.01, alpha=0.75)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `alpha`.", "question_id": 4435},
{"snippet": "torch.optim.ASGD(params, lr=0.01, t0=1000000.0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `t0`.", "question_id": 4436},
{"snippet": "torch.optim.ASGD(params, lr=0.01, weight_decay=0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `weight_decay`.", "question_id": 4437},
{"snippet": "asgd.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 4438},
{"snippet": "asgd.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 4439},
{"snippet": "asgd.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 4440},
{"snippet": "asgd.step()", "intent": "Performs a single optimization step .", "question_id": 4441},
{"snippet": "asgd.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 4442},
{"snippet": "asgd.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 4443},
{"snippet": "asgd.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 4444},
{"snippet": "Tensor.arcsinh()", "intent": "See torch.arcsinh ( )", "question_id": 4445},
{"snippet": "torch.cholesky_inverse(input)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . With arguments `input`.", "question_id": 4446},
{"snippet": "torch.cholesky_inverse(input, upper=False)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . If `upper` is False , uuu is lower triangular such that the returned tensor is With arguments `input`.", "question_id": 4447},
{"snippet": "torch.cholesky_inverse(input, out=None)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . With arguments `input`, `out`.", "question_id": 4448},
{"snippet": "torch.cholesky_inverse(input, upper=False, out=None)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . If `upper` is False , uuu is lower triangular such that the returned tensor is With arguments `input`, `out`.", "question_id": 4449},
{"snippet": "torch.diff(input)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] .", "question_id": 4450},
{"snippet": "torch.diff(input, n=1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`.", "question_id": 4451},
{"snippet": "torch.diff(input, dim=- 1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`.", "question_id": 4452},
{"snippet": "torch.diff(input, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `prepend`.", "question_id": 4453},
{"snippet": "torch.diff(input, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `append`.", "question_id": 4454},
{"snippet": "torch.diff(input, n=1, dim=- 1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `dim`.", "question_id": 4455},
{"snippet": "torch.diff(input, n=1, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `prepend`.", "question_id": 4456},
{"snippet": "torch.diff(input, n=1, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `append`.", "question_id": 4457},
{"snippet": "torch.diff(input, dim=- 1, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`, `prepend`.", "question_id": 4458},
{"snippet": "torch.diff(input, dim=- 1, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`, `append`.", "question_id": 4459},
{"snippet": "torch.range(end)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 4460},
{"snippet": "torch.range(end, start=0)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 4461},
{"snippet": "torch.range(end, step=1)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 4462},
{"snippet": "torch.range(end, out=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `out`.", "question_id": 4463},
{"snippet": "torch.range(end, dtype=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `dtype`.", "question_id": 4464},
{"snippet": "torch.range(end, layout=torch.strided)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `layout`.", "question_id": 4465},
{"snippet": "torch.range(end, device=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `device`.", "question_id": 4466},
{"snippet": "torch.range(end, requires_grad=False)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `requires_grad`.", "question_id": 4467},
{"snippet": "torch.range(end, start=0, step=1)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 4468},
{"snippet": "torch.range(end, start=0, out=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `out`.", "question_id": 4469},
{"snippet": "torch.nn.functional.dropout2d(input)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 4470},
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 4471},
{"snippet": "torch.nn.functional.dropout2d(input, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 4472},
{"snippet": "torch.nn.functional.dropout2d(input, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 4473},
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 4474},
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 4475},
{"snippet": "torch.nn.functional.dropout2d(input, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 4476},
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 4477},
{"snippet": "torch.quantization.observer.NoopObserver()", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) .", "question_id": 4478},
{"snippet": "torch.quantization.observer.NoopObserver(dtype=torch.float16)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`.", "question_id": 4479},
{"snippet": "torch.quantization.observer.NoopObserver(custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`.", "question_id": 4480},
{"snippet": "torch.quantization.observer.NoopObserver(dtype=torch.float16, custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`.", "question_id": 4481},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module)", "intent": "Applies spectral normalization to a parameter in the given `module` .", "question_id": 4482},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight')", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`.", "question_id": 4483},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`.", "question_id": 4484},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `eps`.", "question_id": 4485},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 4486},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `n_power_iterations`.", "question_id": 4487},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `eps`.", "question_id": 4488},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `dim`.", "question_id": 4489},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `eps`.", "question_id": 4490},
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `dim`.", "question_id": 4491},
{"snippet": "torch.bucketize(input, boundaries)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries .", "question_id": 4492},
{"snippet": "torch.bucketize(input, boundaries, out_int32=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out_int32`.", "question_id": 4493},
{"snippet": "torch.bucketize(input, boundaries, right=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed .", "question_id": 4494},
{"snippet": "torch.bucketize(input, boundaries, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out`.", "question_id": 4495},
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, right=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out_int32`.", "question_id": 4496},
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out_int32`, `out`.", "question_id": 4497},
{"snippet": "torch.bucketize(input, boundaries, right=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out`.", "question_id": 4498},
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, right=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out_int32`, `out`.", "question_id": 4499},
{"snippet": "torch.quantization.quantize_dynamic(model)", "intent": "Converts a float `model` to dynamic ( i.e .", "question_id": 4500},
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `qconfig_spec`.", "question_id": 4501},
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 .", "question_id": 4502},
{"snippet": "torch.quantization.quantize_dynamic(model, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) .", "question_id": 4503},
{"snippet": "torch.quantization.quantize_dynamic(model, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `inplace`.", "question_id": 4504},
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . With arguments `qconfig_spec`.", "question_id": 4505},
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) . With arguments `qconfig_spec`.", "question_id": 4506},
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `qconfig_spec`, `inplace`.", "question_id": 4507},
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) .", "question_id": 4508},
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . With arguments `inplace`.", "question_id": 4509},
{"snippet": "torch.quantization.swap_module(mod, mapping, custom_module_class_mapping)", "intent": "Swaps the module if it has a quantized counterpart and it has an observer attached . With arguments `mod`, `mapping`, `custom_module_class_mapping`.", "question_id": 4510},
{"snippet": "torch.sspaddmm(input, mat1, mat2)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result .", "question_id": 4511},
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`.", "question_id": 4512},
{"snippet": "torch.sspaddmm(input, mat1, mat2, alpha=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `alpha`.", "question_id": 4513},
{"snippet": "torch.sspaddmm(input, mat1, mat2, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `out`.", "question_id": 4514},
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, alpha=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `alpha`.", "question_id": 4515},
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `out`.", "question_id": 4516},
{"snippet": "torch.sspaddmm(input, mat1, mat2, alpha=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `alpha`, `out`.", "question_id": 4517},
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, alpha=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `alpha`, `out`.", "question_id": 4518},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`.", "question_id": 4519},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`.", "question_id": 4520},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`.", "question_id": 4521},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `reduce`.", "question_id": 4522},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `reduction`.", "question_id": 4523},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `size_average`.", "question_id": 4524},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduce`.", "question_id": 4525},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduction`.", "question_id": 4526},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduce`.", "question_id": 4527},
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduction`.", "question_id": 4528},
{"snippet": "Tensor.addbmm_(batch1, batch2)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 4529},
{"snippet": "Tensor.addbmm_(batch1, batch2, beta=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 4530},
{"snippet": "Tensor.addbmm_(batch1, batch2, alpha=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 4531},
{"snippet": "Tensor.addbmm_(batch1, batch2, beta=1, alpha=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 4532},
{"snippet": "Tensor.byte()", "intent": "self.byte ( ) is equivalent to self.to ( torch.uint8 ) .", "question_id": 4533},
{"snippet": "Tensor.byte(memory_format=torch.preserve_format)", "intent": "self.byte ( ) is equivalent to self.to ( torch.uint8 ) . With arguments `memory_format`.", "question_id": 4534},
{"snippet": "torch.cuda.device_of(obj)", "intent": "Context-manager that changes the current device to that of given object . With arguments `obj`.", "question_id": 4535},
{"snippet": "Tensor.atanh_(other)", "intent": "In-place version of atanh ( ) With arguments `other`.", "question_id": 4536},
{"snippet": "Tensor.clamp_()", "intent": "In-place version of clamp ( )", "question_id": 4537},
{"snippet": "Tensor.clamp_(min=None)", "intent": "In-place version of clamp ( ) With arguments `min`.", "question_id": 4538},
{"snippet": "Tensor.clamp_(max=None)", "intent": "In-place version of clamp ( ) With arguments `max`.", "question_id": 4539},
{"snippet": "Tensor.clamp_(min=None, max=None)", "intent": "In-place version of clamp ( ) With arguments `min`, `max`.", "question_id": 4540},
{"snippet": "torch.asinh(input)", "intent": "Returns a new tensor with the inverse hyperbolic sine of the elements of `input` .", "question_id": 4541},
{"snippet": "torch.asinh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic sine of the elements of `input` . With arguments `out`.", "question_id": 4542},
{"snippet": "torch.nn.functional.local_response_norm(input, size)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`.", "question_id": 4543},
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`.", "question_id": 4544},
{"snippet": "torch.nn.functional.local_response_norm(input, size, beta=0.75)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`.", "question_id": 4545},
{"snippet": "torch.nn.functional.local_response_norm(input, size, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `k`.", "question_id": 4546},
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`.", "question_id": 4547},
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `k`.", "question_id": 4548},
{"snippet": "torch.nn.functional.local_response_norm(input, size, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`, `k`.", "question_id": 4549},
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`, `k`.", "question_id": 4550},
{"snippet": "torch.nn.GRU(*args, **kwargs)", "intent": "Applies a multi-layer gated recurrent unit ( GRU ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 4551},
{"snippet": "Tensor.long()", "intent": "self.long ( ) is equivalent to self.to ( torch.int64 ) .", "question_id": 4552},
{"snippet": "Tensor.long(memory_format=torch.preserve_format)", "intent": "self.long ( ) is equivalent to self.to ( torch.int64 ) . With arguments `memory_format`.", "question_id": 4553},
{"snippet": "Tensor.renorm_(p, dim, maxnorm)", "intent": "In-place version of renorm ( ) With arguments `p`, `dim`, `maxnorm`.", "question_id": 4554},
{"snippet": "Tensor.floor_divide(value)", "intent": "See torch.floor_divide ( ) With arguments `value`.", "question_id": 4555},
{"snippet": "Tensor.bool()", "intent": "self.bool ( ) is equivalent to self.to ( torch.bool ) .", "question_id": 4556},
{"snippet": "Tensor.bool(memory_format=torch.preserve_format)", "intent": "self.bool ( ) is equivalent to self.to ( torch.bool ) . With arguments `memory_format`.", "question_id": 4557},
{"snippet": "Tensor.lcm(other)", "intent": "See torch.lcm ( ) With arguments `other`.", "question_id": 4558},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`.", "question_id": 4559},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`.", "question_id": 4560},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuser_func=<function fuse_known_modules>)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuser_func`.", "question_id": 4561},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuse_custom_config_dict`.", "question_id": 4562},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuser_func`.", "question_id": 4563},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuse_custom_config_dict`.", "question_id": 4564},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuser_func`, `fuse_custom_config_dict`.", "question_id": 4565},
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuser_func`, `fuse_custom_config_dict`.", "question_id": 4566},
{"snippet": "torch.sub(input, other)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` .", "question_id": 4567},
{"snippet": "torch.sub(input, other, alpha=1)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` .", "question_id": 4568},
{"snippet": "torch.sub(input, other, out=None)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` . With arguments `out`.", "question_id": 4569},
{"snippet": "torch.sub(input, other, alpha=1, out=None)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` . With arguments `out`.", "question_id": 4570},
{"snippet": "torch.nn.PixelUnshuffle(downscale_factor)", "intent": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) to a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) , where r is a downscale factor . With arguments `downscale_factor`.", "question_id": 4571},
{"snippet": "torch.chunk(input, chunks)", "intent": "Splits a tensor into a specific number of `chunks` . Each chunk is a view of the `input` tensor .", "question_id": 4572},
{"snippet": "torch.chunk(input, chunks, dim=0)", "intent": "Splits a tensor into a specific number of `chunks` . Each chunk is a view of the `input` tensor . Last chunk will be smaller if the tensor size along the given dimension `dim` is not divisible by chunks .", "question_id": 4573},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`.", "question_id": 4574},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `step_size_up`.", "question_id": 4575},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_down=None)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `step_size_down`.", "question_id": 4576},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, mode='triangular')", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `mode`.", "question_id": 4577},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, gamma=1.0)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `gamma`.", "question_id": 4578},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, scale_fn=None)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `scale_fn`.", "question_id": 4579},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, scale_mode='cycle')", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `scale_mode`.", "question_id": 4580},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, cycle_momentum=True)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `cycle_momentum`.", "question_id": 4581},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, base_momentum=0.8)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `base_momentum`.", "question_id": 4582},
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, max_momentum=0.9)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `max_momentum`.", "question_id": 4583},
{"snippet": "cyclic_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 4584},
{"snippet": "cyclic_lr.get_lr()", "intent": "Calculates the learning rate at batch index .", "question_id": 4585},
{"snippet": "cyclic_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 4586},
{"snippet": "cyclic_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 4587},
{"snippet": "cyclic_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 4588},
{"snippet": "cyclic_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 4589},
{"snippet": "torch.nn.functional.affine_grid(theta, size)", "intent": "Generates a 2D or 3D flow field ( sampling grid ) , given a batch of affine matrices `theta` . With arguments `size`.", "question_id": 4590},
{"snippet": "torch.nn.functional.affine_grid(theta, size, align_corners=None)", "intent": "Generates a 2D or 3D flow field ( sampling grid ) , given a batch of affine matrices `theta` . With arguments `size`, `align_corners`.", "question_id": 4591},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 4592},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 4593},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 4594},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 4595},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 4596},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 4597},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dilation`.", "question_id": 4598},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 4599},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 4600},
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 4601},
{"snippet": "torch.linalg.inv(A)", "intent": "Computes the inverse of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 4602},
{"snippet": "torch.linalg.inv(A, out=None)", "intent": "Computes the inverse of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 4603},
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`.", "question_id": 4604},
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`.", "question_id": 4605},
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, dtype=torch.qint8)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 4606},
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`, `dtype`.", "question_id": 4607},
{"snippet": "linear.from_float(mod)", "intent": "Create a dynamic quantized module from a float module or qparams_dict With arguments `mod`.", "question_id": 4608},
{"snippet": "Tensor.igamma(other)", "intent": "See torch.igamma ( ) With arguments `other`.", "question_id": 4609},
{"snippet": "torch.nn.LogSigmoid", "intent": "Applies the element-wise function:", "question_id": 4610},
{"snippet": "Tensor.lt(other)", "intent": "See torch.lt ( ) . With arguments `other`.", "question_id": 4611},
{"snippet": "Tensor.arccos()", "intent": "See torch.arccos ( )", "question_id": 4612},
{"snippet": "Tensor.cos_()", "intent": "In-place version of cos ( )", "question_id": 4613},
{"snippet": "torch.optim.SparseAdam(params, 0.999))", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`.", "question_id": 4614},
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`.", "question_id": 4615},
{"snippet": "torch.optim.SparseAdam(params, 0.999), betas=(0.9)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `betas`.", "question_id": 4616},
{"snippet": "torch.optim.SparseAdam(params, 0.999), eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `eps`.", "question_id": 4617},
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 4618},
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 4619},
{"snippet": "torch.optim.SparseAdam(params, 0.999), betas=(0.9, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `betas`, `eps`.", "question_id": 4620},
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, betas=(0.9, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `betas`, `eps`.", "question_id": 4621},
{"snippet": "sparse_adam.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 4622},
{"snippet": "sparse_adam.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 4623},
{"snippet": "sparse_adam.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 4624},
{"snippet": "sparse_adam.step()", "intent": "Performs a single optimization step .", "question_id": 4625},
{"snippet": "sparse_adam.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 4626},
{"snippet": "sparse_adam.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 4627},
{"snippet": "sparse_adam.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 4628},
{"snippet": "Tensor.sgn()", "intent": "See torch.sgn ( )", "question_id": 4629},
{"snippet": "Tensor.atan2(other)", "intent": "See torch.atan2 ( ) With arguments `other`.", "question_id": 4630},
{"snippet": "torch.igammac(input, other)", "intent": "Computes the regularized upper incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive .", "question_id": 4631},
{"snippet": "torch.igammac(input, other, out=None)", "intent": "Computes the regularized upper incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive . If both are zero or either is negative then outi=nan\\text { `out` } _i=\\text { nan } outi\u200b=nan .", "question_id": 4632},
{"snippet": "Tensor.dim()", "intent": "Returns the number of dimensions of self tensor .", "question_id": 4633},
{"snippet": "sparse_adam.flatten_parameters()", "intent": "Resets parameter data pointer so that they can use faster code paths .", "question_id": 4634},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4635},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4636},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4637},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4638},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4639},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4640},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4641},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 4642},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 4643},
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 4644},
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`.", "question_id": 4645},
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=- 1)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `last_epoch`.", "question_id": 4646},
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, verbose=False)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `verbose`.", "question_id": 4647},
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=- 1, verbose=False)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `last_epoch`, `verbose`.", "question_id": 4648},
{"snippet": "multiplicative_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 4649},
{"snippet": "multiplicative_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 4650},
{"snippet": "multiplicative_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 4651},
{"snippet": "multiplicative_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 4652},
{"snippet": "multiplicative_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 4653},
{"snippet": "torch.quantization.observer.default_debug_observer", "intent": "alias of torch.quantization.observer.RecordingObserver", "question_id": 4654},
{"snippet": "torch.div(input, other)", "intent": "Divides each element of the `input` input by the corresponding element of `other` .", "question_id": 4655},
{"snippet": "torch.div(input, other, rounding_mode=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `rounding_mode`.", "question_id": 4656},
{"snippet": "torch.div(input, other, out=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `out`.", "question_id": 4657},
{"snippet": "torch.div(input, other, rounding_mode=None, out=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `rounding_mode`, `out`.", "question_id": 4658},
{"snippet": "Tensor.less_equal(other)", "intent": "See torch.less_equal ( ) . With arguments `other`.", "question_id": 4659},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 4660},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 4661},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 4662},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 4663},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, mode='sum')", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `mode`.", "question_id": 4664},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, sparse=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 4665},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, _weight=None)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 4666},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, include_last_offset=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `include_last_offset`.", "question_id": 4667},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, dtype=torch.quint8)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 4668},
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`, `norm_type`.", "question_id": 4669},
{"snippet": "embedding_bag.from_float(mod)", "intent": "Create a quantized embedding_bag module from a float module With arguments `mod`.", "question_id": 4670},
{"snippet": "torch.zeros(*size)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`.", "question_id": 4671},
{"snippet": "torch.zeros(*size, out=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`.", "question_id": 4672},
{"snippet": "torch.zeros(*size, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `dtype`.", "question_id": 4673},
{"snippet": "torch.zeros(*size, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `layout`.", "question_id": 4674},
{"snippet": "torch.zeros(*size, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `device`.", "question_id": 4675},
{"snippet": "torch.zeros(*size, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `requires_grad`.", "question_id": 4676},
{"snippet": "torch.zeros(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `dtype`.", "question_id": 4677},
{"snippet": "torch.zeros(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `layout`.", "question_id": 4678},
{"snippet": "torch.zeros(*size, out=None, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `device`.", "question_id": 4679},
{"snippet": "torch.zeros(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `requires_grad`.", "question_id": 4680},
{"snippet": "torch.i0(input)", "intent": "Computes the zeroth order modified Bessel function of the first kind for each element of `input` .", "question_id": 4681},
{"snippet": "torch.i0(input, out=None)", "intent": "Computes the zeroth order modified Bessel function of the first kind for each element of `input` . With arguments `out`.", "question_id": 4682},
{"snippet": "torch.jit.unused(fn)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception . With arguments `fn`.", "question_id": 4683},
{"snippet": "torch.jit.wait(future)", "intent": "Forces completion of a torch.jit.Future [ T ] asynchronous task , returning the result of the task . With arguments `future`.", "question_id": 4684},
{"snippet": "torch.nn.PoissonNLLLoss()", "intent": "Negative log likelihood loss with Poisson distribution of target .", "question_id": 4685},
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`.", "question_id": 4686},
{"snippet": "torch.nn.PoissonNLLLoss(full=False)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `full`.", "question_id": 4687},
{"snippet": "torch.nn.PoissonNLLLoss(size_average=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `size_average`.", "question_id": 4688},
{"snippet": "torch.nn.PoissonNLLLoss(eps=1e-08)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `eps`.", "question_id": 4689},
{"snippet": "torch.nn.PoissonNLLLoss(reduce=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `reduce`.", "question_id": 4690},
{"snippet": "torch.nn.PoissonNLLLoss(reduction='mean')", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `reduction`.", "question_id": 4691},
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, full=False)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `full`.", "question_id": 4692},
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, size_average=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `size_average`.", "question_id": 4693},
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, eps=1e-08)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `eps`.", "question_id": 4694},
{"snippet": "torch.cuda.Event()", "intent": "Wrapper around a CUDA event .", "question_id": 4695},
{"snippet": "torch.cuda.Event(enable_timing=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`.", "question_id": 4696},
{"snippet": "torch.cuda.Event(blocking=False)", "intent": "Wrapper around a CUDA event . With arguments `blocking`.", "question_id": 4697},
{"snippet": "torch.cuda.Event(interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `interprocess`.", "question_id": 4698},
{"snippet": "torch.cuda.Event(enable_timing=False, blocking=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `blocking`.", "question_id": 4699},
{"snippet": "torch.cuda.Event(enable_timing=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `interprocess`.", "question_id": 4700},
{"snippet": "torch.cuda.Event(blocking=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `blocking`, `interprocess`.", "question_id": 4701},
{"snippet": "torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `blocking`, `interprocess`.", "question_id": 4702},
{"snippet": "event.elapsed_time(end_event)", "intent": "Returns the time elapsed in milliseconds after the event was recorded and before the `end_event` was recorded .", "question_id": 4703},
{"snippet": "event.from_ipc_handle(device, handle)", "intent": "Reconstruct an event from an IPC `handle` on the given `device` .", "question_id": 4704},
{"snippet": "event.ipc_handle()", "intent": "Returns an IPC handle of this event .", "question_id": 4705},
{"snippet": "event.query()", "intent": "Checks if all work currently captured by event has completed .", "question_id": 4706},
{"snippet": "event.record()", "intent": "Records the event in a given `stream` .", "question_id": 4707},
{"snippet": "event.record(stream=None)", "intent": "Records the event in a given `stream` .", "question_id": 4708},
{"snippet": "event.synchronize()", "intent": "Waits for the event to complete .", "question_id": 4709},
{"snippet": "event.wait()", "intent": "Makes all future work submitted to the given `stream` wait for this event .", "question_id": 4710},
{"snippet": "event.wait(stream=None)", "intent": "Makes all future work submitted to the given `stream` wait for this event .", "question_id": 4711},
{"snippet": "Tensor.narrow_copy(dimension, start, length)", "intent": "Same as Tensor.narrow ( ) except returning a copy rather than shared storage . Calling narrow_copy with dimemsion > self.sparse_dim ( ) will return a copy with the relevant dense `dimension` narrowed , and self.shape updated accordingly . With arguments `start`, `length`.", "question_id": 4712},
{"snippet": "Tensor.triu()", "intent": "See torch.triu ( )", "question_id": 4713},
{"snippet": "Tensor.triu(k=0)", "intent": "See torch.triu ( ) With arguments `k`.", "question_id": 4714},
{"snippet": "Tensor.is_complex()", "intent": "Returns True if the data type of self is a complex data type .", "question_id": 4715},
{"snippet": "torch.nn.quantized.functional.interpolate(input)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4716},
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4717},
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4718},
{"snippet": "torch.nn.quantized.functional.interpolate(input, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 4719},
{"snippet": "torch.nn.quantized.functional.interpolate(input, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 4720},
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 4721},
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 4722},
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 4723},
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 4724},
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 4725},
{"snippet": "torch.nn.Softshrink()", "intent": "Applies the soft shrinkage function elementwise :", "question_id": 4726},
{"snippet": "torch.nn.Softshrink(lambd=0.5)", "intent": "Applies the soft shrinkage function elementwise : With arguments `lambd`.", "question_id": 4727},
{"snippet": "Tensor.fliplr()", "intent": "See torch.fliplr ( )", "question_id": 4728},
{"snippet": "Tensor.logical_and()", "intent": "See torch.logical_and ( )", "question_id": 4729},
{"snippet": "torch.nn.utils.prune.random_unstructured(module, name, amount)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units selected at random .", "question_id": 4730},
{"snippet": "torch.nn.ModuleList()", "intent": "Holds submodules in a list .", "question_id": 4731},
{"snippet": "torch.nn.ModuleList(modules=None)", "intent": "Holds submodules in a list . ModuleList can be indexed like a regular Python list , but `modules` it contains are properly registered , and will be visible by all Module methods .", "question_id": 4732},
{"snippet": "module_list.append(module)", "intent": "Appends a given `module` to the end of the list .", "question_id": 4733},
{"snippet": "module_list.extend(modules)", "intent": "Appends `modules` from a Python iterable to the end of the list .", "question_id": 4734},
{"snippet": "module_list.insert(index, module)", "intent": "Insert a given `module` before a given `index` in the list .", "question_id": 4735},
{"snippet": "torch.fft.ifft2(input, - 1))", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`.", "question_id": 4736},
{"snippet": "torch.fft.ifft2(input, - 1), s=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`.", "question_id": 4737},
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`.", "question_id": 4738},
{"snippet": "torch.fft.ifft2(input, - 1), norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `norm`.", "question_id": 4739},
{"snippet": "torch.fft.ifft2(input, - 1), out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `out`.", "question_id": 4740},
{"snippet": "torch.fft.ifft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 4741},
{"snippet": "torch.fft.ifft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 4742},
{"snippet": "torch.fft.ifft2(input, - 1), s=None, out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 4743},
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 4744},
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 4745},
{"snippet": "Tensor.is_coalesced()", "intent": "Returns True if self is a sparse COO tensor that is coalesced , False otherwise .", "question_id": 4746},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4747},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4748},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4749},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 4750},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4751},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4752},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4753},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4754},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 4755},
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 4756},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 4757},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 4758},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 4759},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 4760},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 4761},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 4762},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4763},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 4764},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 4765},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 4766},
{"snippet": "torch.nn.Sequential(*args)", "intent": "A sequential container . With arguments `*args`.", "question_id": 4767},
{"snippet": "torch.nn.BCEWithLogitsLoss()", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class .", "question_id": 4768},
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc .", "question_id": 4769},
{"snippet": "torch.nn.BCEWithLogitsLoss(size_average=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . With arguments `size_average`.", "question_id": 4770},
{"snippet": "torch.nn.BCEWithLogitsLoss(reduce=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . With arguments `reduce`.", "question_id": 4771},
{"snippet": "torch.nn.BCEWithLogitsLoss(reduction='mean')", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 4772},
{"snippet": "torch.nn.BCEWithLogitsLoss(pos_weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . For example , if a dataset contains 100 positive and 300 negative examples of a single class , then `pos_weight` for the class should be equal to 300100=3\\frac { 300 } { 100 } =3100300\u200b=3 .", "question_id": 4773},
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, size_average=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . With arguments `size_average`.", "question_id": 4774},
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, reduce=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . With arguments `reduce`.", "question_id": 4775},
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, reduction='mean')", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 4776},
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, pos_weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . For example , if a dataset contains 100 positive and 300 negative examples of a single class , then `pos_weight` for the class should be equal to 300100=3\\frac { 300 } { 100 } =3100300\u200b=3 .", "question_id": 4777},
{"snippet": "torch.nn.functional.mish(input)", "intent": "Applies the Mish function , element-wise . With arguments `input`.", "question_id": 4778},
{"snippet": "torch.nn.functional.mish(input, inplace=False)", "intent": "Applies the Mish function , element-wise . With arguments `input`, `inplace`.", "question_id": 4779},
{"snippet": "Tensor.kthvalue(k)", "intent": "See torch.kthvalue ( ) With arguments `k`.", "question_id": 4780},
{"snippet": "Tensor.kthvalue(k, dim=None)", "intent": "See torch.kthvalue ( ) With arguments `k`, `dim`.", "question_id": 4781},
{"snippet": "Tensor.kthvalue(k, keepdim=False)", "intent": "See torch.kthvalue ( ) With arguments `k`, `keepdim`.", "question_id": 4782},
{"snippet": "Tensor.kthvalue(k, dim=None, keepdim=False)", "intent": "See torch.kthvalue ( ) With arguments `k`, `dim`, `keepdim`.", "question_id": 4783},
{"snippet": "torch.nn.modules.module.register_module_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook common to all modules . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 4784},
{"snippet": "torch.dot(input, other)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`.", "question_id": 4785},
{"snippet": "torch.dot(input, other, out=None)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`, `out`.", "question_id": 4786},
{"snippet": "Tensor.addmm_(mat1, mat2)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`.", "question_id": 4787},
{"snippet": "Tensor.addmm_(mat1, mat2, beta=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 4788},
{"snippet": "Tensor.addmm_(mat1, mat2, alpha=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 4789},
{"snippet": "Tensor.addmm_(mat1, mat2, beta=1, alpha=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 4790},
{"snippet": "torch.promote_types(type1, type2)", "intent": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either `type1` or `type2` .", "question_id": 4791},
{"snippet": "Tensor.bitwise_not()", "intent": "See torch.bitwise_not ( )", "question_id": 4792},
{"snippet": "Tensor.frexp(input) -> (Tensor mantissa, Tensor exponent)", "intent": "See torch.frexp ( ) With arguments `input) -> (Tensor mantissa`, `Tensor exponent`.", "question_id": 4793},
{"snippet": "torch.linalg.svd(A)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 4794},
{"snippet": "torch.linalg.svd(A, full_matrices=True)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `full_matrices` chooses between the full ( default ) and reduced SVD .", "question_id": 4795},
{"snippet": "torch.linalg.svd(A, out=None)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 4796},
{"snippet": "torch.linalg.svd(A, full_matrices=True, out=None)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `full_matrices` chooses between the full ( default ) and reduced SVD . With arguments `out`.", "question_id": 4797},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`.", "question_id": 4798},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`.", "question_id": 4799},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`.", "question_id": 4800},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `device`.", "question_id": 4801},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `dtype`.", "question_id": 4802},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `head_bias`.", "question_id": 4803},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `device`.", "question_id": 4804},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `dtype`.", "question_id": 4805},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`, `device`.", "question_id": 4806},
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`, `dtype`.", "question_id": 4807},
{"snippet": "adaptive_log_softmax_with_loss.log_prob(input)", "intent": "Computes log probabilities for all n_classes\\texttt { n\\_classes } n_classes With arguments `input`.", "question_id": 4808},
{"snippet": "adaptive_log_softmax_with_loss.predict(input)", "intent": "This is equivalent to self.log_pob ( `input` ) .argmax ( dim=1 ) , but is more efficient in some cases .", "question_id": 4809},
{"snippet": "torch.trace(input)", "intent": "Returns the sum of the elements of the diagonal of the `input` 2-D matrix .", "question_id": 4810},
{"snippet": "torch.addr(input, vec1, vec2)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` .", "question_id": 4811},
{"snippet": "torch.addr(input, vec1, vec2, beta=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 4812},
{"snippet": "torch.addr(input, vec1, vec2, alpha=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 4813},
{"snippet": "torch.addr(input, vec1, vec2, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 4814},
{"snippet": "torch.addr(input, vec1, vec2, beta=1, alpha=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 4815},
{"snippet": "torch.addr(input, vec1, vec2, beta=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 4816},
{"snippet": "torch.addr(input, vec1, vec2, alpha=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 4817},
{"snippet": "torch.addr(input, vec1, vec2, beta=1, alpha=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 4818},
{"snippet": "torch.view_as_real(input)", "intent": "Returns a view of `input` as a real tensor .", "question_id": 4819},
{"snippet": "torch.nn.functional.softsign(input)", "intent": "Applies element-wise , the function SoftSign ( x ) =x1+\u2223x\u2223\\text { SoftSign } ( x ) = \\frac { x } { 1 + |x| } SoftSign ( x ) =1+\u2223x\u2223x\u200b With arguments `input`.", "question_id": 4820},
{"snippet": "torch.nn.Hardswish()", "intent": "Applies the hardswish function , element-wise , as described in the paper :", "question_id": 4821},
{"snippet": "torch.nn.Hardswish(inplace=False)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `inplace`.", "question_id": 4822},
{"snippet": "Tensor.is_quantized", "intent": "Is True if the Tensor is quantized, False otherwise.", "question_id": 4823},
{"snippet": "torch.autograd.gradcheck(func, inputs)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 4824},
{"snippet": "torch.autograd.gradcheck(func, inputs, eps=1e-06)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `eps`.", "question_id": 4825},
{"snippet": "torch.autograd.gradcheck(func, inputs, atol=1e-05)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `atol`.", "question_id": 4826},
{"snippet": "torch.autograd.gradcheck(func, inputs, rtol=0.001)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `rtol`.", "question_id": 4827},
{"snippet": "torch.autograd.gradcheck(func, inputs, raise_exception=True)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `raise_exception`.", "question_id": 4828},
{"snippet": "torch.autograd.gradcheck(func, inputs, check_sparse_nnz=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_sparse_nnz`.", "question_id": 4829},
{"snippet": "torch.autograd.gradcheck(func, inputs, nondet_tol=0.0)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `nondet_tol`.", "question_id": 4830},
{"snippet": "torch.autograd.gradcheck(func, inputs, check_undefined_grad=True)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_undefined_grad`.", "question_id": 4831},
{"snippet": "torch.autograd.gradcheck(func, inputs, check_grad_dtypes=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_grad_dtypes`.", "question_id": 4832},
{"snippet": "torch.autograd.gradcheck(func, inputs, check_batched_grad=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_batched_grad`.", "question_id": 4833},
{"snippet": "torch.nn.NLLLoss()", "intent": "The negative log likelihood loss .", "question_id": 4834},
{"snippet": "torch.nn.NLLLoss(weight=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes .", "question_id": 4835},
{"snippet": "torch.nn.NLLLoss(size_average=None)", "intent": "The negative log likelihood loss . With arguments `size_average`.", "question_id": 4836},
{"snippet": "torch.nn.NLLLoss(ignore_index=- 100)", "intent": "The negative log likelihood loss . The target that this loss expects should be a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] where C = number of classes ; if `ignore_index` is specified , this loss also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 4837},
{"snippet": "torch.nn.NLLLoss(reduce=None)", "intent": "The negative log likelihood loss . With arguments `reduce`.", "question_id": 4838},
{"snippet": "torch.nn.NLLLoss(reduction='mean')", "intent": "The negative log likelihood loss . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 4839},
{"snippet": "torch.nn.NLLLoss(weight=None, size_average=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `size_average`.", "question_id": 4840},
{"snippet": "torch.nn.NLLLoss(weight=None, ignore_index=- 100)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . The target that this loss expects should be a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] where C = number of classes ; if `ignore_index` is specified , this loss also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 4841},
{"snippet": "torch.nn.NLLLoss(weight=None, reduce=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduce`.", "question_id": 4842},
{"snippet": "torch.nn.NLLLoss(weight=None, reduction='mean')", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 4843},
{"snippet": "torch.greater_equal(input, other)", "intent": "Alias for torch.ge ( ) . With arguments `input`, `other`.", "question_id": 4844},
{"snippet": "torch.greater_equal(input, other, out=None)", "intent": "Alias for torch.ge ( ) . With arguments `input`, `other`, `out`.", "question_id": 4845},
{"snippet": "torch.gradient(input)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`.", "question_id": 4846},
{"snippet": "torch.gradient(input, spacing=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`.", "question_id": 4847},
{"snippet": "torch.gradient(input, dim=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `dim`.", "question_id": 4848},
{"snippet": "torch.gradient(input, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `edge_order`.", "question_id": 4849},
{"snippet": "torch.gradient(input, spacing=None, dim=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `dim`.", "question_id": 4850},
{"snippet": "torch.gradient(input, spacing=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `edge_order`.", "question_id": 4851},
{"snippet": "torch.gradient(input, dim=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `dim`, `edge_order`.", "question_id": 4852},
{"snippet": "torch.gradient(input, spacing=None, dim=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `dim`, `edge_order`.", "question_id": 4853},
{"snippet": "torch.cuda.set_rng_state_all(new_states)", "intent": "Sets the random number generator state of all devices . With arguments `new_states`.", "question_id": 4854},
{"snippet": "torch.nn.BatchNorm2d(num_features)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 4855},
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 4856},
{"snippet": "torch.nn.BatchNorm2d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 4857},
{"snippet": "torch.nn.BatchNorm2d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 4858},
{"snippet": "torch.nn.BatchNorm2d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 4859},
{"snippet": "torch.nn.BatchNorm2d(num_features, device=None)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 4860},
{"snippet": "torch.nn.BatchNorm2d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 4861},
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 4862},
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 4863},
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 4864},
{"snippet": "torch.cuda.set_rng_state(new_state)", "intent": "Sets the random number generator state of the specified GPU . With arguments `new_state`.", "question_id": 4865},
{"snippet": "torch.cuda.set_rng_state(new_state, device='cuda')", "intent": "Sets the random number generator state of the specified GPU . With arguments `new_state`, `device`.", "question_id": 4866},
{"snippet": "torch.fft.rfftn(input)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` .", "question_id": 4867},
{"snippet": "torch.fft.rfftn(input, s=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`.", "question_id": 4868},
{"snippet": "torch.fft.rfftn(input, dim=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`.", "question_id": 4869},
{"snippet": "torch.fft.rfftn(input, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `norm`.", "question_id": 4870},
{"snippet": "torch.fft.rfftn(input, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `out`.", "question_id": 4871},
{"snippet": "torch.fft.rfftn(input, s=None, dim=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `dim`.", "question_id": 4872},
{"snippet": "torch.fft.rfftn(input, s=None, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `norm`.", "question_id": 4873},
{"snippet": "torch.fft.rfftn(input, s=None, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `out`.", "question_id": 4874},
{"snippet": "torch.fft.rfftn(input, dim=None, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`, `norm`.", "question_id": 4875},
{"snippet": "torch.fft.rfftn(input, dim=None, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`, `out`.", "question_id": 4876},
{"snippet": "torch.linalg.eigvalsh(A)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 4877},
{"snippet": "torch.linalg.eigvalsh(A, UPLO='L')", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`.", "question_id": 4878},
{"snippet": "torch.linalg.eigvalsh(A, out=None)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 4879},
{"snippet": "torch.linalg.eigvalsh(A, UPLO='L', out=None)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`, `out`.", "question_id": 4880},
{"snippet": "Tensor.svd()", "intent": "See torch.svd ( )", "question_id": 4881},
{"snippet": "Tensor.svd(some=True)", "intent": "See torch.svd ( ) With arguments `some`.", "question_id": 4882},
{"snippet": "Tensor.svd(compute_uv=True)", "intent": "See torch.svd ( ) With arguments `compute_uv`.", "question_id": 4883},
{"snippet": "Tensor.svd(some=True, compute_uv=True)", "intent": "See torch.svd ( ) With arguments `some`, `compute_uv`.", "question_id": 4884},
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 4885},
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 4886},
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 4887},
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 4888},
{"snippet": "Tensor.masked_scatter(mask, tensor)", "intent": "Out-of-place version of torch.Tensor.masked_scatter_ ( ) With arguments `mask`, `tensor`.", "question_id": 4889},
{"snippet": "torch.angle(input)", "intent": "Computes the element-wise angle ( in radians ) of the given `input` tensor .", "question_id": 4890},
{"snippet": "torch.angle(input, out=None)", "intent": "Computes the element-wise angle ( in radians ) of the given `input` tensor . With arguments `out`.", "question_id": 4891},
{"snippet": "torch.autograd.gradgradcheck(func, inputs)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 4892},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, grad_outputs=None)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 4893},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, eps=1e-06)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `eps`.", "question_id": 4894},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, atol=1e-05)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `atol`.", "question_id": 4895},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, rtol=0.001)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `rtol`.", "question_id": 4896},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, gen_non_contig_grad_outputs=False)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `gen_non_contig_grad_outputs`.", "question_id": 4897},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, raise_exception=True)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `raise_exception`.", "question_id": 4898},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, nondet_tol=0.0)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `nondet_tol`.", "question_id": 4899},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, check_undefined_grad=True)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_undefined_grad`.", "question_id": 4900},
{"snippet": "torch.autograd.gradgradcheck(func, inputs, check_grad_dtypes=False)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_grad_dtypes`.", "question_id": 4901},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 4902},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 4903},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 4904},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 4905},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 4906},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 4907},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 4908},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 4909},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 4910},
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 4911},
{"snippet": "lazy_conv1d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv1d", "question_id": 4912},
{"snippet": "Tensor.square_()", "intent": "In-place version of square ( )", "question_id": 4913},
{"snippet": "torch.arcsin(input)", "intent": "Alias for torch.asin ( ) . With arguments `input`.", "question_id": 4914},
{"snippet": "torch.arcsin(input, out=None)", "intent": "Alias for torch.asin ( ) . With arguments `input`, `out`.", "question_id": 4915},
{"snippet": "torch.nanmedian(input)", "intent": "Returns the median of the values in `input` , ignoring NaN values .", "question_id": 4916},
{"snippet": "torch.isinf(input)", "intent": "Tests if each element of `input` is infinite ( positive or negative infinity ) or not .", "question_id": 4917},
{"snippet": "torch.cumprod(input, dim)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` .", "question_id": 4918},
{"snippet": "torch.cumprod(input, dim, dtype=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `dtype`.", "question_id": 4919},
{"snippet": "torch.cumprod(input, dim, out=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 4920},
{"snippet": "torch.cumprod(input, dim, dtype=None, out=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `dtype`, `out`.", "question_id": 4921},
{"snippet": "torch.not_equal(input, other)", "intent": "Alias for torch.ne ( ) . With arguments `input`, `other`.", "question_id": 4922},
{"snippet": "torch.not_equal(input, other, out=None)", "intent": "Alias for torch.ne ( ) . With arguments `input`, `other`, `out`.", "question_id": 4923},
{"snippet": "torch.cuda.manual_seed_all(seed)", "intent": "Sets the `seed` for generating random numbers on all GPUs .", "question_id": 4924},
{"snippet": "torch.fix(input)", "intent": "Alias for torch.trunc ( ) With arguments `input`.", "question_id": 4925},
{"snippet": "torch.fix(input, out=None)", "intent": "Alias for torch.trunc ( ) With arguments `input`, `out`.", "question_id": 4926},
{"snippet": "Tensor.roll(shifts, dims)", "intent": "See torch.roll ( ) With arguments `shifts`, `dims`.", "question_id": 4927},
{"snippet": "torch.fft.ifftn(input)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` .", "question_id": 4928},
{"snippet": "torch.fft.ifftn(input, s=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`.", "question_id": 4929},
{"snippet": "torch.fft.ifftn(input, dim=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 4930},
{"snippet": "torch.fft.ifftn(input, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 4931},
{"snippet": "torch.fft.ifftn(input, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `out`.", "question_id": 4932},
{"snippet": "torch.fft.ifftn(input, s=None, dim=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `dim`.", "question_id": 4933},
{"snippet": "torch.fft.ifftn(input, s=None, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `norm`.", "question_id": 4934},
{"snippet": "torch.fft.ifftn(input, s=None, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `out`.", "question_id": 4935},
{"snippet": "torch.fft.ifftn(input, dim=None, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 4936},
{"snippet": "torch.fft.ifftn(input, dim=None, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 4937},
{"snippet": "torch.dsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with three or more dimensions , into multiple tensors depthwise according to `indices_or_sections` .", "question_id": 4938},
{"snippet": "torch.cuda.memory_summary()", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` .", "question_id": 4939},
{"snippet": "torch.cuda.memory_summary(device=None)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` .", "question_id": 4940},
{"snippet": "torch.cuda.memory_summary(abbreviated=False)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` . With arguments `abbreviated`.", "question_id": 4941},
{"snippet": "torch.cuda.memory_summary(device=None, abbreviated=False)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` . With arguments `abbreviated`.", "question_id": 4942},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 4943},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`.", "question_id": 4944},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`.", "question_id": 4945},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 4946},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `_random_samples`.", "question_id": 4947},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, output_ratio=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `output_ratio`.", "question_id": 4948},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `return_indices`.", "question_id": 4949},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `_random_samples`.", "question_id": 4950},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `return_indices`.", "question_id": 4951},
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `_random_samples`.", "question_id": 4952},
{"snippet": "torch.sparse_coo_tensor(indices, values)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` .", "question_id": 4953},
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`.", "question_id": 4954},
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`.", "question_id": 4955},
{"snippet": "torch.sparse_coo_tensor(indices, values, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `device`.", "question_id": 4956},
{"snippet": "torch.sparse_coo_tensor(indices, values, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `requires_grad`.", "question_id": 4957},
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, dtype=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `dtype`.", "question_id": 4958},
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `device`.", "question_id": 4959},
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `requires_grad`.", "question_id": 4960},
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`, `device`.", "question_id": 4961},
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`, `requires_grad`.", "question_id": 4962},
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`.", "question_id": 4963},
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `norm_type`.", "question_id": 4964},
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, error_if_nonfinite=False)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `error_if_nonfinite`.", "question_id": 4965},
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `norm_type`, `error_if_nonfinite`.", "question_id": 4966},
{"snippet": "Tensor.minimum(other)", "intent": "See torch.minimum ( ) With arguments `other`.", "question_id": 4967},
{"snippet": "torch.set_default_dtype(d)", "intent": "Sets the default floating point dtype to d. This dtype is : With arguments `d`.", "question_id": 4968},
{"snippet": "Tensor.arccosh()", "intent": "acosh ( ) - > Tensor", "question_id": 4969},
{"snippet": "torch.nn.utils.parametrize.cached()", "intent": "Context manager that enables the caching system within parametrizations registered with register_parametrization ( ) .", "question_id": 4970},
{"snippet": "Tensor.triangular_solve(A)", "intent": "See torch.triangular_solve ( ) With arguments `A`.", "question_id": 4971},
{"snippet": "Tensor.triangular_solve(A, upper=True)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`.", "question_id": 4972},
{"snippet": "Tensor.triangular_solve(A, transpose=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `transpose`.", "question_id": 4973},
{"snippet": "Tensor.triangular_solve(A, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `unitriangular`.", "question_id": 4974},
{"snippet": "Tensor.triangular_solve(A, upper=True, transpose=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `transpose`.", "question_id": 4975},
{"snippet": "Tensor.triangular_solve(A, upper=True, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `unitriangular`.", "question_id": 4976},
{"snippet": "Tensor.triangular_solve(A, transpose=False, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `transpose`, `unitriangular`.", "question_id": 4977},
{"snippet": "Tensor.triangular_solve(A, upper=True, transpose=False, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `transpose`, `unitriangular`.", "question_id": 4978},
{"snippet": "torch.nn.ZeroPad2d(padding)", "intent": "Pads the input tensor boundaries with zero . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 4979},
{"snippet": "Tensor.new_ones(size)", "intent": "Returns a Tensor of `size` size filled with 1 .", "question_id": 4980},
{"snippet": "Tensor.new_ones(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`.", "question_id": 4981},
{"snippet": "Tensor.new_ones(size, device=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `device`.", "question_id": 4982},
{"snippet": "Tensor.new_ones(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `requires_grad`.", "question_id": 4983},
{"snippet": "Tensor.new_ones(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `device`.", "question_id": 4984},
{"snippet": "Tensor.new_ones(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `requires_grad`.", "question_id": 4985},
{"snippet": "Tensor.new_ones(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `device`, `requires_grad`.", "question_id": 4986},
{"snippet": "Tensor.new_ones(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 4987},
{"snippet": "Tensor.abs_()", "intent": "In-place version of abs ( )", "question_id": 4988},
{"snippet": "torch.cuda.get_device_name()", "intent": "Gets the name of a `device` .", "question_id": 4989},
{"snippet": "torch.cuda.get_device_name(device=None)", "intent": "Gets the name of a `device` .", "question_id": 4990},
{"snippet": "torch.symeig(input)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) .", "question_id": 4991},
{"snippet": "torch.symeig(input, eigenvectors=False)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) .", "question_id": 4992},
{"snippet": "torch.symeig(input, upper=True)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default .", "question_id": 4993},
{"snippet": "torch.symeig(input, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . With arguments `out`.", "question_id": 4994},
{"snippet": "torch.symeig(input, eigenvectors=False, upper=True)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default .", "question_id": 4995},
{"snippet": "torch.symeig(input, eigenvectors=False, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . With arguments `out`.", "question_id": 4996},
{"snippet": "torch.symeig(input, upper=True, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default . With arguments `out`.", "question_id": 4997},
{"snippet": "torch.symeig(input, eigenvectors=False, upper=True, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default . With arguments `out`.", "question_id": 4998},
{"snippet": "torch.nn.AdaptiveAvgPool3d(output_size)", "intent": "Applies a 3D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 4999},
{"snippet": "torch.slogdet(input)", "intent": "Alias for torch.linalg.slogdet ( ) With arguments `input`.", "question_id": 5000},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 5001},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 5002},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 5003},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 5004},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 5005},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 5006},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `device`.", "question_id": 5007},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `dtype`.", "question_id": 5008},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`, `device`.", "question_id": 5009},
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`, `dtype`.", "question_id": 5010},
{"snippet": "Tensor.eig()", "intent": "See torch.eig ( )", "question_id": 5011},
{"snippet": "Tensor.eig(eigenvectors=False)", "intent": "See torch.eig ( ) With arguments `eigenvectors`.", "question_id": 5012},
{"snippet": "torch.isposinf(input)", "intent": "Tests if each element of `input` is positive infinity or not .", "question_id": 5013},
{"snippet": "torch.isposinf(input, out=None)", "intent": "Tests if each element of `input` is positive infinity or not . With arguments `out`.", "question_id": 5014},
{"snippet": "torch.argsort(input)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`.", "question_id": 5015},
{"snippet": "torch.argsort(input, dim=- 1)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `dim`.", "question_id": 5016},
{"snippet": "torch.argsort(input, descending=False)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `descending`.", "question_id": 5017},
{"snippet": "torch.argsort(input, dim=- 1, descending=False)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `dim`, `descending`.", "question_id": 5018},
{"snippet": "Tensor.max()", "intent": "See torch.max ( )", "question_id": 5019},
{"snippet": "Tensor.max(dim=None)", "intent": "See torch.max ( ) With arguments `dim`.", "question_id": 5020},
{"snippet": "Tensor.max(keepdim=False)", "intent": "See torch.max ( ) With arguments `keepdim`.", "question_id": 5021},
{"snippet": "Tensor.max(dim=None, keepdim=False)", "intent": "See torch.max ( ) With arguments `dim`, `keepdim`.", "question_id": 5022},
{"snippet": "Optimizer.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 5023},
{"snippet": "Tensor.unique_consecutive()", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements .", "question_id": 5024},
{"snippet": "Tensor.unique_consecutive(return_inverse=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`.", "question_id": 5025},
{"snippet": "Tensor.unique_consecutive(return_counts=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_counts`.", "question_id": 5026},
{"snippet": "Tensor.unique_consecutive(dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `dim`.", "question_id": 5027},
{"snippet": "Tensor.unique_consecutive(return_inverse=False, return_counts=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `return_counts`.", "question_id": 5028},
{"snippet": "Tensor.unique_consecutive(return_inverse=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `dim`.", "question_id": 5029},
{"snippet": "Tensor.unique_consecutive(return_counts=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_counts`, `dim`.", "question_id": 5030},
{"snippet": "Tensor.unique_consecutive(return_inverse=False, return_counts=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `return_counts`, `dim`.", "question_id": 5031},
{"snippet": "torch.cuda.comm.scatter(tensor)", "intent": "Scatters `tensor` across multiple GPUs .", "question_id": 5032},
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`.", "question_id": 5033},
{"snippet": "torch.cuda.comm.scatter(tensor, chunk_sizes=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `chunk_sizes`.", "question_id": 5034},
{"snippet": "torch.cuda.comm.scatter(tensor, dim=0)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `dim`.", "question_id": 5035},
{"snippet": "torch.cuda.comm.scatter(tensor, streams=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `streams`.", "question_id": 5036},
{"snippet": "torch.cuda.comm.scatter(tensor, out=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `out`.", "question_id": 5037},
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `chunk_sizes`.", "question_id": 5038},
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, dim=0)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `dim`.", "question_id": 5039},
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, streams=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `streams`.", "question_id": 5040},
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, out=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `out`.", "question_id": 5041},
{"snippet": "torch.masked_select(input, mask)", "intent": "Returns a new 1-D tensor which indexes the `input` tensor according to the boolean `mask` mask which is a BoolTensor .", "question_id": 5042},
{"snippet": "torch.masked_select(input, mask, out=None)", "intent": "Returns a new 1-D tensor which indexes the `input` tensor according to the boolean `mask` mask which is a BoolTensor . With arguments `out`.", "question_id": 5043},
{"snippet": "torch.pow(input, exponent)", "intent": "Takes the power of each element in `input` with `exponent` and returns a tensor with the result .", "question_id": 5044},
{"snippet": "torch.pow(input, exponent, out=None)", "intent": "Takes the power of each element in `input` with `exponent` and returns a tensor with the result . The returned tensor `out` is of the same shape as exponent", "question_id": 5045},
{"snippet": "Tensor.lgamma()", "intent": "See torch.lgamma ( )", "question_id": 5046},
{"snippet": "torch.nn.functional.log_softmax(input)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`.", "question_id": 5047},
{"snippet": "torch.nn.functional.log_softmax(input, dim=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`.", "question_id": 5048},
{"snippet": "torch.nn.functional.log_softmax(input, _stacklevel=3)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `_stacklevel`.", "question_id": 5049},
{"snippet": "torch.nn.functional.log_softmax(input, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dtype`.", "question_id": 5050},
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `_stacklevel`.", "question_id": 5051},
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `dtype`.", "question_id": 5052},
{"snippet": "torch.nn.functional.log_softmax(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 5053},
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `_stacklevel`, `dtype`.", "question_id": 5054},
{"snippet": "torch.fft.fftshift(input)", "intent": "Reorders n-dimensional FFT data , as provided by fftn ( ) , to have negative frequency terms first . With arguments `input`.", "question_id": 5055},
{"snippet": "torch.fft.fftshift(input, dim=None)", "intent": "Reorders n-dimensional FFT data , as provided by fftn ( ) , to have negative frequency terms first . Specifically , to input.shape [ `dim` ] // 2 in each selected dimension . With arguments `input`.", "question_id": 5056},
{"snippet": "torch.seed()", "intent": "Sets the seed for generating random numbers to a non-deterministic random number .", "question_id": 5057},
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 5058},
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, stride=None)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 5059},
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 5060},
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 5061},
{"snippet": "Tensor.values()", "intent": "Return the values tensor of a sparse COO tensor .", "question_id": 5062},
{"snippet": "torch.hamming_window(window_length)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 5063},
{"snippet": "torch.hamming_window(window_length, periodic=True)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 5064},
{"snippet": "torch.hamming_window(window_length, alpha=0.54)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `alpha`.", "question_id": 5065},
{"snippet": "torch.hamming_window(window_length, beta=0.46)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `beta`.", "question_id": 5066},
{"snippet": "torch.hamming_window(window_length, dtype=None)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 5067},
{"snippet": "torch.hamming_window(window_length, layout=torch.strided)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 5068},
{"snippet": "torch.hamming_window(window_length, device=None)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 5069},
{"snippet": "torch.hamming_window(window_length, requires_grad=False)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 5070},
{"snippet": "torch.hamming_window(window_length, periodic=True, alpha=0.54)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `alpha`.", "question_id": 5071},
{"snippet": "torch.hamming_window(window_length, periodic=True, beta=0.46)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `beta`.", "question_id": 5072},
{"snippet": "torch.float_power(input, exponent)", "intent": "Raises `input` to the power of `exponent` , elementwise , in double precision .", "question_id": 5073},
{"snippet": "torch.float_power(input, exponent, out=None)", "intent": "Raises `input` to the power of `exponent` , elementwise , in double precision . With arguments `out`.", "question_id": 5074},
{"snippet": "torch.linalg.cholesky_ex(A)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 5075},
{"snippet": "torch.linalg.cholesky_ex(A, check_errors=False)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `check_errors`.", "question_id": 5076},
{"snippet": "torch.linalg.cholesky_ex(A, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 5077},
{"snippet": "torch.linalg.cholesky_ex(A, check_errors=False, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `check_errors`, `out`.", "question_id": 5078},
{"snippet": "torch.autograd.functional.vjp(func, inputs)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`.", "question_id": 5079},
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`.", "question_id": 5080},
{"snippet": "torch.autograd.functional.vjp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 5081},
{"snippet": "torch.autograd.functional.vjp(func, inputs, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 5082},
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 5083},
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 5084},
{"snippet": "torch.autograd.functional.vjp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 5085},
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 5086},
{"snippet": "torch.smm(input, mat)", "intent": "Performs a matrix multiplication of the sparse matrix `input` with the dense matrix `mat` .", "question_id": 5087},
{"snippet": "torch.nn.utils.rnn.pack_sequence(sequences)", "intent": "Packs a list of variable length Tensors `sequences` should be a list of Tensors of size L x * , where L is the length of a sequence and * is any number of trailing dimensions , including zero .", "question_id": 5088},
{"snippet": "torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)", "intent": "Packs a list of variable length Tensors `sequences` should be a list of Tensors of size L x * , where L is the length of a sequence and * is any number of trailing dimensions , including zero . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 5089},
{"snippet": "Tensor.greater_equal_(other)", "intent": "In-place version of greater_equal ( ) . With arguments `other`.", "question_id": 5090},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 5091},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 5092},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 5093},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 5094},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 5095},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 5096},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 5097},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 5098},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 5099},
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 5100},
{"snippet": "torch.autograd.functional.vhp(func, inputs)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`.", "question_id": 5101},
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`.", "question_id": 5102},
{"snippet": "torch.autograd.functional.vhp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 5103},
{"snippet": "torch.autograd.functional.vhp(func, inputs, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 5104},
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 5105},
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 5106},
{"snippet": "torch.autograd.functional.vhp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 5107},
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 5108},
{"snippet": "Tensor.clip()", "intent": "Alias for clamp ( ) .", "question_id": 5109},
{"snippet": "Tensor.clip(min=None)", "intent": "Alias for clamp ( ) . With arguments `min`.", "question_id": 5110},
{"snippet": "Tensor.clip(max=None)", "intent": "Alias for clamp ( ) . With arguments `max`.", "question_id": 5111},
{"snippet": "Tensor.clip(min=None, max=None)", "intent": "Alias for clamp ( ) . With arguments `min`, `max`.", "question_id": 5112},
{"snippet": "torch.take(input, index)", "intent": "Returns a new tensor with the elements of `input` at the given indices . With arguments `index`.", "question_id": 5113},
{"snippet": "torch.floor(input)", "intent": "Returns a new tensor with the floor of the elements of `input` , the largest integer less than or equal to each element .", "question_id": 5114},
{"snippet": "torch.floor(input, out=None)", "intent": "Returns a new tensor with the floor of the elements of `input` , the largest integer less than or equal to each element . With arguments `out`.", "question_id": 5115},
{"snippet": "Tensor.igammac_(other)", "intent": "In-place version of igammac ( ) With arguments `other`.", "question_id": 5116},
{"snippet": "torch.nn.parameter.Parameter()", "intent": "A kind of Tensor that is to be considered a module parameter .", "question_id": 5117},
{"snippet": "torch.nn.parameter.Parameter(data=None)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `data`.", "question_id": 5118},
{"snippet": "torch.nn.parameter.Parameter(requires_grad=True)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `requires_grad`.", "question_id": 5119},
{"snippet": "torch.nn.parameter.Parameter(data=None, requires_grad=True)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `data`, `requires_grad`.", "question_id": 5120},
{"snippet": "torch.is_floating_point(input)", "intent": "Returns True if the data type of `input` is a floating point data type i.e. , one of torch.float64 , torch.float32 , torch.float16 , and torch.bfloat16 .", "question_id": 5121},
{"snippet": "Tensor.is_set_to(tensor)", "intent": "Returns True if both tensors are pointing to the exact same memory ( same storage , offset , size and stride ) . With arguments `tensor`.", "question_id": 5122},
{"snippet": "torch.nn.functional.hardsigmoid(input)", "intent": "Applies the element-wise function With arguments `input`.", "question_id": 5123},
{"snippet": "torch.nn.functional.hardsigmoid(input, inplace=False)", "intent": "Applies the element-wise function With arguments `input`, `inplace`.", "question_id": 5124},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 5125},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 5126},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 5127},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 5128},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 5129},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 5130},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 5131},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 5132},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 5133},
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 5134},
{"snippet": "Tensor.index_add(tensor1, dim, index, tensor2)", "intent": "Out-of-place version of torch.Tensor.index_add_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_add_ ( ) . With arguments `dim`, `index`, `tensor2`.", "question_id": 5135},
{"snippet": "torch.reshape(input, shape)", "intent": "Returns a tensor with the same data and number of elements as `input` , but with the specified `shape` .", "question_id": 5136},
{"snippet": "torch.atleast_2d(*tensors)", "intent": "Returns a 2-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 5137},
{"snippet": "Tensor.numpy()", "intent": "Returns self tensor as a NumPy ndarray .", "question_id": 5138},
{"snippet": "Tensor.sub_(other)", "intent": "In-place version of sub ( ) With arguments `other`.", "question_id": 5139},
{"snippet": "Tensor.sub_(other, alpha=1)", "intent": "In-place version of sub ( ) With arguments `other`, `alpha`.", "question_id": 5140},
{"snippet": "Tensor.divide(value)", "intent": "See torch.divide ( ) With arguments `value`.", "question_id": 5141},
{"snippet": "Tensor.divide(value, rounding_mode=None)", "intent": "See torch.divide ( ) With arguments `value`, `rounding_mode`.", "question_id": 5142},
{"snippet": "torch.amin(input, dim)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` .", "question_id": 5143},
{"snippet": "torch.amin(input, dim, keepdim=False)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is True , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 .", "question_id": 5144},
{"snippet": "torch.amin(input, dim, out=None)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . With arguments `out`.", "question_id": 5145},
{"snippet": "torch.amin(input, dim, keepdim=False, out=None)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is True , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 . With arguments `out`.", "question_id": 5146},
{"snippet": "Tensor.inverse()", "intent": "See torch.inverse ( )", "question_id": 5147},
{"snippet": "torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 5148},
{"snippet": "torch.linspace(start, end, steps)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive .", "question_id": 5149},
{"snippet": "torch.linspace(start, end, steps, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`.", "question_id": 5150},
{"snippet": "torch.linspace(start, end, steps, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `dtype`.", "question_id": 5151},
{"snippet": "torch.linspace(start, end, steps, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `layout`.", "question_id": 5152},
{"snippet": "torch.linspace(start, end, steps, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `device`.", "question_id": 5153},
{"snippet": "torch.linspace(start, end, steps, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `requires_grad`.", "question_id": 5154},
{"snippet": "torch.linspace(start, end, steps, out=None, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `dtype`.", "question_id": 5155},
{"snippet": "torch.linspace(start, end, steps, out=None, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `layout`.", "question_id": 5156},
{"snippet": "torch.linspace(start, end, steps, out=None, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `device`.", "question_id": 5157},
{"snippet": "torch.linspace(start, end, steps, out=None, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `requires_grad`.", "question_id": 5158},
{"snippet": "torch.set_default_tensor_type(t)", "intent": "Sets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor ( ) . With arguments `t`.", "question_id": 5159},
{"snippet": "Tensor.i0_()", "intent": "In-place version of i0 ( )", "question_id": 5160},
{"snippet": "Tensor.baddbmm(batch1, batch2)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 5161},
{"snippet": "Tensor.baddbmm(batch1, batch2, beta=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 5162},
{"snippet": "Tensor.baddbmm(batch1, batch2, alpha=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 5163},
{"snippet": "Tensor.baddbmm(batch1, batch2, beta=1, alpha=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 5164},
{"snippet": "torch.index_select(input, dim, index)", "intent": "Returns a new tensor which indexes the `input` tensor along dimension `dim` using the entries in `index` which is a LongTensor .", "question_id": 5165},
{"snippet": "torch.index_select(input, dim, index, out=None)", "intent": "Returns a new tensor which indexes the `input` tensor along dimension `dim` using the entries in `index` which is a LongTensor . With arguments `out`.", "question_id": 5166},
{"snippet": "torch.nn.functional.nll_loss(input, target)", "intent": "The negative log likelihood loss . With arguments `input`, `target`.", "question_id": 5167},
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`.", "question_id": 5168},
{"snippet": "torch.nn.functional.nll_loss(input, target, size_average=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `size_average`.", "question_id": 5169},
{"snippet": "torch.nn.functional.nll_loss(input, target, ignore_index=- 100)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `ignore_index`.", "question_id": 5170},
{"snippet": "torch.nn.functional.nll_loss(input, target, reduce=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `reduce`.", "question_id": 5171},
{"snippet": "torch.nn.functional.nll_loss(input, target, reduction='mean')", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `reduction`.", "question_id": 5172},
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, size_average=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 5173},
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, ignore_index=- 100)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `ignore_index`.", "question_id": 5174},
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, reduce=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `reduce`.", "question_id": 5175},
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, reduction='mean')", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `reduction`.", "question_id": 5176},
{"snippet": "torch.cuda.comm.gather(tensors)", "intent": "Gathers `tensors` from multiple GPU devices .", "question_id": 5177},
{"snippet": "torch.cuda.comm.gather(tensors, dim=0)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`.", "question_id": 5178},
{"snippet": "torch.cuda.comm.gather(tensors, destination=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `destination`.", "question_id": 5179},
{"snippet": "torch.cuda.comm.gather(tensors, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `out`.", "question_id": 5180},
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, destination=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `destination`.", "question_id": 5181},
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `out`.", "question_id": 5182},
{"snippet": "torch.cuda.comm.gather(tensors, destination=None, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `destination`, `out`.", "question_id": 5183},
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, destination=None, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `destination`, `out`.", "question_id": 5184},
{"snippet": "Tensor.double()", "intent": "self.double ( ) is equivalent to self.to ( torch.float64 ) .", "question_id": 5185},
{"snippet": "Tensor.double(memory_format=torch.preserve_format)", "intent": "self.double ( ) is equivalent to self.to ( torch.float64 ) . With arguments `memory_format`.", "question_id": 5186},
{"snippet": "Tensor.geqrf()", "intent": "See torch.geqrf ( )", "question_id": 5187},
{"snippet": "torch.is_warn_always_enabled()", "intent": "Returns True if the global warn_always flag is turned on .", "question_id": 5188},
{"snippet": "torch.nn.intrinsic.ConvBn3d(conv, bn)", "intent": "This is a sequential container which calls the Conv 3d and Batch Norm 3d modules . With arguments `conv`, `bn`.", "question_id": 5189},
{"snippet": "Tensor.logical_and_()", "intent": "In-place version of logical_and ( )", "question_id": 5190},
{"snippet": "Tensor.log()", "intent": "See torch.log ( )", "question_id": 5191},
{"snippet": "torch.nn.MSELoss()", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy .", "question_id": 5192},
{"snippet": "torch.nn.MSELoss(size_average=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `size_average`.", "question_id": 5193},
{"snippet": "torch.nn.MSELoss(reduce=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `reduce`.", "question_id": 5194},
{"snippet": "torch.nn.MSELoss(reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 5195},
{"snippet": "torch.nn.MSELoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `size_average`, `reduce`.", "question_id": 5196},
{"snippet": "torch.nn.MSELoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 5197},
{"snippet": "torch.nn.MSELoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 5198},
{"snippet": "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`, `reduce`.", "question_id": 5199},
{"snippet": "torch.roll(input, shifts)", "intent": "Roll the tensor along the given dimension ( s ) . With arguments `input`, `shifts`.", "question_id": 5200},
{"snippet": "torch.roll(input, shifts, dims=None)", "intent": "Roll the tensor along the given dimension ( s ) . With arguments `input`, `shifts`, `dims`.", "question_id": 5201},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`.", "question_id": 5202},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`.", "question_id": 5203},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, full=False)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `full`.", "question_id": 5204},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, size_average=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `size_average`.", "question_id": 5205},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, eps=1e-08)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `eps`.", "question_id": 5206},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, reduce=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `reduce`.", "question_id": 5207},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, reduction='mean')", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `reduction`.", "question_id": 5208},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `full`.", "question_id": 5209},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, size_average=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `size_average`.", "question_id": 5210},
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, eps=1e-08)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `eps`.", "question_id": 5211},
{"snippet": "torch.nn.AvgPool3d(kernel_size)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as :", "question_id": 5212},
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be :", "question_id": 5213},
{"snippet": "torch.nn.AvgPool3d(kernel_size, padding=0)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on all three sides for padding number of points .", "question_id": 5214},
{"snippet": "torch.nn.AvgPool3d(kernel_size, ceil_mode=False)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 5215},
{"snippet": "torch.nn.AvgPool3d(kernel_size, count_include_pad=True)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `count_include_pad`.", "question_id": 5216},
{"snippet": "torch.nn.AvgPool3d(kernel_size, divisor_override=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `divisor_override`.", "question_id": 5217},
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, padding=0)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : If `padding` is non-zero , then the input is implicitly zero-padded on all three sides for padding number of points .", "question_id": 5218},
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `ceil_mode`.", "question_id": 5219},
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `count_include_pad`.", "question_id": 5220},
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, divisor_override=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `divisor_override`.", "question_id": 5221},
{"snippet": "Tensor.cpu()", "intent": "Returns a copy of this object in CPU memory .", "question_id": 5222},
{"snippet": "Tensor.cpu(memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CPU memory . With arguments `memory_format`.", "question_id": 5223},
{"snippet": "torch.orgqr(input, tau)", "intent": "Alias for torch.linalg.householder_product ( ) . With arguments `input`, `tau`.", "question_id": 5224},
{"snippet": "torch.nn.MaxPool3d(kernel_size)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as :", "question_id": 5225},
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be :", "question_id": 5226},
{"snippet": "torch.nn.MaxPool3d(kernel_size, padding=0)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 5227},
{"snippet": "torch.nn.MaxPool3d(kernel_size, dilation=1)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : `dilation` controls the spacing between the kernel points .", "question_id": 5228},
{"snippet": "torch.nn.MaxPool3d(kernel_size, return_indices=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `return_indices`.", "question_id": 5229},
{"snippet": "torch.nn.MaxPool3d(kernel_size, ceil_mode=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 5230},
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, padding=0)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 5231},
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : `dilation` controls the spacing between the kernel points .", "question_id": 5232},
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `return_indices`.", "question_id": 5233},
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `ceil_mode`.", "question_id": 5234},
{"snippet": "Tensor.arctan_()", "intent": "In-place version of arctan ( )", "question_id": 5235},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`.", "question_id": 5236},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `eps`.", "question_id": 5237},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, momentum=0.1)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `momentum`.", "question_id": 5238},
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05, momentum=0.1)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `eps`, `momentum`.", "question_id": 5239},
{"snippet": "Tensor.le_(other)", "intent": "In-place version of le ( ) . With arguments `other`.", "question_id": 5240},
{"snippet": "Tensor.addbmm(batch1, batch2)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 5241},
{"snippet": "Tensor.addbmm(batch1, batch2, beta=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 5242},
{"snippet": "Tensor.addbmm(batch1, batch2, alpha=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 5243},
{"snippet": "Tensor.addbmm(batch1, batch2, beta=1, alpha=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 5244},
{"snippet": "torch.vsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with two or more dimensions , into multiple tensors vertically according to `indices_or_sections` .", "question_id": 5245},
{"snippet": "torch.nn.utils.remove_spectral_norm(module)", "intent": "Removes the spectral normalization reparameterization from a `module` .", "question_id": 5246},
{"snippet": "torch.nn.utils.remove_spectral_norm(module, name='weight')", "intent": "Removes the spectral normalization reparameterization from a `module` . With arguments `name`.", "question_id": 5247},
{"snippet": "torch.cross(input, other)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` .", "question_id": 5248},
{"snippet": "torch.cross(input, other, dim=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` .", "question_id": 5249},
{"snippet": "torch.cross(input, other, out=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` . With arguments `out`.", "question_id": 5250},
{"snippet": "torch.cross(input, other, dim=None, out=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` . With arguments `out`.", "question_id": 5251},
{"snippet": "torch.nn.functional.celu(input)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`.", "question_id": 5252},
{"snippet": "torch.nn.functional.celu(input, alpha=1.)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `alpha`.", "question_id": 5253},
{"snippet": "torch.nn.functional.celu(input, inplace=False)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `inplace`.", "question_id": 5254},
{"snippet": "torch.nn.functional.celu(input, alpha=1., inplace=False)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `alpha`, `inplace`.", "question_id": 5255},
{"snippet": "torch.autograd.enable_grad", "intent": "Context-manager that enables gradient calculation.", "question_id": 5256},
{"snippet": "Tensor.polygamma_(n)", "intent": "In-place version of polygamma ( ) With arguments `n`.", "question_id": 5257},
{"snippet": "torch.mode(input)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e .", "question_id": 5258},
{"snippet": "torch.mode(input, dim=- 1)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e .", "question_id": 5259},
{"snippet": "torch.mode(input, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 .", "question_id": 5260},
{"snippet": "torch.mode(input, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . With arguments `out`.", "question_id": 5261},
{"snippet": "torch.mode(input, dim=- 1, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 .", "question_id": 5262},
{"snippet": "torch.mode(input, dim=- 1, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . With arguments `out`.", "question_id": 5263},
{"snippet": "torch.mode(input, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 5264},
{"snippet": "torch.mode(input, dim=- 1, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 5265},
{"snippet": "Tensor.addmv(mat, vec)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`.", "question_id": 5266},
{"snippet": "Tensor.addmv(mat, vec, beta=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `beta`.", "question_id": 5267},
{"snippet": "Tensor.addmv(mat, vec, alpha=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `alpha`.", "question_id": 5268},
{"snippet": "Tensor.addmv(mat, vec, beta=1, alpha=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `beta`, `alpha`.", "question_id": 5269},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 5270},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 5271},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 5272},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 5273},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 5274},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 5275},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 5276},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 5277},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 5278},
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 5279},
{"snippet": "torch.nn.CTCLoss()", "intent": "The Connectionist Temporal Classification loss .", "question_id": 5280},
{"snippet": "torch.nn.CTCLoss(blank=0)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`.", "question_id": 5281},
{"snippet": "torch.nn.CTCLoss(reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `reduction`.", "question_id": 5282},
{"snippet": "torch.nn.CTCLoss(zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `zero_infinity`.", "question_id": 5283},
{"snippet": "torch.nn.CTCLoss(blank=0, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `reduction`.", "question_id": 5284},
{"snippet": "torch.nn.CTCLoss(blank=0, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `zero_infinity`.", "question_id": 5285},
{"snippet": "torch.nn.CTCLoss(reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `reduction`, `zero_infinity`.", "question_id": 5286},
{"snippet": "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `reduction`, `zero_infinity`.", "question_id": 5287},
{"snippet": "Tensor.ndimension()", "intent": "Alias for dim ( )", "question_id": 5288},
{"snippet": "torch.nn.quantized.Hardswish(scale, zero_point)", "intent": "This is the quantized version of Hardswish . With arguments `scale`, `zero_point`.", "question_id": 5289},
{"snippet": "torch.nn.Softmax()", "intent": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0,1 ] and sum to 1 .", "question_id": 5290},
{"snippet": "torch.nn.Softmax(dim=None)", "intent": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0,1 ] and sum to 1 . With arguments `dim`.", "question_id": 5291},
{"snippet": "torch.cuda.comm.broadcast_coalesced(tensors, devices)", "intent": "Broadcasts a sequence `tensors` to the specified GPUs . With arguments `devices`.", "question_id": 5292},
{"snippet": "torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)", "intent": "Broadcasts a sequence `tensors` to the specified GPUs . With arguments `devices`, `buffer_size`.", "question_id": 5293},
{"snippet": "torch.nn.utils.parametrize.remove_parametrizations(module, tensor_name)", "intent": "Removes the parametrizations on a tensor in a `module` . With arguments `tensor_name`.", "question_id": 5294},
{"snippet": "torch.nn.utils.parametrize.remove_parametrizations(module, tensor_name, leave_parametrized=True)", "intent": "Removes the parametrizations on a tensor in a `module` . With arguments `tensor_name`, `leave_parametrized`.", "question_id": 5295},
{"snippet": "torch.flatten(input)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor .", "question_id": 5296},
{"snippet": "torch.flatten(input, start_dim=0)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 5297},
{"snippet": "torch.flatten(input, end_dim=- 1)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 5298},
{"snippet": "torch.flatten(input, start_dim=0, end_dim=- 1)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 5299},
{"snippet": "torch.nn.AdaptiveAvgPool2d(output_size)", "intent": "Applies a 2D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 5300},
{"snippet": "Tensor.igammac(other)", "intent": "See torch.igammac ( ) With arguments `other`.", "question_id": 5301},
{"snippet": "Tensor.abs()", "intent": "See torch.abs ( )", "question_id": 5302},
{"snippet": "torch.set_printoptions()", "intent": "Set options for printing .", "question_id": 5303},
{"snippet": "torch.set_printoptions(precision=None)", "intent": "Set options for printing . With arguments `precision`.", "question_id": 5304},
{"snippet": "torch.set_printoptions(threshold=None)", "intent": "Set options for printing . With arguments `threshold`.", "question_id": 5305},
{"snippet": "torch.set_printoptions(edgeitems=None)", "intent": "Set options for printing . With arguments `edgeitems`.", "question_id": 5306},
{"snippet": "torch.set_printoptions(linewidth=None)", "intent": "Set options for printing . With arguments `linewidth`.", "question_id": 5307},
{"snippet": "torch.set_printoptions(profile=None)", "intent": "Set options for printing . With arguments `profile`.", "question_id": 5308},
{"snippet": "torch.set_printoptions(sci_mode=None)", "intent": "Set options for printing . With arguments `sci_mode`.", "question_id": 5309},
{"snippet": "torch.set_printoptions(precision=None, threshold=None)", "intent": "Set options for printing . With arguments `precision`, `threshold`.", "question_id": 5310},
{"snippet": "torch.set_printoptions(precision=None, edgeitems=None)", "intent": "Set options for printing . With arguments `precision`, `edgeitems`.", "question_id": 5311},
{"snippet": "torch.set_printoptions(precision=None, linewidth=None)", "intent": "Set options for printing . With arguments `precision`, `linewidth`.", "question_id": 5312},
{"snippet": "torch.inner(input, other)", "intent": "Computes the dot product for 1D tensors . For higher dimensions , sums the product of elements from `input` and `other` along their last dimension .", "question_id": 5313},
{"snippet": "torch.inner(input, other, out=None)", "intent": "Computes the dot product for 1D tensors . For higher dimensions , sums the product of elements from `input` and `other` along their last dimension . With arguments `out`.", "question_id": 5314},
{"snippet": "Tensor.triu_()", "intent": "In-place version of triu ( )", "question_id": 5315},
{"snippet": "Tensor.triu_(k=0)", "intent": "In-place version of triu ( ) With arguments `k`.", "question_id": 5316},
{"snippet": "torch.nn.functional.leaky_relu(input)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`.", "question_id": 5317},
{"snippet": "torch.nn.functional.leaky_relu(input, negative_slope=0.01)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `negative_slope`.", "question_id": 5318},
{"snippet": "torch.nn.functional.leaky_relu(input, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `inplace`.", "question_id": 5319},
{"snippet": "torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `negative_slope`, `inplace`.", "question_id": 5320},
{"snippet": "torch.empty_like(input)", "intent": "Returns an uninitialized tensor with the same size as `input` .", "question_id": 5321},
{"snippet": "torch.empty_like(input, dtype=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`.", "question_id": 5322},
{"snippet": "torch.empty_like(input, layout=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `layout`.", "question_id": 5323},
{"snippet": "torch.empty_like(input, device=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `device`.", "question_id": 5324},
{"snippet": "torch.empty_like(input, requires_grad=False)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `requires_grad`.", "question_id": 5325},
{"snippet": "torch.empty_like(input, memory_format=torch.preserve_format)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `memory_format`.", "question_id": 5326},
{"snippet": "torch.empty_like(input, dtype=None, layout=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 5327},
{"snippet": "torch.empty_like(input, dtype=None, device=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 5328},
{"snippet": "torch.empty_like(input, dtype=None, requires_grad=False)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 5329},
{"snippet": "torch.empty_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 5330},
{"snippet": "Tensor.subtract(other)", "intent": "See torch.subtract ( ) . With arguments `other`.", "question_id": 5331},
{"snippet": "Tensor.subtract(other, alpha=1)", "intent": "See torch.subtract ( ) . With arguments `other`, `alpha`.", "question_id": 5332},
{"snippet": "Tensor.sparse_dim()", "intent": "Return the number of sparse dimensions in a sparse tensor self .", "question_id": 5333},
{"snippet": "Tensor.geometric_(p)", "intent": "Fills self tensor with elements drawn from the geometric distribution : With arguments `p`.", "question_id": 5334},
{"snippet": "Tensor.geometric_(p, generator=None)", "intent": "Fills self tensor with elements drawn from the geometric distribution : With arguments `p`, `generator`.", "question_id": 5335},
{"snippet": "torch.nn.functional.selu(input)", "intent": "Applies element-wise , SELU ( x ) =scale\u2217 ( max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) ) \\text { SELU } ( x ) = scale * ( \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ) SELU ( x ) =scale\u2217 ( max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) ) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . With arguments `input`.", "question_id": 5336},
{"snippet": "torch.nn.functional.selu(input, inplace=False)", "intent": "Applies element-wise , SELU ( x ) =scale\u2217 ( max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) ) \\text { SELU } ( x ) = scale * ( \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ) SELU ( x ) =scale\u2217 ( max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) ) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . With arguments `input`, `inplace`.", "question_id": 5337},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`.", "question_id": 5338},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`.", "question_id": 5339},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`.", "question_id": 5340},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `reduce`.", "question_id": 5341},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `reduction`.", "question_id": 5342},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `size_average`.", "question_id": 5343},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `reduce`.", "question_id": 5344},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `reduction`.", "question_id": 5345},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`, `reduce`.", "question_id": 5346},
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`, `reduction`.", "question_id": 5347},
{"snippet": "torch.nn.quantized.functional.hardsigmoid(input)", "intent": "This is the quantized version of hardsigmoid ( ) . With arguments `input`.", "question_id": 5348},
{"snippet": "torch.cuda.reset_peak_memory_stats()", "intent": "Resets the \u201c peak \u201d stats tracked by the CUDA memory allocator .", "question_id": 5349},
{"snippet": "torch.cuda.reset_peak_memory_stats(device=None)", "intent": "Resets the \u201c peak \u201d stats tracked by the CUDA memory allocator . With arguments `device`.", "question_id": 5350},
{"snippet": "Tensor.true_divide_(value)", "intent": "In-place version of true_divide_ ( ) With arguments `value`.", "question_id": 5351},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`.", "question_id": 5352},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`.", "question_id": 5353},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `eps`.", "question_id": 5354},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `reduction`.", "question_id": 5355},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `eps`.", "question_id": 5356},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `reduction`.", "question_id": 5357},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `eps`, `reduction`.", "question_id": 5358},
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `eps`, `reduction`.", "question_id": 5359},
{"snippet": "torch.nn.Dropout3d()", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) .", "question_id": 5360},
{"snippet": "torch.nn.Dropout3d(p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 5361},
{"snippet": "torch.nn.Dropout3d(inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . With arguments `inplace`.", "question_id": 5362},
{"snippet": "torch.nn.Dropout3d(p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 5363},
{"snippet": "torch.save(obj, f)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`.", "question_id": 5364},
{"snippet": "torch.save(obj, f, pickle_module=pickle)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`.", "question_id": 5365},
{"snippet": "torch.save(obj, f, pickle_protocol=DEFAULT_PROTOCOL)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_protocol`.", "question_id": 5366},
{"snippet": "torch.save(obj, f, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `_use_new_zipfile_serialization`.", "question_id": 5367},
{"snippet": "torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `pickle_protocol`.", "question_id": 5368},
{"snippet": "torch.save(obj, f, pickle_module=pickle, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `_use_new_zipfile_serialization`.", "question_id": 5369},
{"snippet": "torch.save(obj, f, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_protocol`, `_use_new_zipfile_serialization`.", "question_id": 5370},
{"snippet": "torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `pickle_protocol`, `_use_new_zipfile_serialization`.", "question_id": 5371},
{"snippet": "torch.searchsorted(sorted_sequence, values)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved .", "question_id": 5372},
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out_int32`.", "question_id": 5373},
{"snippet": "torch.searchsorted(sorted_sequence, values, right=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed .", "question_id": 5374},
{"snippet": "torch.searchsorted(sorted_sequence, values, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out`.", "question_id": 5375},
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, right=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out_int32`.", "question_id": 5376},
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out_int32`, `out`.", "question_id": 5377},
{"snippet": "torch.searchsorted(sorted_sequence, values, right=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out`.", "question_id": 5378},
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, right=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out_int32`, `out`.", "question_id": 5379},
{"snippet": "torch.kron(input, other)", "intent": "Computes the Kronecker product , denoted by \u2297\\otimes\u2297 , of `input` and `other` .", "question_id": 5380},
{"snippet": "torch.kron(input, other, out=None)", "intent": "Computes the Kronecker product , denoted by \u2297\\otimes\u2297 , of `input` and `other` . With arguments `out`.", "question_id": 5381},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 5382},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 5383},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`.", "question_id": 5384},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 5385},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 5386},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `elementwise_affine`.", "question_id": 5387},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `device`.", "question_id": 5388},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `dtype`.", "question_id": 5389},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`, `device`.", "question_id": 5390},
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`, `dtype`.", "question_id": 5391},
{"snippet": "Tensor.lgamma_()", "intent": "In-place version of lgamma ( )", "question_id": 5392},
{"snippet": "Tensor.tan()", "intent": "See torch.tan ( )", "question_id": 5393},
{"snippet": "torch.erfc(input)", "intent": "Alias for torch.special.erfc ( ) . With arguments `input`.", "question_id": 5394},
{"snippet": "torch.erfc(input, out=None)", "intent": "Alias for torch.special.erfc ( ) . With arguments `input`, `out`.", "question_id": 5395},
{"snippet": "torch.dist(input, other)", "intent": "Returns the p-norm of ( `input` - `other` )", "question_id": 5396},
{"snippet": "torch.dist(input, other, p=2)", "intent": "Returns the p-norm of ( `input` - `other` ) With arguments `p`.", "question_id": 5397},
{"snippet": "Tensor.put_(index, source)", "intent": "Copies the elements from `source` into the positions specified by `index` .", "question_id": 5398},
{"snippet": "Tensor.put_(index, source, accumulate=False)", "intent": "Copies the elements from `source` into the positions specified by `index` . If `accumulate` is True , the elements in source are added to self .", "question_id": 5399},
{"snippet": "Tensor.narrow(dimension, start, length)", "intent": "See torch.narrow ( ) With arguments `dimension`, `start`, `length`.", "question_id": 5400},
{"snippet": "Tensor.signbit()", "intent": "See torch.signbit ( )", "question_id": 5401},
{"snippet": "torch.nn.PixelShuffle(upscale_factor)", "intent": "Rearranges elements in a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) to a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) , where r is an upscale factor . With arguments `upscale_factor`.", "question_id": 5402},
{"snippet": "torch.t(input)", "intent": "Expects `input` to be < = 2-D tensor and transposes dimensions 0 and 1 .", "question_id": 5403},
{"snippet": "Tensor.expand_as(other)", "intent": "Expand this tensor to the same size as `other` .", "question_id": 5404},
{"snippet": "profile.export_chrome_trace(path)", "intent": "Exports an EventList as a Chrome tracing tools file . With arguments `path`.", "question_id": 5405},
{"snippet": "torch.nn.functional.conv3d(input, weight)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`.", "question_id": 5406},
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`.", "question_id": 5407},
{"snippet": "torch.nn.functional.conv3d(input, weight, stride=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `stride`.", "question_id": 5408},
{"snippet": "torch.nn.functional.conv3d(input, weight, padding=0)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `padding`.", "question_id": 5409},
{"snippet": "torch.nn.functional.conv3d(input, weight, dilation=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 5410},
{"snippet": "torch.nn.functional.conv3d(input, weight, groups=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `groups`.", "question_id": 5411},
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, stride=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 5412},
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, padding=0)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 5413},
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, dilation=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 5414},
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, groups=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 5415},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5416},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5417},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5418},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5419},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5420},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, qscheme=torch.per_tensor_affine)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5421},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5422},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5423},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5424},
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 5425},
{"snippet": "torch.nn.utils.prune.remove(module, name)", "intent": "Removes the pruning reparameterization from a `module` and the pruning method from the forward hook . The pruned parameter named `name` remains permanently pruned , and the parameter named name+'_orig ' is removed from the parameter list .", "question_id": 5426},
{"snippet": "torch.arctan(input)", "intent": "Alias for torch.atan ( ) . With arguments `input`.", "question_id": 5427},
{"snippet": "torch.arctan(input, out=None)", "intent": "Alias for torch.atan ( ) . With arguments `input`, `out`.", "question_id": 5428},
{"snippet": "Tensor.smm(mat)", "intent": "See torch.smm ( ) With arguments `mat`.", "question_id": 5429},
{"snippet": "torch.reciprocal(input)", "intent": "Returns a new tensor with the reciprocal of the elements of `input`", "question_id": 5430},
{"snippet": "torch.reciprocal(input, out=None)", "intent": "Returns a new tensor with the reciprocal of the elements of `input` With arguments `out`.", "question_id": 5431},
{"snippet": "torch.nn.AdaptiveMaxPool3d(output_size)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 5432},
{"snippet": "torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 5433},
{"snippet": "Tensor.not_equal_(other)", "intent": "In-place version of not_equal ( ) . With arguments `other`.", "question_id": 5434},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`.", "question_id": 5435},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`.", "question_id": 5436},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, bias=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `bias`.", "question_id": 5437},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, training=False)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `training`.", "question_id": 5438},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, momentum=0.1)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `momentum`.", "question_id": 5439},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, eps=1e-05)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `eps`.", "question_id": 5440},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `bias`.", "question_id": 5441},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, training=False)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `training`.", "question_id": 5442},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, momentum=0.1)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `momentum`.", "question_id": 5443},
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, eps=1e-05)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `eps`.", "question_id": 5444},
{"snippet": "torch.cuda.is_available()", "intent": "Returns a bool indicating if CUDA is currently available .", "question_id": 5445},
{"snippet": "Tensor.index_add_(dim, index, tensor)", "intent": "Accumulate the elements of attr : `alpha` times `tensor` into the self tensor by adding to the indices in the order given in `index` . For example , if `dim` == 0 , index [ i ] == j , and alpha=-1 , then the ith row of tensor is subtracted from the jth row of self .", "question_id": 5446},
{"snippet": "Tensor.index_add_(dim, index, tensor, alpha=1)", "intent": "Accumulate the elements of attr : `alpha` times `tensor` into the self tensor by adding to the indices in the order given in `index` . For example , if `dim` == 0 , index [ i ] == j , and alpha=-1 , then the ith row of tensor is subtracted from the jth row of self .", "question_id": 5447},
{"snippet": "torch.cuda.get_device_properties(device)", "intent": "Gets the properties of a `device` .", "question_id": 5448},
{"snippet": "Tensor.istft(n_fft)", "intent": "See torch.istft ( ) With arguments `n_fft`.", "question_id": 5449},
{"snippet": "Tensor.istft(n_fft, hop_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `hop_length`.", "question_id": 5450},
{"snippet": "Tensor.istft(n_fft, win_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `win_length`.", "question_id": 5451},
{"snippet": "Tensor.istft(n_fft, window=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `window`.", "question_id": 5452},
{"snippet": "Tensor.istft(n_fft, center=True)", "intent": "See torch.istft ( ) With arguments `n_fft`, `center`.", "question_id": 5453},
{"snippet": "Tensor.istft(n_fft, normalized=False)", "intent": "See torch.istft ( ) With arguments `n_fft`, `normalized`.", "question_id": 5454},
{"snippet": "Tensor.istft(n_fft, onesided=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `onesided`.", "question_id": 5455},
{"snippet": "Tensor.istft(n_fft, length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `length`.", "question_id": 5456},
{"snippet": "Tensor.istft(n_fft, return_complex=False)", "intent": "See torch.istft ( ) With arguments `n_fft`, `return_complex`.", "question_id": 5457},
{"snippet": "Tensor.istft(n_fft, hop_length=None, win_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `hop_length`, `win_length`.", "question_id": 5458},
{"snippet": "torch.nn.utils.parameters_to_vector(parameters)", "intent": "Convert `parameters` to one vector", "question_id": 5459},
{"snippet": "Tensor.deg2rad()", "intent": "See torch.deg2rad ( )", "question_id": 5460},
{"snippet": "Tensor.igamma_(other)", "intent": "In-place version of igamma ( ) With arguments `other`.", "question_id": 5461},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 5462},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 5463},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 5464},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 5465},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 5466},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 5467},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 5468},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 5469},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 5470},
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 5471},
{"snippet": "torch.nn.utils.prune.l1_unstructured(module, name, amount)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units with the lowest L1-norm .", "question_id": 5472},
{"snippet": "torch.nn.utils.prune.l1_unstructured(module, name, amount, importance_scores=None)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units with the lowest L1-norm . With arguments `importance_scores`.", "question_id": 5473},
{"snippet": "Tensor.cumprod_(dim)", "intent": "In-place version of cumprod ( ) With arguments `dim`.", "question_id": 5474},
{"snippet": "Tensor.cumprod_(dim, dtype=None)", "intent": "In-place version of cumprod ( ) With arguments `dim`, `dtype`.", "question_id": 5475},
{"snippet": "Tensor.log1p()", "intent": "See torch.log1p ( )", "question_id": 5476},
{"snippet": "torch.nn.BCELoss()", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output :", "question_id": 5477},
{"snippet": "torch.nn.BCELoss(weight=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`.", "question_id": 5478},
{"snippet": "torch.nn.BCELoss(size_average=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `size_average`.", "question_id": 5479},
{"snippet": "torch.nn.BCELoss(reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `reduce`.", "question_id": 5480},
{"snippet": "torch.nn.BCELoss(reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as :", "question_id": 5481},
{"snippet": "torch.nn.BCELoss(weight=None, size_average=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`, `size_average`.", "question_id": 5482},
{"snippet": "torch.nn.BCELoss(weight=None, reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`, `reduce`.", "question_id": 5483},
{"snippet": "torch.nn.BCELoss(weight=None, reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as : With arguments `weight`.", "question_id": 5484},
{"snippet": "torch.nn.BCELoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `size_average`, `reduce`.", "question_id": 5485},
{"snippet": "torch.nn.BCELoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 5486},
{"snippet": "Tensor.is_signed()", "intent": "Returns True if the data type of self is a signed data type .", "question_id": 5487},
{"snippet": "torch.narrow(input, dim, start, length)", "intent": "Returns a new tensor that is a narrowed version of `input` tensor . The dimension `dim` is input from `start` to start + `length` .", "question_id": 5488},
{"snippet": "torch.quantization.observer.ObserverBase(dtype)", "intent": "Base observer Module . With arguments `dtype`.", "question_id": 5489},
{"snippet": "observer_base.with_args(**kwargs)", "intent": "Wrapper that allows creation of class factories . With arguments `**kwargs`.", "question_id": 5490},
{"snippet": "torch.nn.LocalResponseNorm(size)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`.", "question_id": 5491},
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`.", "question_id": 5492},
{"snippet": "torch.nn.LocalResponseNorm(size, beta=0.75)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`.", "question_id": 5493},
{"snippet": "torch.nn.LocalResponseNorm(size, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `k`.", "question_id": 5494},
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`.", "question_id": 5495},
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `k`.", "question_id": 5496},
{"snippet": "torch.nn.LocalResponseNorm(size, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`, `k`.", "question_id": 5497},
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`, `k`.", "question_id": 5498},
{"snippet": "torch.erfinv(input)", "intent": "Alias for torch.special.erfinv ( ) . With arguments `input`.", "question_id": 5499},
{"snippet": "torch.erfinv(input, out=None)", "intent": "Alias for torch.special.erfinv ( ) . With arguments `input`, `out`.", "question_id": 5500},
{"snippet": "torch.geqrf(input)", "intent": "This is a low-level function for calling LAPACK \u2019 s geqrf directly . Computes a QR decomposition of `input` .", "question_id": 5501},
{"snippet": "torch.geqrf(input, out=None)", "intent": "This is a low-level function for calling LAPACK \u2019 s geqrf directly . Computes a QR decomposition of `input` . With arguments `out`.", "question_id": 5502},
{"snippet": "Tensor.norm()", "intent": "See torch.norm ( )", "question_id": 5503},
{"snippet": "Tensor.norm(p='fro')", "intent": "See torch.norm ( ) With arguments `p`.", "question_id": 5504},
{"snippet": "Tensor.norm(dim=None)", "intent": "See torch.norm ( ) With arguments `dim`.", "question_id": 5505},
{"snippet": "Tensor.norm(keepdim=False)", "intent": "See torch.norm ( ) With arguments `keepdim`.", "question_id": 5506},
{"snippet": "Tensor.norm(dtype=None)", "intent": "See torch.norm ( ) With arguments `dtype`.", "question_id": 5507},
{"snippet": "Tensor.norm(p='fro', dim=None)", "intent": "See torch.norm ( ) With arguments `p`, `dim`.", "question_id": 5508},
{"snippet": "Tensor.norm(p='fro', keepdim=False)", "intent": "See torch.norm ( ) With arguments `p`, `keepdim`.", "question_id": 5509},
{"snippet": "Tensor.norm(p='fro', dtype=None)", "intent": "See torch.norm ( ) With arguments `p`, `dtype`.", "question_id": 5510},
{"snippet": "Tensor.norm(dim=None, keepdim=False)", "intent": "See torch.norm ( ) With arguments `dim`, `keepdim`.", "question_id": 5511},
{"snippet": "Tensor.norm(dim=None, dtype=None)", "intent": "See torch.norm ( ) With arguments `dim`, `dtype`.", "question_id": 5512},
{"snippet": "torch.nn.parameter.UninitializedBuffer()", "intent": "A buffer that is not initialized .", "question_id": 5513},
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False)", "intent": "A buffer that is not initialized . With arguments `requires_grad`.", "question_id": 5514},
{"snippet": "torch.nn.parameter.UninitializedBuffer(device=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor .", "question_id": 5515},
{"snippet": "torch.nn.parameter.UninitializedBuffer(dtype=None)", "intent": "A buffer that is not initialized . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g .", "question_id": 5516},
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . With arguments `requires_grad`.", "question_id": 5517},
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, dtype=None)", "intent": "A buffer that is not initialized . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 5518},
{"snippet": "torch.nn.parameter.UninitializedBuffer(device=None, dtype=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g .", "question_id": 5519},
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None, dtype=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 5520},
{"snippet": "torch.hypot(input, other)", "intent": "Given the legs of a right triangle , return its hypotenuse . The shapes of `input` and `other` must be broadcastable .", "question_id": 5521},
{"snippet": "torch.hypot(input, other, out=None)", "intent": "Given the legs of a right triangle , return its hypotenuse . The shapes of `input` and `other` must be broadcastable . With arguments `out`.", "question_id": 5522},
{"snippet": "Tensor.conj()", "intent": "See torch.conj ( )", "question_id": 5523},
{"snippet": "torch.nn.Threshold(threshold, value)", "intent": "Thresholds each element of the input Tensor . With arguments `threshold`, `value`.", "question_id": 5524},
{"snippet": "torch.nn.Threshold(threshold, value, inplace=False)", "intent": "Thresholds each element of the input Tensor . With arguments `threshold`, `value`, `inplace`.", "question_id": 5525},
{"snippet": "torch.squeeze(input)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed .", "question_id": 5526},
{"snippet": "torch.squeeze(input, dim=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . When `dim` is given , a squeeze operation is done only in the given dimension .", "question_id": 5527},
{"snippet": "torch.squeeze(input, out=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . For example , if input is of shape : ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) ( A \\times 1 \\times B \\times C \\times 1 \\times D ) ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) then the `out` tensor will be of shape : ( A\u00d7B\u00d7C\u00d7D ) ( A \\times B \\times C \\times D ) ( A\u00d7B\u00d7C\u00d7D ) .", "question_id": 5528},
{"snippet": "torch.squeeze(input, dim=None, out=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . When `dim` is given , a squeeze operation is done only in the given dimension . For example , if input is of shape : ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) ( A \\times 1 \\times B \\times C \\times 1 \\times D ) ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) then the `out` tensor will be of shape : ( A\u00d7B\u00d7C\u00d7D ) ( A \\times B \\times C \\times D ) ( A\u00d7B\u00d7C\u00d7D ) .", "question_id": 5529},
{"snippet": "torch.ones(*size)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`.", "question_id": 5530},
{"snippet": "torch.ones(*size, out=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`.", "question_id": 5531},
{"snippet": "torch.ones(*size, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `dtype`.", "question_id": 5532},
{"snippet": "torch.ones(*size, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `layout`.", "question_id": 5533},
{"snippet": "torch.ones(*size, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `device`.", "question_id": 5534},
{"snippet": "torch.ones(*size, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `requires_grad`.", "question_id": 5535},
{"snippet": "torch.ones(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `dtype`.", "question_id": 5536},
{"snippet": "torch.ones(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `layout`.", "question_id": 5537},
{"snippet": "torch.ones(*size, out=None, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `device`.", "question_id": 5538},
{"snippet": "torch.ones(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `requires_grad`.", "question_id": 5539},
{"snippet": "Tensor.map_(tensor, callable)", "intent": "Applies `callable` for each element in self `tensor` and the given tensor and stores the results in self tensor .", "question_id": 5540},
{"snippet": "torch.nn.BatchNorm1d(num_features)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 5541},
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 5542},
{"snippet": "torch.nn.BatchNorm1d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 5543},
{"snippet": "torch.nn.BatchNorm1d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 5544},
{"snippet": "torch.nn.BatchNorm1d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 5545},
{"snippet": "torch.nn.BatchNorm1d(num_features, device=None)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 5546},
{"snippet": "torch.nn.BatchNorm1d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 5547},
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 5548},
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 5549},
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 5550},
{"snippet": "torch.quasirandom.SobolEngine(dimension)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 .", "question_id": 5551},
{"snippet": "torch.quasirandom.SobolEngine(dimension, scramble=False)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `scramble`.", "question_id": 5552},
{"snippet": "torch.quasirandom.SobolEngine(dimension, seed=None)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `seed`.", "question_id": 5553},
{"snippet": "torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `scramble`, `seed`.", "question_id": 5554},
{"snippet": "sobol_engine.draw()", "intent": "Function to draw a sequence of `n` points from a Sobol sequence .", "question_id": 5555},
{"snippet": "sobol_engine.draw(n=1)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence .", "question_id": 5556},
{"snippet": "sobol_engine.draw(out=None)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`.", "question_id": 5557},
{"snippet": "sobol_engine.draw(dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `dtype`.", "question_id": 5558},
{"snippet": "sobol_engine.draw(n=1, out=None)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`.", "question_id": 5559},
{"snippet": "sobol_engine.draw(n=1, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `dtype`.", "question_id": 5560},
{"snippet": "sobol_engine.draw(out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 5561},
{"snippet": "sobol_engine.draw(n=1, out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 5562},
{"snippet": "sobol_engine.draw_base2(m)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence .", "question_id": 5563},
{"snippet": "sobol_engine.draw_base2(m, out=None)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `out`.", "question_id": 5564},
{"snippet": "sobol_engine.draw_base2(m, dtype=torch.float32)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `dtype`.", "question_id": 5565},
{"snippet": "sobol_engine.draw_base2(m, out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 5566},
{"snippet": "sobol_engine.fast_forward(n)", "intent": "Function to fast-forward the state of the SobolEngine by `n` steps .", "question_id": 5567},
{"snippet": "sobol_engine.reset()", "intent": "Function to reset the SobolEngine to base state .", "question_id": 5568},
{"snippet": "torch.cumsum(input, dim)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` .", "question_id": 5569},
{"snippet": "torch.cumsum(input, dim, dtype=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `dtype`.", "question_id": 5570},
{"snippet": "torch.cumsum(input, dim, out=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 5571},
{"snippet": "torch.cumsum(input, dim, dtype=None, out=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `dtype`, `out`.", "question_id": 5572},
{"snippet": "Tensor.cumsum_(dim)", "intent": "In-place version of cumsum ( ) With arguments `dim`.", "question_id": 5573},
{"snippet": "Tensor.cumsum_(dim, dtype=None)", "intent": "In-place version of cumsum ( ) With arguments `dim`, `dtype`.", "question_id": 5574},
{"snippet": "torch.inverse(input)", "intent": "Alias for torch.linalg.inv ( ) With arguments `input`.", "question_id": 5575},
{"snippet": "torch.inverse(input, out=None)", "intent": "Alias for torch.linalg.inv ( ) With arguments `input`, `out`.", "question_id": 5576},
{"snippet": "Tensor.nan_to_num_()", "intent": "In-place version of nan_to_num ( ) .", "question_id": 5577},
{"snippet": "Tensor.nan_to_num_(nan=0.0)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`.", "question_id": 5578},
{"snippet": "Tensor.nan_to_num_(posinf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `posinf`.", "question_id": 5579},
{"snippet": "Tensor.nan_to_num_(neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `neginf`.", "question_id": 5580},
{"snippet": "Tensor.nan_to_num_(nan=0.0, posinf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `posinf`.", "question_id": 5581},
{"snippet": "Tensor.nan_to_num_(nan=0.0, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `neginf`.", "question_id": 5582},
{"snippet": "Tensor.nan_to_num_(posinf=None, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `posinf`, `neginf`.", "question_id": 5583},
{"snippet": "Tensor.nan_to_num_(nan=0.0, posinf=None, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `posinf`, `neginf`.", "question_id": 5584},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 5585},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 5586},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 5587},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 5588},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, mode='mean')", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . This scales the output of the Embedding before performing a weighted reduction as specified by `mode` . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 5589},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, sparse=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 5590},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, _weight=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 5591},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, include_last_offset=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `include_last_offset`.", "question_id": 5592},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, padding_idx=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . For bags of constant length , no per_sample_weights , no indices equal to `padding_idx` , and with 2D inputs , this class With arguments `num_embeddings`, `embedding_dim`.", "question_id": 5593},
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, device=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `device`.", "question_id": 5594},
{"snippet": "embedding_bag.forward(input)", "intent": "Forward pass of EmbeddingBag . With arguments `input`.", "question_id": 5595},
{"snippet": "embedding_bag.forward(input, offsets=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `offsets`.", "question_id": 5596},
{"snippet": "embedding_bag.forward(input, per_sample_weights=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `per_sample_weights`.", "question_id": 5597},
{"snippet": "embedding_bag.forward(input, offsets=None, per_sample_weights=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `offsets`, `per_sample_weights`.", "question_id": 5598},
{"snippet": "embedding_bag.from_pretrained(embeddings)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`.", "question_id": 5599},
{"snippet": "embedding_bag.from_pretrained(embeddings, freeze=True)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`.", "question_id": 5600},
{"snippet": "embedding_bag.from_pretrained(embeddings, max_norm=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `max_norm`.", "question_id": 5601},
{"snippet": "embedding_bag.from_pretrained(embeddings, norm_type=2.0)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `norm_type`.", "question_id": 5602},
{"snippet": "embedding_bag.from_pretrained(embeddings, scale_grad_by_freq=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `scale_grad_by_freq`.", "question_id": 5603},
{"snippet": "embedding_bag.from_pretrained(embeddings, mode='mean')", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `mode`.", "question_id": 5604},
{"snippet": "embedding_bag.from_pretrained(embeddings, sparse=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `sparse`.", "question_id": 5605},
{"snippet": "embedding_bag.from_pretrained(embeddings, include_last_offset=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `include_last_offset`.", "question_id": 5606},
{"snippet": "embedding_bag.from_pretrained(embeddings, padding_idx=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `padding_idx`.", "question_id": 5607},
{"snippet": "embedding_bag.from_pretrained(embeddings, freeze=True, max_norm=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `max_norm`.", "question_id": 5608},
{"snippet": "torch.nn.quantized.functional.linear(input, weight)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`.", "question_id": 5609},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`.", "question_id": 5610},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, scale=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `scale`.", "question_id": 5611},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `zero_point`.", "question_id": 5612},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `scale`.", "question_id": 5613},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `zero_point`.", "question_id": 5614},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, scale=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `scale`, `zero_point`.", "question_id": 5615},
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 5616},
{"snippet": "torch.equal(input, other)", "intent": "True if two tensors have the same size and elements , False otherwise . With arguments `input`, `other`.", "question_id": 5617},
{"snippet": "torch.std_mean(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`.", "question_id": 5618},
{"snippet": "torch.std_mean(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 5619},
{"snippet": "torch.std_mean(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 5620},
{"snippet": "torch.std_mean(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 5621},
{"snippet": "Tensor.addcdiv(tensor1, tensor2)", "intent": "See torch.addcdiv ( ) With arguments `tensor1`, `tensor2`.", "question_id": 5622},
{"snippet": "Tensor.addcdiv(tensor1, tensor2, value=1)", "intent": "See torch.addcdiv ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 5623},
{"snippet": "torch.nn.Flatten()", "intent": "Flattens a contiguous range of dims into a tensor .", "question_id": 5624},
{"snippet": "torch.nn.Flatten(start_dim=1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `start_dim`.", "question_id": 5625},
{"snippet": "torch.nn.Flatten(end_dim=- 1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `end_dim`.", "question_id": 5626},
{"snippet": "torch.nn.Flatten(start_dim=1, end_dim=- 1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `start_dim`, `end_dim`.", "question_id": 5627},
{"snippet": "torch.det(input)", "intent": "Alias for torch.linalg.det ( ) With arguments `input`.", "question_id": 5628},
{"snippet": "torch.nn.functional.adaptive_avg_pool3d(input, output_size)", "intent": "Applies a 3D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 5629},
{"snippet": "torch.gt(input, other)", "intent": "Computes `input` > other\\text { input } > \\text { `other` } input > other element-wise .", "question_id": 5630},
{"snippet": "torch.gt(input, other, out=None)", "intent": "Computes `input` > other\\text { input } > \\text { `other` } input > other element-wise . With arguments `out`.", "question_id": 5631},
{"snippet": "torch.sort(input)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value .", "question_id": 5632},
{"snippet": "torch.sort(input, dim=- 1)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 5633},
{"snippet": "torch.sort(input, descending=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value .", "question_id": 5634},
{"snippet": "torch.sort(input, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 5635},
{"snippet": "torch.sort(input, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . With arguments `out`.", "question_id": 5636},
{"snippet": "torch.sort(input, dim=- 1, descending=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . If `descending` is True then the elements are sorted in descending order by value .", "question_id": 5637},
{"snippet": "torch.sort(input, dim=- 1, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 5638},
{"snippet": "torch.sort(input, dim=- 1, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . With arguments `out`.", "question_id": 5639},
{"snippet": "torch.sort(input, descending=False, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 5640},
{"snippet": "torch.sort(input, descending=False, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value . With arguments `out`.", "question_id": 5641},
{"snippet": "torch.cuda.seed_all()", "intent": "Sets the seed for generating random numbers to a random number on all GPUs .", "question_id": 5642},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`.", "question_id": 5643},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`.", "question_id": 5644},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`.", "question_id": 5645},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `device`.", "question_id": 5646},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `dtype`.", "question_id": 5647},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `momentum`.", "question_id": 5648},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `device`.", "question_id": 5649},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `dtype`.", "question_id": 5650},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`, `device`.", "question_id": 5651},
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`, `dtype`.", "question_id": 5652},
{"snippet": "Tensor.index_put(tensor1, indices, values)", "intent": "Out-place version of index_put_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_put_ ( ) . With arguments `indices`, `values`.", "question_id": 5653},
{"snippet": "Tensor.index_put(tensor1, indices, values, accumulate=False)", "intent": "Out-place version of index_put_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_put_ ( ) . With arguments `indices`, `values`, `accumulate`.", "question_id": 5654},
{"snippet": "torch.nn.ParameterList()", "intent": "Holds `parameters` in a list .", "question_id": 5655},
{"snippet": "torch.nn.ParameterList(parameters=None)", "intent": "Holds `parameters` in a list .", "question_id": 5656},
{"snippet": "parameter_list.append(parameter)", "intent": "Appends a given `parameter` at the end of the list .", "question_id": 5657},
{"snippet": "parameter_list.extend(parameters)", "intent": "Appends `parameters` from a Python iterable to the end of the list .", "question_id": 5658},
{"snippet": "Tensor.swapdims(dim0, dim1)", "intent": "See torch.swapdims ( ) With arguments `dim0`, `dim1`.", "question_id": 5659},
{"snippet": "Tensor.det()", "intent": "See torch.det ( )", "question_id": 5660},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`.", "question_id": 5661},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`.", "question_id": 5662},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, padding=0)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `padding`.", "question_id": 5663},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `ceil_mode`.", "question_id": 5664},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `count_include_pad`.", "question_id": 5665},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `divisor_override`.", "question_id": 5666},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 5667},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 5668},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 5669},
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 5670},
{"snippet": "Tensor.hypot(other)", "intent": "See torch.hypot ( ) With arguments `other`.", "question_id": 5671},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`.", "question_id": 5672},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`.", "question_id": 5673},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, padding=0)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `padding`.", "question_id": 5674},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, dilation=1)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `dilation`.", "question_id": 5675},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 5676},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, return_indices=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 5677},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 5678},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, dilation=1)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `dilation`.", "question_id": 5679},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 5680},
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, return_indices=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `return_indices`.", "question_id": 5681}
