{"snippet": "Tensor.expm1_()", "intent": "In-place version of expm1 ( )", "question_id": 48000}
{"snippet": "torch.nn.quantized.functional.elu(input, scale, zero_point)", "intent": "This is the quantized version of elu ( ) . With arguments `input`, `scale`, `zero_point`.", "question_id": 48001}
{"snippet": "torch.nn.quantized.functional.elu(input, scale, zero_point, alpha=1.0)", "intent": "This is the quantized version of elu ( ) . With arguments `input`, `scale`, `zero_point`, `alpha`.", "question_id": 48002}
{"snippet": "Tensor.select(dim, index)", "intent": "Slices the self tensor along the selected dimension at the given `index` . With arguments `dim`.", "question_id": 48003}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`.", "question_id": 48004}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`.", "question_id": 48005}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dropout=0.1)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dropout`.", "question_id": 48006}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, activation='relu')", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `activation`.", "question_id": 48007}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, layer_norm_eps=1e-05)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `layer_norm_eps`.", "question_id": 48008}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, batch_first=False)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `batch_first`.", "question_id": 48009}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, device=None)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `device`.", "question_id": 48010}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dtype=None)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dtype`.", "question_id": 48011}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1)", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `dropout`.", "question_id": 48012}
{"snippet": "torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, activation='relu')", "intent": "TransformerEncoderLayer is made up of self-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `activation`.", "question_id": 48013}
{"snippet": "transformer_encoder_layer.forward(src)", "intent": "Pass the input through the encoder layer . With arguments `src`.", "question_id": 48014}
{"snippet": "transformer_encoder_layer.forward(src, src_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_mask`.", "question_id": 48015}
{"snippet": "transformer_encoder_layer.forward(src, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_key_padding_mask`.", "question_id": 48016}
{"snippet": "transformer_encoder_layer.forward(src, src_mask=None, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layer . With arguments `src`, `src_mask`, `src_key_padding_mask`.", "question_id": 48017}
{"snippet": "torch.linalg.tensorinv(A)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal .", "question_id": 48018}
{"snippet": "torch.linalg.tensorinv(A, ind=2)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal .", "question_id": 48019}
{"snippet": "torch.linalg.tensorinv(A, out=None)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal . With arguments `out`.", "question_id": 48020}
{"snippet": "torch.linalg.tensorinv(A, ind=2, out=None)", "intent": "Computes the multiplicative inverse of torch.tensordot ( ) . If m is the product of the first `ind` dimensions of `A` and n is the product of the rest of the dimensions , this function expects m and n to be equal . With arguments `out`.", "question_id": 48021}
{"snippet": "Tensor.is_cuda", "intent": "Is True if the Tensor is stored on the GPU, False otherwise.", "question_id": 48022}
{"snippet": "torch.round(input)", "intent": "Returns a new tensor with each of the elements of `input` rounded to the closest integer .", "question_id": 48023}
{"snippet": "torch.round(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` rounded to the closest integer . With arguments `out`.", "question_id": 48024}
{"snippet": "torch.log(input)", "intent": "Returns a new tensor with the natural logarithm of the elements of `input` .", "question_id": 48025}
{"snippet": "torch.log(input, out=None)", "intent": "Returns a new tensor with the natural logarithm of the elements of `input` . With arguments `out`.", "question_id": 48026}
{"snippet": "Tensor.addcmul(tensor1, tensor2)", "intent": "See torch.addcmul ( ) With arguments `tensor1`, `tensor2`.", "question_id": 48027}
{"snippet": "Tensor.addcmul(tensor1, tensor2, value=1)", "intent": "See torch.addcmul ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 48028}
{"snippet": "Tensor.index_fill(tensor1, dim, index, value)", "intent": "Out-of-place version of torch.Tensor.index_fill_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_fill_ ( ) . With arguments `dim`, `index`, `value`.", "question_id": 48029}
{"snippet": "Tensor.int_repr()", "intent": "Given a quantized Tensor , self.int_repr ( ) returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor .", "question_id": 48030}
{"snippet": "Tensor.slogdet()", "intent": "See torch.slogdet ( )", "question_id": 48031}
{"snippet": "torch.nn.functional.one_hot(tensor)", "intent": "Takes LongTensor with index values of shape ( * ) and returns a `tensor` of shape ( * , `num_classes` ) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor , in which case it will be 1 .", "question_id": 48032}
{"snippet": "torch.nn.functional.one_hot(tensor, num_classes=- 1)", "intent": "Takes LongTensor with index values of shape ( * ) and returns a `tensor` of shape ( * , `num_classes` ) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor , in which case it will be 1 .", "question_id": 48033}
{"snippet": "Tensor.to_sparse(sparseDims)", "intent": "Returns a sparse copy of the tensor . With arguments `sparseDims`.", "question_id": 48034}
{"snippet": "torch.nn.functional.hardtanh(input)", "intent": "Applies the HardTanh function element-wise . With arguments `input`.", "question_id": 48035}
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`.", "question_id": 48036}
{"snippet": "torch.nn.functional.hardtanh(input, max_val=1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `max_val`.", "question_id": 48037}
{"snippet": "torch.nn.functional.hardtanh(input, inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `inplace`.", "question_id": 48038}
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., max_val=1.)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `max_val`.", "question_id": 48039}
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `inplace`.", "question_id": 48040}
{"snippet": "torch.nn.functional.hardtanh(input, max_val=1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `max_val`, `inplace`.", "question_id": 48041}
{"snippet": "torch.nn.functional.hardtanh(input, min_val=- 1., max_val=1., inplace=False)", "intent": "Applies the HardTanh function element-wise . With arguments `input`, `min_val`, `max_val`, `inplace`.", "question_id": 48042}
{"snippet": "Tensor.div(value)", "intent": "See torch.div ( ) With arguments `value`.", "question_id": 48043}
{"snippet": "Tensor.div(value, rounding_mode=None)", "intent": "See torch.div ( ) With arguments `value`, `rounding_mode`.", "question_id": 48044}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`.", "question_id": 48045}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`.", "question_id": 48046}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`.", "question_id": 48047}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `reduce`.", "question_id": 48048}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `reduction`.", "question_id": 48049}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `size_average`.", "question_id": 48050}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduce`.", "question_id": 48051}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduction`.", "question_id": 48052}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None, reduce=None)", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduce`.", "question_id": 48053}
{"snippet": "torch.nn.functional.cosine_embedding_loss(input1, input2, target, size_average=None, reduction='mean')", "intent": "See CosineEmbeddingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduction`.", "question_id": 48054}
{"snippet": "Tensor.fill_diagonal_(fill_value)", "intent": "Fill the main diagonal of a tensor that has at least 2-dimensions . With arguments `fill_value`.", "question_id": 48055}
{"snippet": "Tensor.fill_diagonal_(fill_value, wrap=False)", "intent": "Fill the main diagonal of a tensor that has at least 2-dimensions . With arguments `fill_value`, `wrap`.", "question_id": 48056}
{"snippet": "torch.jit.save(m, f)", "intent": "Save an offline version of this module for use in a separate process . With arguments `m`, `f`.", "question_id": 48057}
{"snippet": "torch.jit.save(m, f, _extra_files=None)", "intent": "Save an offline version of this module for use in a separate process . With arguments `m`, `f`, `_extra_files`.", "question_id": 48058}
{"snippet": "torch.nn.MultiMarginLoss()", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) :", "question_id": 48059}
{"snippet": "torch.nn.MultiMarginLoss(p=1)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`.", "question_id": 48060}
{"snippet": "torch.nn.MultiMarginLoss(margin=1.0)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `margin`.", "question_id": 48061}
{"snippet": "torch.nn.MultiMarginLoss(weight=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : Optionally , you can give non-equal weighting on the classes by passing a 1D `weight` tensor into the constructor .", "question_id": 48062}
{"snippet": "torch.nn.MultiMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `size_average`.", "question_id": 48063}
{"snippet": "torch.nn.MultiMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `reduce`.", "question_id": 48064}
{"snippet": "torch.nn.MultiMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `reduction`.", "question_id": 48065}
{"snippet": "torch.nn.MultiMarginLoss(p=1, margin=1.0)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`, `margin`.", "question_id": 48066}
{"snippet": "torch.nn.MultiMarginLoss(p=1, weight=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : Optionally , you can give non-equal weighting on the classes by passing a 1D `weight` tensor into the constructor . With arguments `p`.", "question_id": 48067}
{"snippet": "torch.nn.MultiMarginLoss(p=1, size_average=None)", "intent": "Creates a criterion that optimizes a multi-class classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 1D tensor of target class indices , 0\u2264y\u2264x.size ( 1 ) \u221210 \\leq y \\leq \\text { x.size } ( 1 ) -10\u2264y\u2264x.size ( 1 ) \u22121 ) : With arguments `p`, `size_average`.", "question_id": 48068}
{"snippet": "torch.nn.functional.softshrink(input)", "intent": "Applies the soft shrinkage function elementwise With arguments `input`.", "question_id": 48069}
{"snippet": "torch.nn.functional.softshrink(input, lambd=0.5)", "intent": "Applies the soft shrinkage function elementwise With arguments `input`, `lambd`.", "question_id": 48070}
{"snippet": "Tensor.ne_(other)", "intent": "In-place version of ne ( ) . With arguments `other`.", "question_id": 48071}
{"snippet": "torch.atanh(input)", "intent": "Returns a new tensor with the inverse hyperbolic tangent of the elements of `input` .", "question_id": 48072}
{"snippet": "torch.atanh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic tangent of the elements of `input` . With arguments `out`.", "question_id": 48073}
{"snippet": "Tensor.tensor_split(indices_or_sections)", "intent": "See torch.tensor_split ( ) With arguments `indices_or_sections`.", "question_id": 48074}
{"snippet": "Tensor.tensor_split(indices_or_sections, dim=0)", "intent": "See torch.tensor_split ( ) With arguments `indices_or_sections`, `dim`.", "question_id": 48075}
{"snippet": "torch.nn.ConstantPad2d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 48076}
{"snippet": "torch.linalg.pinv(A)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 48077}
{"snippet": "torch.linalg.pinv(A, rcond=1e-15)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation .", "question_id": 48078}
{"snippet": "torch.linalg.pinv(A, hermitian=False)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`.", "question_id": 48079}
{"snippet": "torch.linalg.pinv(A, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 48080}
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, hermitian=False)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `hermitian`.", "question_id": 48081}
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `out`.", "question_id": 48082}
{"snippet": "torch.linalg.pinv(A, hermitian=False, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`, `out`.", "question_id": 48083}
{"snippet": "torch.linalg.pinv(A, rcond=1e-15, hermitian=False, out=None)", "intent": "Computes the pseudoinverse ( Moore-Penrose inverse ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The singular values ( or the norm of the eigenvalues when hermitian= True ) that are below the specified `rcond` threshold are treated as zero and discarded in the computation . With arguments `hermitian`, `out`.", "question_id": 48084}
{"snippet": "torch.addmm(input, mat1, mat2)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result .", "question_id": 48085}
{"snippet": "torch.addmm(input, mat1, mat2, beta=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 48086}
{"snippet": "torch.addmm(input, mat1, mat2, alpha=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 48087}
{"snippet": "torch.addmm(input, mat1, mat2, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48088}
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, alpha=1)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively .", "question_id": 48089}
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48090}
{"snippet": "torch.addmm(input, mat1, mat2, alpha=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48091}
{"snippet": "torch.addmm(input, mat1, mat2, beta=1, alpha=1, out=None)", "intent": "Performs a matrix multiplication of the matrices `mat1` and `mat2` . The matrix `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively . If mat1 is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , then input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48092}
{"snippet": "torch.nn.utils.parametrize.register_parametrization(module, tensor_name, parametrization)", "intent": "Adds a `parametrization` to a tensor in a `module` . With arguments `tensor_name`.", "question_id": 48093}
{"snippet": "Tensor.requires_grad_()", "intent": "Change if autograd should record operations on this tensor : sets this tensor \u2019 s `requires_grad` attribute in-place .", "question_id": 48094}
{"snippet": "Tensor.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on this tensor : sets this tensor \u2019 s `requires_grad` attribute in-place .", "question_id": 48095}
{"snippet": "Tensor.negative_()", "intent": "In-place version of negative ( )", "question_id": 48096}
{"snippet": "torch.autograd.set_grad_enabled(mode)", "intent": "Context-manager that sets gradient calculation to on or off . set_grad_enabled will enable or disable grads based on its argument `mode` .", "question_id": 48097}
{"snippet": "torch.vstack(tensors)", "intent": "Stack `tensors` in sequence vertically ( row wise ) .", "question_id": 48098}
{"snippet": "torch.vstack(tensors, out=None)", "intent": "Stack `tensors` in sequence vertically ( row wise ) . With arguments `out`.", "question_id": 48099}
{"snippet": "torch.absolute(input)", "intent": "Alias for torch.abs ( ) With arguments `input`.", "question_id": 48100}
{"snippet": "torch.absolute(input, out=None)", "intent": "Alias for torch.abs ( ) With arguments `input`, `out`.", "question_id": 48101}
{"snippet": "torch.log1p(input)", "intent": "Returns a new tensor with the natural logarithm of ( 1 + `input` ) .", "question_id": 48102}
{"snippet": "torch.log1p(input, out=None)", "intent": "Returns a new tensor with the natural logarithm of ( 1 + `input` ) . With arguments `out`.", "question_id": 48103}
{"snippet": "torch.pinverse(input)", "intent": "Alias for torch.linalg.pinv ( ) With arguments `input`.", "question_id": 48104}
{"snippet": "torch.pinverse(input, rcond=1e-15)", "intent": "Alias for torch.linalg.pinv ( ) With arguments `input`, `rcond`.", "question_id": 48105}
{"snippet": "Tensor.bfloat16()", "intent": "self.bfloat16 ( ) is equivalent to self.to ( torch.bfloat16 ) .", "question_id": 48106}
{"snippet": "Tensor.bfloat16(memory_format=torch.preserve_format)", "intent": "self.bfloat16 ( ) is equivalent to self.to ( torch.bfloat16 ) . With arguments `memory_format`.", "question_id": 48107}
{"snippet": "Tensor.add_(other)", "intent": "In-place version of add ( ) With arguments `other`.", "question_id": 48108}
{"snippet": "Tensor.add_(other, alpha=1)", "intent": "In-place version of add ( ) With arguments `other`, `alpha`.", "question_id": 48109}
{"snippet": "torch.full_like(input, fill_value, \\*)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`.", "question_id": 48110}
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`.", "question_id": 48111}
{"snippet": "torch.full_like(input, fill_value, \\*, layout=torch.strided)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `layout`.", "question_id": 48112}
{"snippet": "torch.full_like(input, fill_value, \\*, device=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `device`.", "question_id": 48113}
{"snippet": "torch.full_like(input, fill_value, \\*, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `requires_grad`.", "question_id": 48114}
{"snippet": "torch.full_like(input, fill_value, \\*, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `memory_format`.", "question_id": 48115}
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, layout=torch.strided)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `layout`.", "question_id": 48116}
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `device`.", "question_id": 48117}
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `requires_grad`.", "question_id": 48118}
{"snippet": "torch.full_like(input, fill_value, \\*, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` filled with `fill_value` . With arguments `\\*`, `dtype`, `memory_format`.", "question_id": 48119}
{"snippet": "torch.nn.Softmin()", "intent": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0 , 1 ] and sum to 1 .", "question_id": 48120}
{"snippet": "torch.nn.Softmin(dim=None)", "intent": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0 , 1 ] and sum to 1 . With arguments `dim`.", "question_id": 48121}
{"snippet": "torch.triu(input)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 48122}
{"snippet": "torch.triu(input, diagonal=0)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The upper triangular part of the matrix is defined as the elements on and above the `diagonal` .", "question_id": 48123}
{"snippet": "torch.triu(input, out=None)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 48124}
{"snippet": "torch.triu(input, diagonal=0, out=None)", "intent": "Returns the upper triangular part of a matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The upper triangular part of the matrix is defined as the elements on and above the `diagonal` .", "question_id": 48125}
{"snippet": "Tensor.bernoulli_()", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) .", "question_id": 48126}
{"snippet": "Tensor.bernoulli_(p=0.5)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) .", "question_id": 48127}
{"snippet": "Tensor.bernoulli_(generator=None)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) . With arguments `generator`.", "question_id": 48128}
{"snippet": "Tensor.bernoulli_(p=0.5, generator=None)", "intent": "Fills each location of self with an independent sample from Bernoulli ( `p` ) \\text { Bernoulli } ( \\texttt { p } ) Bernoulli ( p ) . With arguments `generator`.", "question_id": 48129}
{"snippet": "Tensor.index_fill_(dim, index, value)", "intent": "Fills the elements of the self tensor with `value` value by selecting the indices in the order given in `index` . With arguments `dim`.", "question_id": 48130}
{"snippet": "torch.baddbmm(input, batch1, batch2)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result .", "question_id": 48131}
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 48132}
{"snippet": "torch.baddbmm(input, batch1, batch2, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 48133}
{"snippet": "torch.baddbmm(input, batch1, batch2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 48134}
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) .", "question_id": 48135}
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 48136}
{"snippet": "torch.baddbmm(input, batch1, batch2, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 48137}
{"snippet": "torch.baddbmm(input, batch1, batch2, beta=1, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices in `batch1` and `batch2` . `input` is added to the final result . Both `alpha` and `beta` mean the same as the scaling factors used in torch.addbmm ( ) . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , then input must be broadcastable with a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor and `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 48138}
{"snippet": "Tensor.atan()", "intent": "See torch.atan ( )", "question_id": 48139}
{"snippet": "Tensor.masked_fill(mask, value)", "intent": "Out-of-place version of torch.Tensor.masked_fill_ ( ) With arguments `mask`, `value`.", "question_id": 48140}
{"snippet": "Tensor.take(indices)", "intent": "See torch.take ( ) With arguments `indices`.", "question_id": 48141}
{"snippet": "Tensor.detach_()", "intent": "Detaches the Tensor from the graph that created it , making it a leaf .", "question_id": 48142}
{"snippet": "torch.rand_like(input)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) .", "question_id": 48143}
{"snippet": "torch.rand_like(input, dtype=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`.", "question_id": 48144}
{"snippet": "torch.rand_like(input, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `layout`.", "question_id": 48145}
{"snippet": "torch.rand_like(input, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `device`.", "question_id": 48146}
{"snippet": "torch.rand_like(input, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `requires_grad`.", "question_id": 48147}
{"snippet": "torch.rand_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `memory_format`.", "question_id": 48148}
{"snippet": "torch.rand_like(input, dtype=None, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `layout`.", "question_id": 48149}
{"snippet": "torch.rand_like(input, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `device`.", "question_id": 48150}
{"snippet": "torch.rand_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `requires_grad`.", "question_id": 48151}
{"snippet": "torch.rand_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) . With arguments `dtype`, `memory_format`.", "question_id": 48152}
{"snippet": "torch.nn.functional.rrelu(input)", "intent": "Randomized leaky ReLU . With arguments `input`.", "question_id": 48153}
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`.", "question_id": 48154}
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`.", "question_id": 48155}
{"snippet": "torch.nn.functional.rrelu(input, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `training`.", "question_id": 48156}
{"snippet": "torch.nn.functional.rrelu(input, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `inplace`.", "question_id": 48157}
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, upper=1. / 3)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `upper`.", "question_id": 48158}
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `training`.", "question_id": 48159}
{"snippet": "torch.nn.functional.rrelu(input, lower=1. / 8, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `lower`, `inplace`.", "question_id": 48160}
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3, training=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`, `training`.", "question_id": 48161}
{"snippet": "torch.nn.functional.rrelu(input, upper=1. / 3, inplace=False)", "intent": "Randomized leaky ReLU . With arguments `input`, `upper`, `inplace`.", "question_id": 48162}
{"snippet": "Tensor.exp_()", "intent": "In-place version of exp ( )", "question_id": 48163}
{"snippet": "torch.rsqrt(input)", "intent": "Returns a new tensor with the reciprocal of the square-root of each of the elements of `input` .", "question_id": 48164}
{"snippet": "torch.rsqrt(input, out=None)", "intent": "Returns a new tensor with the reciprocal of the square-root of each of the elements of `input` . With arguments `out`.", "question_id": 48165}
{"snippet": "Tensor.nansum()", "intent": "See torch.nansum ( )", "question_id": 48166}
{"snippet": "Tensor.nansum(dim=None)", "intent": "See torch.nansum ( ) With arguments `dim`.", "question_id": 48167}
{"snippet": "Tensor.nansum(keepdim=False)", "intent": "See torch.nansum ( ) With arguments `keepdim`.", "question_id": 48168}
{"snippet": "Tensor.nansum(dtype=None)", "intent": "See torch.nansum ( ) With arguments `dtype`.", "question_id": 48169}
{"snippet": "Tensor.nansum(dim=None, keepdim=False)", "intent": "See torch.nansum ( ) With arguments `dim`, `keepdim`.", "question_id": 48170}
{"snippet": "Tensor.nansum(dim=None, dtype=None)", "intent": "See torch.nansum ( ) With arguments `dim`, `dtype`.", "question_id": 48171}
{"snippet": "Tensor.nansum(keepdim=False, dtype=None)", "intent": "See torch.nansum ( ) With arguments `keepdim`, `dtype`.", "question_id": 48172}
{"snippet": "Tensor.nansum(dim=None, keepdim=False, dtype=None)", "intent": "See torch.nansum ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 48173}
{"snippet": "torch.nn.functional.l1_loss(input, target)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`.", "question_id": 48174}
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`.", "question_id": 48175}
{"snippet": "torch.nn.functional.l1_loss(input, target, reduce=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduce`.", "question_id": 48176}
{"snippet": "torch.nn.functional.l1_loss(input, target, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduction`.", "question_id": 48177}
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None)", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 48178}
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 48179}
{"snippet": "torch.nn.functional.l1_loss(input, target, reduce=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 48180}
{"snippet": "torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "Function that takes the mean element-wise absolute value difference . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 48181}
{"snippet": "Tensor.cauchy_()", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution :", "question_id": 48182}
{"snippet": "Tensor.cauchy_(median=0)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`.", "question_id": 48183}
{"snippet": "Tensor.cauchy_(sigma=1)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `sigma`.", "question_id": 48184}
{"snippet": "Tensor.cauchy_(generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `generator`.", "question_id": 48185}
{"snippet": "Tensor.cauchy_(median=0, sigma=1)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `sigma`.", "question_id": 48186}
{"snippet": "Tensor.cauchy_(median=0, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `generator`.", "question_id": 48187}
{"snippet": "Tensor.cauchy_(sigma=1, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `sigma`, `generator`.", "question_id": 48188}
{"snippet": "Tensor.cauchy_(median=0, sigma=1, generator=None)", "intent": "Fills the tensor with numbers drawn from the Cauchy distribution : With arguments `median`, `sigma`, `generator`.", "question_id": 48189}
{"snippet": "Tensor.floor_()", "intent": "In-place version of floor ( )", "question_id": 48190}
{"snippet": "torch.quantization.quantize_fx.prepare_qat_fx(model, qconfig_dict)", "intent": "Prepare a `model` for quantization aware training : param model : torch.nn.Module model , must be in train mode : param `qconfig_dict` : see prepare_fx ( ) : param `prepare_custom_config_dict` : see prepare_fx ( )", "question_id": 48191}
{"snippet": "torch.quantization.quantize_fx.prepare_qat_fx(model, qconfig_dict, prepare_custom_config_dict=None)", "intent": "Prepare a `model` for quantization aware training : param model : torch.nn.Module model , must be in train mode : param `qconfig_dict` : see prepare_fx ( ) : param `prepare_custom_config_dict` : see prepare_fx ( )", "question_id": 48192}
{"snippet": "torch.enable_grad", "intent": "Context-manager that enables gradient calculation.", "question_id": 48193}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 48194}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 48195}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, padding=0)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 48196}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 48197}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 48198}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 48199}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 48200}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 48201}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 48202}
{"snippet": "torch.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kD timeskH\u00d7kWkD \\ times kH \\times kWkD timeskH\u00d7kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 48203}
{"snippet": "Tensor.logical_not()", "intent": "See torch.logical_not ( )", "question_id": 48204}
{"snippet": "torch.quantize_per_tensor(input, scale, zero_point, dtype)", "intent": "Converts a float tensor to a quantized tensor with given `scale` and zero point . With arguments `input`, `zero_point`, `dtype`.", "question_id": 48205}
{"snippet": "torch.nn.Linear(in_features, out_features)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`.", "question_id": 48206}
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`.", "question_id": 48207}
{"snippet": "torch.nn.Linear(in_features, out_features, device=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `device`.", "question_id": 48208}
{"snippet": "torch.nn.Linear(in_features, out_features, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `dtype`.", "question_id": 48209}
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, device=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `device`.", "question_id": 48210}
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 48211}
{"snippet": "torch.nn.Linear(in_features, out_features, device=None, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `device`, `dtype`.", "question_id": 48212}
{"snippet": "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b With arguments `in_features`, `out_features`, `bias`, `device`, `dtype`.", "question_id": 48213}
{"snippet": "Tensor.vsplit(split_size_or_sections)", "intent": "See torch.vsplit ( ) With arguments `split_size_or_sections`.", "question_id": 48214}
{"snippet": "Tensor.tile(*reps)", "intent": "See torch.tile ( ) With arguments `*reps`.", "question_id": 48215}
{"snippet": "Tensor.hardshrink()", "intent": "See torch.nn.functional.hardshrink ( )", "question_id": 48216}
{"snippet": "Tensor.hardshrink(lambd=0.5)", "intent": "See torch.nn.functional.hardshrink ( ) With arguments `lambd`.", "question_id": 48217}
{"snippet": "torch.nn.functional.sigmoid(input)", "intent": "Applies the element-wise function Sigmoid ( x ) =11+exp\u2061 ( \u2212x ) \\text { Sigmoid } ( x ) = \\frac { 1 } { 1 + \\exp ( -x ) } Sigmoid ( x ) =1+exp ( \u2212x ) 1\u200b With arguments `input`.", "question_id": 48218}
{"snippet": "Tensor.mode()", "intent": "See torch.mode ( )", "question_id": 48219}
{"snippet": "Tensor.mode(dim=None)", "intent": "See torch.mode ( ) With arguments `dim`.", "question_id": 48220}
{"snippet": "Tensor.mode(keepdim=False)", "intent": "See torch.mode ( ) With arguments `keepdim`.", "question_id": 48221}
{"snippet": "Tensor.mode(dim=None, keepdim=False)", "intent": "See torch.mode ( ) With arguments `dim`, `keepdim`.", "question_id": 48222}
{"snippet": "torch.eig(input)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`.", "question_id": 48223}
{"snippet": "torch.eig(input, eigenvectors=False)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`.", "question_id": 48224}
{"snippet": "torch.eig(input, out=None)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`, `out`.", "question_id": 48225}
{"snippet": "torch.eig(input, eigenvectors=False, out=None)", "intent": "Computes the eigenvalues and `eigenvectors` of a real square matrix . With arguments `input`, `out`.", "question_id": 48226}
{"snippet": "Tensor.bmm(batch2)", "intent": "See torch.bmm ( ) With arguments `batch2`.", "question_id": 48227}
{"snippet": "torch.nn.functional.relu_(input)", "intent": "In-place version of relu ( ) . With arguments `input`.", "question_id": 48228}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`.", "question_id": 48229}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`.", "question_id": 48230}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, p=2)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `p`.", "question_id": 48231}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, eps=1e-06)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `eps`.", "question_id": 48232}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, swap=False)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `swap`.", "question_id": 48233}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, size_average=None)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `size_average`.", "question_id": 48234}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, reduce=None)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `reduce`.", "question_id": 48235}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, reduction='mean')", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `reduction`.", "question_id": 48236}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`, `p`.", "question_id": 48237}
{"snippet": "torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, eps=1e-06)", "intent": "See TripletMarginLoss for details With arguments `anchor`, `positive`, `negative`, `margin`, `eps`.", "question_id": 48238}
{"snippet": "Tensor.index_copy_(dim, index, tensor)", "intent": "Copies the elements of `tensor` into the self tensor by selecting the indices in the order given in `index` . For example , if `dim` == 0 and index [ i ] == j , then the ith row of tensor is copied to the jth row of self .", "question_id": 48239}
{"snippet": "torch.clamp(input)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 48240}
{"snippet": "torch.clamp(input, min=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 48241}
{"snippet": "torch.clamp(input, max=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 48242}
{"snippet": "torch.clamp(input, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 48243}
{"snippet": "torch.clamp(input, min=None, max=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] .", "question_id": 48244}
{"snippet": "torch.clamp(input, min=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 48245}
{"snippet": "torch.clamp(input, max=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 48246}
{"snippet": "torch.clamp(input, min=None, max=None, out=None)", "intent": "Clamps all elements in `input` into the range [ `min` , `max` ] . With arguments `out`.", "question_id": 48247}
{"snippet": "Tensor.q_scale()", "intent": "Given a Tensor quantized by linear ( affine ) quantization , returns the scale of the underlying quantizer ( ) .", "question_id": 48248}
{"snippet": "Tensor.stft(n_fft)", "intent": "See torch.stft ( ) With arguments `n_fft`.", "question_id": 48249}
{"snippet": "Tensor.stft(n_fft, hop_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `hop_length`.", "question_id": 48250}
{"snippet": "Tensor.stft(n_fft, win_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `win_length`.", "question_id": 48251}
{"snippet": "Tensor.stft(n_fft, window=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `window`.", "question_id": 48252}
{"snippet": "Tensor.stft(n_fft, center=True)", "intent": "See torch.stft ( ) With arguments `n_fft`, `center`.", "question_id": 48253}
{"snippet": "Tensor.stft(n_fft, pad_mode='reflect')", "intent": "See torch.stft ( ) With arguments `n_fft`, `pad_mode`.", "question_id": 48254}
{"snippet": "Tensor.stft(n_fft, normalized=False)", "intent": "See torch.stft ( ) With arguments `n_fft`, `normalized`.", "question_id": 48255}
{"snippet": "Tensor.stft(n_fft, onesided=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `onesided`.", "question_id": 48256}
{"snippet": "Tensor.stft(n_fft, return_complex=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `return_complex`.", "question_id": 48257}
{"snippet": "Tensor.stft(n_fft, hop_length=None, win_length=None)", "intent": "See torch.stft ( ) With arguments `n_fft`, `hop_length`, `win_length`.", "question_id": 48258}
{"snippet": "torch.fft.rfftfreq(n)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` .", "question_id": 48259}
{"snippet": "torch.fft.rfftfreq(n, d=1.0)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`.", "question_id": 48260}
{"snippet": "torch.fft.rfftfreq(n, out=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `out`.", "question_id": 48261}
{"snippet": "torch.fft.rfftfreq(n, dtype=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `dtype`.", "question_id": 48262}
{"snippet": "torch.fft.rfftfreq(n, layout=torch.strided)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `layout`.", "question_id": 48263}
{"snippet": "torch.fft.rfftfreq(n, device=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `device`.", "question_id": 48264}
{"snippet": "torch.fft.rfftfreq(n, requires_grad=False)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `requires_grad`.", "question_id": 48265}
{"snippet": "torch.fft.rfftfreq(n, d=1.0, out=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `out`.", "question_id": 48266}
{"snippet": "torch.fft.rfftfreq(n, d=1.0, dtype=None)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `dtype`.", "question_id": 48267}
{"snippet": "torch.fft.rfftfreq(n, d=1.0, layout=torch.strided)", "intent": "Computes the sample frequencies for rfft ( ) with a signal of size `n` . With arguments `d`, `layout`.", "question_id": 48268}
{"snippet": "torch.quantization.quantize(model, run_fn, run_args)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`.", "question_id": 48269}
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, mapping=None)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `mapping`.", "question_id": 48270}
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, inplace=False)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `inplace`.", "question_id": 48271}
{"snippet": "torch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)", "intent": "Quantize the input float `model` with post training static quantization . First it will prepare the model for calibration , then it calls `run_fn` which will run the calibration step , after that we will convert the model to a quantized model . With arguments `run_args`, `mapping`, `inplace`.", "question_id": 48272}
{"snippet": "torch.all(input)", "intent": "Tests if all elements in `input` evaluate to True .", "question_id": 48273}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 48274}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 48275}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 48276}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 48277}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 48278}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 48279}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 48280}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 48281}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 48282}
{"snippet": "torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 1D convolution over an input signal composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 48283}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`.", "question_id": 48284}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 48285}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduce=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 48286}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 48287}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None)", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 48288}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 48289}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, reduce=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 48290}
{"snippet": "torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "See SoftMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 48291}
{"snippet": "Tensor.swapaxes(axis0, axis1)", "intent": "See torch.swapaxes ( ) With arguments `axis0`, `axis1`.", "question_id": 48292}
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values .", "question_id": 48293}
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, size=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`.", "question_id": 48294}
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `scale_factor`.", "question_id": 48295}
{"snippet": "torch.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`, `scale_factor`.", "question_id": 48296}
{"snippet": "Tensor.item()", "intent": "Returns the value of this tensor as a standard Python number .", "question_id": 48297}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 48298}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 48299}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 48300}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 48301}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 48302}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 48303}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 48304}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 48305}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 48306}
{"snippet": "torch.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn1d module is a module fused from Conv1d and BatchNorm1d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 48307}
{"snippet": "torch.can_cast(from, to)", "intent": "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation . With arguments `from`, `to`.", "question_id": 48308}
{"snippet": "torch.fft.fftfreq(n)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` .", "question_id": 48309}
{"snippet": "torch.fft.fftfreq(n, d=1.0)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`.", "question_id": 48310}
{"snippet": "torch.fft.fftfreq(n, out=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `out`.", "question_id": 48311}
{"snippet": "torch.fft.fftfreq(n, dtype=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `dtype`.", "question_id": 48312}
{"snippet": "torch.fft.fftfreq(n, layout=torch.strided)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `layout`.", "question_id": 48313}
{"snippet": "torch.fft.fftfreq(n, device=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `device`.", "question_id": 48314}
{"snippet": "torch.fft.fftfreq(n, requires_grad=False)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `requires_grad`.", "question_id": 48315}
{"snippet": "torch.fft.fftfreq(n, d=1.0, out=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `out`.", "question_id": 48316}
{"snippet": "torch.fft.fftfreq(n, d=1.0, dtype=None)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `dtype`.", "question_id": 48317}
{"snippet": "torch.fft.fftfreq(n, d=1.0, layout=torch.strided)", "intent": "Computes the discrete Fourier Transform sample frequencies for a signal of size `n` . With arguments `d`, `layout`.", "question_id": 48318}
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 48319}
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 48320}
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, padding_value=0.0)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 48321}
{"snippet": "torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0.0)", "intent": "Pad a list of variable length Tensors with `padding_value` For example , if the input is list of `sequences` with size L x * and if `batch_first` is False , and T x B x * otherwise .", "question_id": 48322}
{"snippet": "Tensor.amax()", "intent": "See torch.amax ( )", "question_id": 48323}
{"snippet": "Tensor.amax(dim=None)", "intent": "See torch.amax ( ) With arguments `dim`.", "question_id": 48324}
{"snippet": "Tensor.amax(keepdim=False)", "intent": "See torch.amax ( ) With arguments `keepdim`.", "question_id": 48325}
{"snippet": "Tensor.amax(dim=None, keepdim=False)", "intent": "See torch.amax ( ) With arguments `dim`, `keepdim`.", "question_id": 48326}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 48327}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 48328}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 48329}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 48330}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 48331}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 48332}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 48333}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 48334}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 48335}
{"snippet": "torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 48336}
{"snippet": "torch.logical_not(input)", "intent": "Computes the element-wise logical NOT of the given `input` tensor .", "question_id": 48337}
{"snippet": "torch.logical_not(input, out=None)", "intent": "Computes the element-wise logical NOT of the given `input` tensor . With arguments `out`.", "question_id": 48338}
{"snippet": "Tensor.diag_embed()", "intent": "See torch.diag_embed ( )", "question_id": 48339}
{"snippet": "Tensor.diag_embed(offset=0)", "intent": "See torch.diag_embed ( ) With arguments `offset`.", "question_id": 48340}
{"snippet": "Tensor.diag_embed(dim1=- 2)", "intent": "See torch.diag_embed ( ) With arguments `dim1`.", "question_id": 48341}
{"snippet": "Tensor.diag_embed(dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `dim2`.", "question_id": 48342}
{"snippet": "Tensor.diag_embed(offset=0, dim1=- 2)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim1`.", "question_id": 48343}
{"snippet": "Tensor.diag_embed(offset=0, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim2`.", "question_id": 48344}
{"snippet": "Tensor.diag_embed(dim1=- 2, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `dim1`, `dim2`.", "question_id": 48345}
{"snippet": "Tensor.diag_embed(offset=0, dim1=- 2, dim2=- 1)", "intent": "See torch.diag_embed ( ) With arguments `offset`, `dim1`, `dim2`.", "question_id": 48346}
{"snippet": "torch.autograd.inference_mode()", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 48347}
{"snippet": "torch.autograd.inference_mode(mode=True)", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 48348}
{"snippet": "Tensor.digamma()", "intent": "See torch.digamma ( )", "question_id": 48349}
{"snippet": "torch.tensordot(a, b)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions .", "question_id": 48350}
{"snippet": "torch.tensordot(a, b, dims=2)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . When called with a non-negative integer argument `dims` = ddd , and the number of dimensions of a and b is mmm and nnn , respectively , tensordot ( ) computes", "question_id": 48351}
{"snippet": "torch.tensordot(a, b, out=None)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . With arguments `out`.", "question_id": 48352}
{"snippet": "torch.tensordot(a, b, dims=2, out=None)", "intent": "Returns `a` contraction of a and `b` over multiple dimensions . When called with a non-negative integer argument `dims` = ddd , and the number of dimensions of a and b is mmm and nnn , respectively , tensordot ( ) computes With arguments `out`.", "question_id": 48353}
{"snippet": "torch.nn.intrinsic.ConvBnReLU2d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 2d , Batch Norm 2d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 48354}
{"snippet": "Tensor.view(*shape)", "intent": "Returns a new tensor with the same data as the self tensor but of a different shape . With arguments `*shape`.", "question_id": 48355}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48356}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48357}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48358}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48359}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 48360}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 48361}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 48362}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 48363}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 48364}
{"snippet": "torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 3D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48365}
{"snippet": "torch.sgn(input)", "intent": "This function is an extension of torch.sign ( ) to complex tensors . It computes a new tensor whose elements have the same angles as the corresponding elements of `input` and absolute values ( i.e .", "question_id": 48366}
{"snippet": "torch.sgn(input, out=None)", "intent": "This function is an extension of torch.sign ( ) to complex tensors . It computes a new tensor whose elements have the same angles as the corresponding elements of `input` and absolute values ( i.e . With arguments `out`.", "question_id": 48367}
{"snippet": "Tensor.lu()", "intent": "See torch.lu ( )", "question_id": 48368}
{"snippet": "Tensor.lu(pivot=True)", "intent": "See torch.lu ( ) With arguments `pivot`.", "question_id": 48369}
{"snippet": "Tensor.lu(get_infos=False)", "intent": "See torch.lu ( ) With arguments `get_infos`.", "question_id": 48370}
{"snippet": "Tensor.lu(pivot=True, get_infos=False)", "intent": "See torch.lu ( ) With arguments `pivot`, `get_infos`.", "question_id": 48371}
{"snippet": "torch.nn.quantized.Linear(in_features, out_features)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`.", "question_id": 48372}
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, bias_=True)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`.", "question_id": 48373}
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, dtype=torch.qint8)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 48374}
{"snippet": "torch.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)", "intent": "A quantized linear module with quantized tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`, `dtype`.", "question_id": 48375}
{"snippet": "linear.from_float(mod)", "intent": "Create a quantized module from a float module or qparams_dict With arguments `mod`.", "question_id": 48376}
{"snippet": "torch.sparse.softmax(input, dim)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`.", "question_id": 48377}
{"snippet": "torch.sparse.softmax(input, dim, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `dtype`.", "question_id": 48378}
{"snippet": "torch.tensor(data)", "intent": "Constructs a tensor with `data` .", "question_id": 48379}
{"snippet": "torch.tensor(data, dtype=None)", "intent": "Constructs a tensor with `data` . With arguments `dtype`.", "question_id": 48380}
{"snippet": "torch.tensor(data, device=None)", "intent": "Constructs a tensor with `data` . With arguments `device`.", "question_id": 48381}
{"snippet": "torch.tensor(data, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `requires_grad`.", "question_id": 48382}
{"snippet": "torch.tensor(data, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `pin_memory`.", "question_id": 48383}
{"snippet": "torch.tensor(data, dtype=None, device=None)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `device`.", "question_id": 48384}
{"snippet": "torch.tensor(data, dtype=None, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `requires_grad`.", "question_id": 48385}
{"snippet": "torch.tensor(data, dtype=None, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `dtype`, `pin_memory`.", "question_id": 48386}
{"snippet": "torch.tensor(data, device=None, requires_grad=False)", "intent": "Constructs a tensor with `data` . With arguments `device`, `requires_grad`.", "question_id": 48387}
{"snippet": "torch.tensor(data, device=None, pin_memory=False)", "intent": "Constructs a tensor with `data` . With arguments `device`, `pin_memory`.", "question_id": 48388}
{"snippet": "torch.real(input)", "intent": "Returns a new tensor containing real values of the self tensor . With arguments `input`.", "question_id": 48389}
{"snippet": "Tensor.bincount()", "intent": "See torch.bincount ( )", "question_id": 48390}
{"snippet": "Tensor.bincount(weights=None)", "intent": "See torch.bincount ( ) With arguments `weights`.", "question_id": 48391}
{"snippet": "Tensor.bincount(minlength=0)", "intent": "See torch.bincount ( ) With arguments `minlength`.", "question_id": 48392}
{"snippet": "Tensor.bincount(weights=None, minlength=0)", "intent": "See torch.bincount ( ) With arguments `weights`, `minlength`.", "question_id": 48393}
{"snippet": "torch.linalg.multi_dot(tensors)", "intent": "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed . Every tensor in `tensors` must be 2D , except for the first and last which may be 1D .", "question_id": 48394}
{"snippet": "torch.linalg.multi_dot(tensors, out=None)", "intent": "Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed . Every tensor in `tensors` must be 2D , except for the first and last which may be 1D . With arguments `out`.", "question_id": 48395}
{"snippet": "Tensor.renorm(p, dim, maxnorm)", "intent": "See torch.renorm ( ) With arguments `p`, `dim`, `maxnorm`.", "question_id": 48396}
{"snippet": "Tensor.sqrt_()", "intent": "In-place version of sqrt ( )", "question_id": 48397}
{"snippet": "Tensor.bitwise_and_()", "intent": "In-place version of bitwise_and ( )", "question_id": 48398}
{"snippet": "Tensor.lstsq(A)", "intent": "See torch.lstsq ( ) With arguments `A`.", "question_id": 48399}
{"snippet": "torch.nn.MarginRankingLoss()", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) .", "question_id": 48400}
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`.", "question_id": 48401}
{"snippet": "torch.nn.MarginRankingLoss(size_average=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 48402}
{"snippet": "torch.nn.MarginRankingLoss(reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 48403}
{"snippet": "torch.nn.MarginRankingLoss(reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 48404}
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, size_average=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `size_average`.", "question_id": 48405}
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduce`.", "question_id": 48406}
{"snippet": "torch.nn.MarginRankingLoss(margin=0.0, reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduction`.", "question_id": 48407}
{"snippet": "torch.nn.MarginRankingLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 48408}
{"snippet": "torch.nn.MarginRankingLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the loss given inputs x1x1x1 , x2x2x2 , two 1D mini-batch Tensors , and a label 1D mini-batch tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 48409}
{"snippet": "torch.lu_solve(b, LU_data, LU_pivots)", "intent": "Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu ( ) . With arguments `b`, `LU_data`, `LU_pivots`.", "question_id": 48410}
{"snippet": "torch.lu_solve(b, LU_data, LU_pivots, out=None)", "intent": "Returns the LU solve of the linear system Ax=bAx = bAx=b using the partially pivoted LU factorization of A from torch.lu ( ) . With arguments `b`, `LU_data`, `LU_pivots`, `out`.", "question_id": 48411}
{"snippet": "torch.cuda.memory_cached()", "intent": "Deprecated ; see memory_reserved ( ) .", "question_id": 48412}
{"snippet": "torch.cuda.memory_cached(device=None)", "intent": "Deprecated ; see memory_reserved ( ) . With arguments `device`.", "question_id": 48413}
{"snippet": "torch.nn.RNN(*args, **kwargs)", "intent": "Applies a multi-layer Elman RNN with tanh\u2061\\tanhtanh or ReLU\\text { ReLU } ReLU non-linearity to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 48414}
{"snippet": "Tensor.fill_(value)", "intent": "Fills self tensor with the specified `value` .", "question_id": 48415}
{"snippet": "torch.asin(input)", "intent": "Returns a new tensor with the arcsine of the elements of `input` .", "question_id": 48416}
{"snippet": "torch.asin(input, out=None)", "intent": "Returns a new tensor with the arcsine of the elements of `input` . With arguments `out`.", "question_id": 48417}
{"snippet": "Tensor.floor()", "intent": "See torch.floor ( )", "question_id": 48418}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 48419}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 48420}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 48421}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 48422}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 48423}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 48424}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 48425}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 48426}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 48427}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 48428}
{"snippet": "torch.is_storage(obj)", "intent": "Returns True if `obj` is a PyTorch storage object .", "question_id": 48429}
{"snippet": "torch.fft.hfft(input)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal .", "question_id": 48430}
{"snippet": "torch.fft.hfft(input, n=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`.", "question_id": 48431}
{"snippet": "torch.fft.hfft(input, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`.", "question_id": 48432}
{"snippet": "torch.fft.hfft(input, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `norm`.", "question_id": 48433}
{"snippet": "torch.fft.hfft(input, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `out`.", "question_id": 48434}
{"snippet": "torch.fft.hfft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `dim`.", "question_id": 48435}
{"snippet": "torch.fft.hfft(input, n=None, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `norm`.", "question_id": 48436}
{"snippet": "torch.fft.hfft(input, n=None, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `n`, `out`.", "question_id": 48437}
{"snippet": "torch.fft.hfft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`, `norm`.", "question_id": 48438}
{"snippet": "torch.fft.hfft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric `input` signal . With arguments `dim`, `out`.", "question_id": 48439}
{"snippet": "Tensor.reshape(*shape)", "intent": "Returns a tensor with the same data and number of elements as self but with the specified shape . With arguments `*shape`.", "question_id": 48440}
{"snippet": "torch.subtract(input, other)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`.", "question_id": 48441}
{"snippet": "torch.subtract(input, other, alpha=1)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `alpha`.", "question_id": 48442}
{"snippet": "torch.subtract(input, other, out=None)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `out`.", "question_id": 48443}
{"snippet": "torch.subtract(input, other, alpha=1, out=None)", "intent": "Alias for torch.sub ( ) . With arguments `input`, `other`, `alpha`, `out`.", "question_id": 48444}
{"snippet": "Tensor.zero_()", "intent": "Fills self tensor with zeros .", "question_id": 48445}
{"snippet": "Tensor.size()", "intent": "Returns the size of the self tensor .", "question_id": 48446}
{"snippet": "torch.cosh(input)", "intent": "Returns a new tensor with the hyperbolic cosine of the elements of `input` .", "question_id": 48447}
{"snippet": "torch.cosh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic cosine of the elements of `input` . With arguments `out`.", "question_id": 48448}
{"snippet": "Tensor.squeeze_()", "intent": "In-place version of squeeze ( )", "question_id": 48449}
{"snippet": "Tensor.squeeze_(dim=None)", "intent": "In-place version of squeeze ( ) With arguments `dim`.", "question_id": 48450}
{"snippet": "torch.nn.functional.dropout(input)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 48451}
{"snippet": "torch.nn.functional.dropout(input, p=0.5)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 48452}
{"snippet": "torch.nn.functional.dropout(input, training=True)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 48453}
{"snippet": "torch.nn.functional.dropout(input, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 48454}
{"snippet": "torch.nn.functional.dropout(input, p=0.5, training=True)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 48455}
{"snippet": "torch.nn.functional.dropout(input, p=0.5, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 48456}
{"snippet": "torch.nn.functional.dropout(input, training=True, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 48457}
{"snippet": "torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)", "intent": "During `training` , randomly zeroes some of the elements of the `input` tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 48458}
{"snippet": "Tensor.bitwise_or()", "intent": "See torch.bitwise_or ( )", "question_id": 48459}
{"snippet": "torch.nn.Sigmoid", "intent": "Applies the element-wise function:", "question_id": 48460}
{"snippet": "torch.matmul(input, other)", "intent": "Matrix product of two tensors . With arguments `input`, `other`.", "question_id": 48461}
{"snippet": "torch.matmul(input, other, out=None)", "intent": "Matrix product of two tensors . With arguments `input`, `other`, `out`.", "question_id": 48462}
{"snippet": "Tensor.logsumexp(dim)", "intent": "See torch.logsumexp ( ) With arguments `dim`.", "question_id": 48463}
{"snippet": "Tensor.logsumexp(dim, keepdim=False)", "intent": "See torch.logsumexp ( ) With arguments `dim`, `keepdim`.", "question_id": 48464}
{"snippet": "Tensor.expand(*sizes)", "intent": "Returns a new view of the self tensor with singleton dimensions expanded to a larger size . With arguments `*sizes`.", "question_id": 48465}
{"snippet": "Tensor.unsqueeze(dim)", "intent": "See torch.unsqueeze ( ) With arguments `dim`.", "question_id": 48466}
{"snippet": "torch.unbind(input)", "intent": "Removes a tensor dimension . With arguments `input`.", "question_id": 48467}
{"snippet": "torch.unbind(input, dim=0)", "intent": "Removes a tensor dimension . With arguments `input`, `dim`.", "question_id": 48468}
{"snippet": "Tensor.outer(vec2)", "intent": "See torch.outer ( ) . With arguments `vec2`.", "question_id": 48469}
{"snippet": "torch.addcdiv(input, tensor1, tensor2)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 48470}
{"snippet": "torch.addcdiv(input, tensor1, tensor2, value=1)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 48471}
{"snippet": "torch.addcdiv(input, tensor1, tensor2, out=None)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 48472}
{"snippet": "torch.addcdiv(input, tensor1, tensor2, value=1, out=None)", "intent": "Performs the element-wise division of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 48473}
{"snippet": "torch.var_mean(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`.", "question_id": 48474}
{"snippet": "torch.var_mean(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 48475}
{"snippet": "torch.var_mean(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 48476}
{"snippet": "torch.var_mean(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the variance . Calculates the variance and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 48477}
{"snippet": "Tensor.solve(A)", "intent": "See torch.solve ( ) With arguments `A`.", "question_id": 48478}
{"snippet": "torch.quantization.observer.MinMaxObserver()", "intent": "Observer module for computing the quantization parameters based on the running min and max values .", "question_id": 48479}
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`.", "question_id": 48480}
{"snippet": "torch.quantization.observer.MinMaxObserver(qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `qscheme`.", "question_id": 48481}
{"snippet": "torch.quantization.observer.MinMaxObserver(reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `reduce_range`.", "question_id": 48482}
{"snippet": "torch.quantization.observer.MinMaxObserver(quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `quant_min`.", "question_id": 48483}
{"snippet": "torch.quantization.observer.MinMaxObserver(quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `quant_max`.", "question_id": 48484}
{"snippet": "torch.quantization.observer.MinMaxObserver(factory_kwargs=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `factory_kwargs`.", "question_id": 48485}
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `qscheme`.", "question_id": 48486}
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `reduce_range`.", "question_id": 48487}
{"snippet": "torch.quantization.observer.MinMaxObserver(dtype=torch.quint8, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running min and max values . With arguments `dtype`, `quant_min`.", "question_id": 48488}
{"snippet": "min_max_observer.calculate_qparams()", "intent": "Calculates the quantization parameters .", "question_id": 48489}
{"snippet": "min_max_observer.forward(x_orig)", "intent": "Records the running minimum and maximum of x . With arguments `x_orig`.", "question_id": 48490}
{"snippet": "torch.renorm(input, p, dim, maxnorm)", "intent": "Returns a tensor where each sub-tensor of `input` along dimension `dim` is normalized such that the p-norm of the sub-tensor is lower than the value `maxnorm` With arguments `p`.", "question_id": 48491}
{"snippet": "torch.renorm(input, p, dim, maxnorm, out=None)", "intent": "Returns a tensor where each sub-tensor of `input` along dimension `dim` is normalized such that the p-norm of the sub-tensor is lower than the value `maxnorm` With arguments `p`, `out`.", "question_id": 48492}
{"snippet": "torch.zeros_like(input)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` .", "question_id": 48493}
{"snippet": "torch.zeros_like(input, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`.", "question_id": 48494}
{"snippet": "torch.zeros_like(input, layout=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `layout`.", "question_id": 48495}
{"snippet": "torch.zeros_like(input, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `device`.", "question_id": 48496}
{"snippet": "torch.zeros_like(input, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `requires_grad`.", "question_id": 48497}
{"snippet": "torch.zeros_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `memory_format`.", "question_id": 48498}
{"snippet": "torch.zeros_like(input, dtype=None, layout=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 48499}
{"snippet": "torch.zeros_like(input, dtype=None, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 48500}
{"snippet": "torch.zeros_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 48501}
{"snippet": "torch.zeros_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 0 , with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 48502}
{"snippet": "torch.quantization.quantize_fx.fuse_fx(model)", "intent": "Fuse modules like conv+bn , conv+bn+relu etc , `model` must be in eval mode .", "question_id": 48503}
{"snippet": "torch.quantization.quantize_fx.fuse_fx(model, fuse_custom_config_dict=None)", "intent": "Fuse modules like conv+bn , conv+bn+relu etc , `model` must be in eval mode . Fusion rules are defined in torch.quantization.fx.fusion_pattern.py : param model : a torch.nn.Module model : param `fuse_custom_config_dict` : Dictionary for custom configurations for fuse_fx , e.g .", "question_id": 48504}
{"snippet": "Tensor.addcmul_(tensor1, tensor2)", "intent": "In-place version of addcmul ( ) With arguments `tensor1`, `tensor2`.", "question_id": 48505}
{"snippet": "Tensor.addcmul_(tensor1, tensor2, value=1)", "intent": "In-place version of addcmul ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 48506}
{"snippet": "torch.unique_consecutive(*args, **kwargs)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `*args`, `**kwargs`.", "question_id": 48507}
{"snippet": "torch.set_num_threads(int)", "intent": "Sets the number of threads used for intraop parallelism on CPU . With arguments `int`.", "question_id": 48508}
{"snippet": "torch.fmax(input, other)", "intent": "Computes the element-wise maximum of `input` and `other` .", "question_id": 48509}
{"snippet": "torch.fmax(input, other, out=None)", "intent": "Computes the element-wise maximum of `input` and `other` . With arguments `out`.", "question_id": 48510}
{"snippet": "torch.nn.modules.module.register_module_full_backward_hook(hook)", "intent": "Registers a backward `hook` common to all the modules .", "question_id": 48511}
{"snippet": "Tensor.pin_memory()", "intent": "Copies the tensor to pinned memory , if it \u2019 s not already pinned .", "question_id": 48512}
{"snippet": "Tensor.logaddexp(other)", "intent": "See torch.logaddexp ( ) With arguments `other`.", "question_id": 48513}
{"snippet": "Tensor.clone()", "intent": "See torch.clone ( )", "question_id": 48514}
{"snippet": "Tensor.clone(memory_format=torch.preserve_format)", "intent": "See torch.clone ( ) With arguments `memory_format`.", "question_id": 48515}
{"snippet": "torch.quantization.quantize_fx.prepare_fx(model, qconfig_dict)", "intent": "Prepare a `model` for post training static quantization `qconfig_dict` = { \u201c \u201d : qconfig } prepared_model = prepare_fx ( float_model , qconfig_dict ) # Run calibration calibrate ( prepared_model , sample_inference_data ) `` `", "question_id": 48516}
{"snippet": "torch.quantization.quantize_fx.prepare_fx(model, qconfig_dict, prepare_custom_config_dict=None)", "intent": "Prepare a `model` for post training static quantization `qconfig_dict` = { \u201c \u201d : qconfig } prepared_model = prepare_fx ( float_model , qconfig_dict ) # Run calibration calibrate ( prepared_model , sample_inference_data ) `` ` : param : : param # priority : global , object_type , module_name_regex , module_name : type # priority : in increasing order : param # qconfig == None means fusion and quantization should be skipped for anything : : param # matching the rule : : param } : : param `prepare_custom_config_dict` : customization configuration dictionary for : param quantization tool : : param prepare_custom_config_dict = { : # optional : specify the path for standalone modules", "question_id": 48517}
{"snippet": "Tensor.mvlgamma(p)", "intent": "See torch.mvlgamma ( ) With arguments `p`.", "question_id": 48518}
{"snippet": "torch.atan2(input, other)", "intent": "Element-wise arctangent of inputi/otheri\\text { `input` } _ { i } / \\text { `other` } _ { i } inputi\u200b/otheri\u200b with consideration of the quadrant .", "question_id": 48519}
{"snippet": "torch.atan2(input, other, out=None)", "intent": "Element-wise arctangent of inputi/otheri\\text { `input` } _ { i } / \\text { `other` } _ { i } inputi\u200b/otheri\u200b with consideration of the quadrant . With arguments `out`.", "question_id": 48520}
{"snippet": "torch.count_nonzero(input)", "intent": "Counts the number of non-zero values in the tensor `input` along the given `dim` .", "question_id": 48521}
{"snippet": "torch.count_nonzero(input, dim=None)", "intent": "Counts the number of non-zero values in the tensor `input` along the given `dim` .", "question_id": 48522}
{"snippet": "Tensor.nelement()", "intent": "Alias for numel ( )", "question_id": 48523}
{"snippet": "torch.cuda.memory_allocated()", "intent": "Returns the current GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 48524}
{"snippet": "torch.cuda.memory_allocated(device=None)", "intent": "Returns the current GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 48525}
{"snippet": "torch.gather(input, dim, index)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions .", "question_id": 48526}
{"snippet": "torch.gather(input, dim, index, sparse_grad=False)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . With arguments `sparse_grad`.", "question_id": 48527}
{"snippet": "torch.gather(input, dim, index, out=None)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . `out` will have the same shape as index .", "question_id": 48528}
{"snippet": "torch.gather(input, dim, index, sparse_grad=False, out=None)", "intent": "Gathers values along an axis specified by `dim` . `input` and `index` must have the same number of dimensions . `out` will have the same shape as index . With arguments `sparse_grad`.", "question_id": 48529}
{"snippet": "Tensor.diag()", "intent": "See torch.diag ( )", "question_id": 48530}
{"snippet": "Tensor.diag(diagonal=0)", "intent": "See torch.diag ( ) With arguments `diagonal`.", "question_id": 48531}
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` .", "question_id": 48532}
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, dim=1)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` .", "question_id": 48533}
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, eps=1e-8)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` . With arguments `eps`.", "question_id": 48534}
{"snippet": "torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8)", "intent": "Returns cosine similarity between `x1` and `x2` , computed along `dim` . With arguments `eps`.", "question_id": 48535}
{"snippet": "torch.nn.Tanh", "intent": "Applies the element-wise function:", "question_id": 48536}
{"snippet": "Tensor.gcd(other)", "intent": "See torch.gcd ( ) With arguments `other`.", "question_id": 48537}
{"snippet": "torch.quantization.QuantStub()", "intent": "Quantize stub module , before calibration , this is same as an observer , it will be swapped as nnq.Quantize in convert .", "question_id": 48538}
{"snippet": "torch.quantization.QuantStub(qconfig=None)", "intent": "Quantize stub module , before calibration , this is same as an observer , it will be swapped as nnq.Quantize in convert . With arguments `qconfig`.", "question_id": 48539}
{"snippet": "torch.addbmm(input, batch1, batch2)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result .", "question_id": 48540}
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated .", "question_id": 48541}
{"snippet": "torch.addbmm(input, batch1, batch2, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers .", "question_id": 48542}
{"snippet": "torch.addbmm(input, batch1, batch2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48543}
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, alpha=1)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers .", "question_id": 48544}
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48545}
{"snippet": "torch.addbmm(input, batch1, batch2, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48546}
{"snippet": "torch.addbmm(input, batch1, batch2, beta=1, alpha=1, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `batch1` and `batch2` , with a reduced add step ( all matrix multiplications get accumulated along the first dimension ) . `input` is added to the final result . If `beta` is 0 , then input will be ignored , and nan and inf in it will not be propagated . For inputs of type FloatTensor or DoubleTensor , arguments beta and `alpha` must be real numbers , otherwise they should be integers . If batch1 is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , batch2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , input must be broadcastable with a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor and `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 48547}
{"snippet": "torch.column_stack(tensors)", "intent": "Creates a new tensor by horizontally stacking the `tensors` in tensors .", "question_id": 48548}
{"snippet": "torch.column_stack(tensors, out=None)", "intent": "Creates a new tensor by horizontally stacking the `tensors` in tensors . With arguments `out`.", "question_id": 48549}
{"snippet": "torch.tensor_split(input, indices_or_sections)", "intent": "Splits a tensor into multiple sub-tensors , all of which are views of `input` , along dimension `dim` according to the indices or number of sections specified by `indices_or_sections` .", "question_id": 48550}
{"snippet": "torch.tensor_split(input, indices_or_sections, dim=0)", "intent": "Splits a tensor into multiple sub-tensors , all of which are views of `input` , along dimension `dim` according to the indices or number of sections specified by `indices_or_sections` .", "question_id": 48551}
{"snippet": "torch.nn.functional.relu6(input)", "intent": "Applies the element-wise function ReLU6 ( x ) =min\u2061 ( max\u2061 ( 0 , x ) ,6 ) \\text { ReLU6 } ( x ) = \\min ( \\max ( 0 , x ) , 6 ) ReLU6 ( x ) =min ( max ( 0 , x ) ,6 ) . With arguments `input`.", "question_id": 48552}
{"snippet": "torch.nn.functional.relu6(input, inplace=False)", "intent": "Applies the element-wise function ReLU6 ( x ) =min\u2061 ( max\u2061 ( 0 , x ) ,6 ) \\text { ReLU6 } ( x ) = \\min ( \\max ( 0 , x ) , 6 ) ReLU6 ( x ) =min ( max ( 0 , x ) ,6 ) . With arguments `input`, `inplace`.", "question_id": 48553}
{"snippet": "Tensor.sgn_()", "intent": "In-place version of sgn ( )", "question_id": 48554}
{"snippet": "Tensor.pinverse()", "intent": "See torch.pinverse ( )", "question_id": 48555}
{"snippet": "torch.jit.load(f)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`.", "question_id": 48556}
{"snippet": "torch.jit.load(f, map_location=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `map_location`.", "question_id": 48557}
{"snippet": "torch.jit.load(f, _extra_files=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `_extra_files`.", "question_id": 48558}
{"snippet": "torch.jit.load(f, map_location=None, _extra_files=None)", "intent": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save With arguments `f`, `map_location`, `_extra_files`.", "question_id": 48559}
{"snippet": "Function.forward(ctx, *args, **kwargs)", "intent": "Performs the operation . It must accept a context `ctx` as the first argument , followed by any number of arguments ( tensors or other types ) . With arguments `*args`, `**kwargs`.", "question_id": 48560}
{"snippet": "torch.cartesian_prod(*tensors)", "intent": "Do cartesian product of the given sequence of tensors . With arguments `*tensors`.", "question_id": 48561}
{"snippet": "Tensor.chunk(chunks)", "intent": "See torch.chunk ( ) With arguments `chunks`.", "question_id": 48562}
{"snippet": "Tensor.chunk(chunks, dim=0)", "intent": "See torch.chunk ( ) With arguments `chunks`, `dim`.", "question_id": 48563}
{"snippet": "Tensor.dense_dim()", "intent": "Return the number of dense dimensions in a sparse tensor self .", "question_id": 48564}
{"snippet": "Tensor.logical_xor()", "intent": "See torch.logical_xor ( )", "question_id": 48565}
{"snippet": "torch.cuda.memory_stats()", "intent": "Returns a dictionary of CUDA memory allocator statistics for a given `device` .", "question_id": 48566}
{"snippet": "torch.cuda.memory_stats(device=None)", "intent": "Returns a dictionary of CUDA memory allocator statistics for a given `device` .", "question_id": 48567}
{"snippet": "torch.randint_like(input, high, \\*)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`.", "question_id": 48568}
{"snippet": "torch.randint_like(input, high, \\*, low=0)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`.", "question_id": 48569}
{"snippet": "torch.randint_like(input, high, \\*, dtype=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `dtype`.", "question_id": 48570}
{"snippet": "torch.randint_like(input, high, \\*, layout=torch.strided)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `layout`.", "question_id": 48571}
{"snippet": "torch.randint_like(input, high, \\*, device=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `device`.", "question_id": 48572}
{"snippet": "torch.randint_like(input, high, \\*, requires_grad=False)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `requires_grad`.", "question_id": 48573}
{"snippet": "torch.randint_like(input, high, \\*, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `memory_format`.", "question_id": 48574}
{"snippet": "torch.randint_like(input, high, \\*, low=0, dtype=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `dtype`.", "question_id": 48575}
{"snippet": "torch.randint_like(input, high, \\*, low=0, layout=torch.strided)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `layout`.", "question_id": 48576}
{"snippet": "torch.randint_like(input, high, \\*, low=0, device=None)", "intent": "Returns a tensor with the same shape as Tensor `input` filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . With arguments `\\*`, `device`.", "question_id": 48577}
{"snippet": "torch.nn.PReLU()", "intent": "Applies the element-wise function :", "question_id": 48578}
{"snippet": "torch.nn.PReLU(num_parameters=1)", "intent": "Applies the element-wise function : With arguments `num_parameters`.", "question_id": 48579}
{"snippet": "torch.nn.PReLU(init=0.25)", "intent": "Applies the element-wise function : With arguments `init`.", "question_id": 48580}
{"snippet": "torch.nn.PReLU(device=None)", "intent": "Applies the element-wise function : With arguments `device`.", "question_id": 48581}
{"snippet": "torch.nn.PReLU(dtype=None)", "intent": "Applies the element-wise function : With arguments `dtype`.", "question_id": 48582}
{"snippet": "torch.nn.PReLU(num_parameters=1, init=0.25)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `init`.", "question_id": 48583}
{"snippet": "torch.nn.PReLU(num_parameters=1, device=None)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `device`.", "question_id": 48584}
{"snippet": "torch.nn.PReLU(num_parameters=1, dtype=None)", "intent": "Applies the element-wise function : With arguments `num_parameters`, `dtype`.", "question_id": 48585}
{"snippet": "torch.nn.PReLU(init=0.25, device=None)", "intent": "Applies the element-wise function : With arguments `init`, `device`.", "question_id": 48586}
{"snippet": "torch.nn.PReLU(init=0.25, dtype=None)", "intent": "Applies the element-wise function : With arguments `init`, `dtype`.", "question_id": 48587}
{"snippet": "Tensor.q_zero_point()", "intent": "Given a Tensor quantized by linear ( affine ) quantization , returns the zero_point of the underlying quantizer ( ) .", "question_id": 48588}
{"snippet": "torch.nn.functional.elu_(input)", "intent": "In-place version of elu ( ) . With arguments `input`.", "question_id": 48589}
{"snippet": "torch.nn.functional.elu_(input, alpha=1.)", "intent": "In-place version of elu ( ) . With arguments `input`, `alpha`.", "question_id": 48590}
{"snippet": "Tensor.random_()", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 48591}
{"snippet": "Tensor.random_(from=0)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 48592}
{"snippet": "Tensor.random_(to=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 48593}
{"snippet": "Tensor.random_(generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 48594}
{"snippet": "Tensor.random_(from=0, to=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] .", "question_id": 48595}
{"snippet": "Tensor.random_(from=0, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 48596}
{"snippet": "Tensor.random_(to=None, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 48597}
{"snippet": "Tensor.random_(from=0, to=None, generator=None)", "intent": "Fills self tensor with numbers sampled `from` the discrete uniform distribution over [ from , `to` - 1 ] . With arguments `generator`.", "question_id": 48598}
{"snippet": "Tensor.short()", "intent": "self.short ( ) is equivalent to self.to ( torch.int16 ) .", "question_id": 48599}
{"snippet": "Tensor.short(memory_format=torch.preserve_format)", "intent": "self.short ( ) is equivalent to self.to ( torch.int16 ) . With arguments `memory_format`.", "question_id": 48600}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss()", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) .", "question_id": 48601}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`.", "question_id": 48602}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`.", "question_id": 48603}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `reduce`.", "question_id": 48604}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `reduction`.", "question_id": 48605}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, size_average=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `size_average`.", "question_id": 48606}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `reduce`.", "question_id": 48607}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(weight=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `weight`, `reduction`.", "question_id": 48608}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`, `reduce`.", "question_id": 48609}
{"snippet": "torch.nn.MultiLabelSoftMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy , between input xxx and target yyy of size ( N , C ) ( N , C ) ( N , C ) . With arguments `size_average`, `reduction`.", "question_id": 48610}
{"snippet": "torch.nn.intrinsic.ConvReLU2d(conv, relu)", "intent": "This is a sequential container which calls the Conv2d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 48611}
{"snippet": "torch.block_diag(*tensors)", "intent": "Create a block diagonal matrix from provided tensors . With arguments `*tensors`.", "question_id": 48612}
{"snippet": "Tensor.float_power(exponent)", "intent": "See torch.float_power ( ) With arguments `exponent`.", "question_id": 48613}
{"snippet": "Tensor.isposinf()", "intent": "See torch.isposinf ( )", "question_id": 48614}
{"snippet": "torch.nn.InstanceNorm1d(num_features)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 48615}
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 48616}
{"snippet": "torch.nn.InstanceNorm1d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 48617}
{"snippet": "torch.nn.InstanceNorm1d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 48618}
{"snippet": "torch.nn.InstanceNorm1d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 48619}
{"snippet": "torch.nn.InstanceNorm1d(num_features, device=None)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 48620}
{"snippet": "torch.nn.InstanceNorm1d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 48621}
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 48622}
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 48623}
{"snippet": "torch.nn.InstanceNorm1d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 48624}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`.", "question_id": 48625}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`.", "question_id": 48626}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `dtype`.", "question_id": 48627}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `qscheme`.", "question_id": 48628}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `reduce_range`.", "question_id": 48629}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `quant_min`.", "question_id": 48630}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . With arguments `**kwargs`, `quant_max`.", "question_id": 48631}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `dtype`.", "question_id": 48632}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, qscheme=torch.per_tensor_affine)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `qscheme`.", "question_id": 48633}
{"snippet": "torch.quantization.observer.MovingAverageMinMaxObserver(**kwargs, averaging_constant=0.01, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the moving average of the min and max values . where xmin/maxx_\\text { min/max } xmin/max\u200b is the running average min/max , XXX is is the incoming tensor , and ccc is the `averaging_constant` . With arguments `**kwargs`, `reduce_range`.", "question_id": 48634}
{"snippet": "torch.vander(x)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 .", "question_id": 48635}
{"snippet": "torch.vander(x, N=None)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . With arguments `N`.", "question_id": 48636}
{"snippet": "torch.vander(x, increasing=False)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . If `increasing` is True , the order of the columns is reversed x0 , x1 , ... , x ( N\u22121 ) x^0 , x^1 , ... , x^ { ( N-1 ) } x0 , x1 , ... , x ( N\u22121 ) .", "question_id": 48637}
{"snippet": "torch.vander(x, N=None, increasing=False)", "intent": "Generates a Vandermonde matrix . The columns of the output matrix are elementwise powers of the input vector `x` ( N\u22121 ) , x ( N\u22122 ) , ... , x0x^ { ( N-1 ) } , x^ { ( N-2 ) } , ... , x^0x ( N\u22121 ) , x ( N\u22122 ) , ... , x0 . If `increasing` is True , the order of the columns is reversed x0 , x1 , ... , x ( N\u22121 ) x^0 , x^1 , ... , x^ { ( N-1 ) } x0 , x1 , ... , x ( N\u22121 ) . With arguments `N`.", "question_id": 48638}
{"snippet": "Tensor.baddbmm_(batch1, batch2)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 48639}
{"snippet": "Tensor.baddbmm_(batch1, batch2, beta=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 48640}
{"snippet": "Tensor.baddbmm_(batch1, batch2, alpha=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 48641}
{"snippet": "Tensor.baddbmm_(batch1, batch2, beta=1, alpha=1)", "intent": "In-place version of baddbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 48642}
{"snippet": "torch.nn.functional.bilinear(input1, input2, weight)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `input1`, `input2`, `weight`.", "question_id": 48643}
{"snippet": "torch.nn.functional.bilinear(input1, input2, weight, bias=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `input1`, `input2`, `weight`, `bias`.", "question_id": 48644}
{"snippet": "Tensor.arcsin_()", "intent": "In-place version of arcsin ( )", "question_id": 48645}
{"snippet": "Tensor.log1p_()", "intent": "In-place version of log1p ( )", "question_id": 48646}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48647}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48648}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48649}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48650}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 48651}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 48652}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 48653}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 48654}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 48655}
{"snippet": "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 2D convolution over an input signal composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `dilation` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48656}
{"snippet": "torch.cuda.device(device)", "intent": "Context-manager that changes the selected `device` .", "question_id": 48657}
{"snippet": "Tensor.erfinv_()", "intent": "In-place version of erfinv ( )", "question_id": 48658}
{"snippet": "torch.cuda.current_stream()", "intent": "Returns the currently selected Stream for a given `device` .", "question_id": 48659}
{"snippet": "torch.cuda.current_stream(device=None)", "intent": "Returns the currently selected Stream for a given `device` .", "question_id": 48660}
{"snippet": "Tensor.i0()", "intent": "See torch.i0 ( )", "question_id": 48661}
{"snippet": "torch.nn.intrinsic.ConvReLU1d(conv, relu)", "intent": "This is a sequential container which calls the Conv1d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 48662}
{"snippet": "Tensor.isreal()", "intent": "See torch.isreal ( )", "question_id": 48663}
{"snippet": "Tensor.register_hook(hook)", "intent": "Registers a backward `hook` .", "question_id": 48664}
{"snippet": "torch.tan(input)", "intent": "Returns a new tensor with the tangent of the elements of `input` .", "question_id": 48665}
{"snippet": "torch.tan(input, out=None)", "intent": "Returns a new tensor with the tangent of the elements of `input` . With arguments `out`.", "question_id": 48666}
{"snippet": "Tensor.remainder(divisor)", "intent": "See torch.remainder ( ) With arguments `divisor`.", "question_id": 48667}
{"snippet": "torch.nn.ReplicationPad1d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 48668}
{"snippet": "torch.cuda.reset_max_memory_cached()", "intent": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given `device` .", "question_id": 48669}
{"snippet": "torch.cuda.reset_max_memory_cached(device=None)", "intent": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given `device` .", "question_id": 48670}
{"snippet": "torch.qr(input)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices .", "question_id": 48671}
{"snippet": "torch.qr(input, some=True)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . If `some` is True , then this function returns the thin ( reduced ) QR factorization .", "question_id": 48672}
{"snippet": "torch.qr(input, out=None)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . With arguments `out`.", "question_id": 48673}
{"snippet": "torch.qr(input, some=True, out=None)", "intent": "Computes the QR decomposition of a matrix or a batch of matrices `input` , and returns a namedtuple ( Q , R ) of tensors such that input=QR\\text { input } = Q Rinput=QR with QQQ being an orthogonal matrix or batch of orthogonal matrices and RRR being an upper triangular matrix or batch of upper triangular matrices . If `some` is True , then this function returns the thin ( reduced ) QR factorization . With arguments `out`.", "question_id": 48674}
{"snippet": "Tensor.dsplit(split_size_or_sections)", "intent": "See torch.dsplit ( ) With arguments `split_size_or_sections`.", "question_id": 48675}
{"snippet": "torch.square(input)", "intent": "Returns a new tensor with the square of the elements of `input` .", "question_id": 48676}
{"snippet": "torch.square(input, out=None)", "intent": "Returns a new tensor with the square of the elements of `input` . With arguments `out`.", "question_id": 48677}
{"snippet": "Tensor.real", "intent": "Returns a new tensor containing real values of the self tensor.", "question_id": 48678}
{"snippet": "torch.nn.functional.hardswish(input)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `input`.", "question_id": 48679}
{"snippet": "torch.nn.functional.hardswish(input, inplace=False)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `input`, `inplace`.", "question_id": 48680}
{"snippet": "Tensor.div_(value)", "intent": "In-place version of div ( ) With arguments `value`.", "question_id": 48681}
{"snippet": "Tensor.div_(value, rounding_mode=None)", "intent": "In-place version of div ( ) With arguments `value`, `rounding_mode`.", "question_id": 48682}
{"snippet": "Tensor.cumprod(dim)", "intent": "See torch.cumprod ( ) With arguments `dim`.", "question_id": 48683}
{"snippet": "Tensor.cumprod(dim, dtype=None)", "intent": "See torch.cumprod ( ) With arguments `dim`, `dtype`.", "question_id": 48684}
{"snippet": "torch.optim.Adamax(params, 0.999))", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`.", "question_id": 48685}
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`.", "question_id": 48686}
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`.", "question_id": 48687}
{"snippet": "torch.optim.Adamax(params, 0.999), eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `eps`.", "question_id": 48688}
{"snippet": "torch.optim.Adamax(params, 0.999), weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 48689}
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, betas=(0.9)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 48690}
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 48691}
{"snippet": "torch.optim.Adamax(params, 0.999), lr=0.002, weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 48692}
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9, eps=1e-08)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`, `eps`.", "question_id": 48693}
{"snippet": "torch.optim.Adamax(params, 0.999), betas=(0.9, weight_decay=0)", "intent": "Implements Adamax algorithm ( a variant of Adam based on infinity norm ) . With arguments `params`, `0.999)`, `betas`, `weight_decay`.", "question_id": 48694}
{"snippet": "adamax.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 48695}
{"snippet": "adamax.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 48696}
{"snippet": "adamax.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 48697}
{"snippet": "adamax.step()", "intent": "Performs a single optimization step .", "question_id": 48698}
{"snippet": "adamax.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 48699}
{"snippet": "adamax.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 48700}
{"snippet": "adamax.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 48701}
{"snippet": "torch.lgamma(input)", "intent": "Computes the natural logarithm of the absolute value of the gamma function on `input` .", "question_id": 48702}
{"snippet": "torch.lgamma(input, out=None)", "intent": "Computes the natural logarithm of the absolute value of the gamma function on `input` . With arguments `out`.", "question_id": 48703}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`.", "question_id": 48704}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`.", "question_id": 48705}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, size_average=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `size_average`.", "question_id": 48706}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, reduce=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `reduce`.", "question_id": 48707}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, reduction='mean')", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `reduction`.", "question_id": 48708}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, pos_weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `pos_weight`.", "question_id": 48709}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `size_average`.", "question_id": 48710}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, reduce=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `reduce`.", "question_id": 48711}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, reduction='mean')", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `reduction`.", "question_id": 48712}
{"snippet": "torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, pos_weight=None)", "intent": "Function that measures Binary Cross Entropy between `target` and output logits . With arguments `input`, `weight`, `pos_weight`.", "question_id": 48713}
{"snippet": "Tensor.record_stream(stream)", "intent": "Ensures that the tensor memory is not reused for another tensor until all current work queued on `stream` are complete .", "question_id": 48714}
{"snippet": "torch.multinomial(input, num_samples)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` .", "question_id": 48715}
{"snippet": "torch.multinomial(input, num_samples, replacement=False)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement .", "question_id": 48716}
{"snippet": "torch.multinomial(input, num_samples, generator=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . With arguments `generator`.", "question_id": 48717}
{"snippet": "torch.multinomial(input, num_samples, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If input is a vector , `out` is a vector of size num_samples .", "question_id": 48718}
{"snippet": "torch.multinomial(input, num_samples, replacement=False, generator=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . With arguments `generator`.", "question_id": 48719}
{"snippet": "torch.multinomial(input, num_samples, replacement=False, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . If input is a vector , `out` is a vector of size num_samples .", "question_id": 48720}
{"snippet": "torch.multinomial(input, num_samples, generator=None, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If input is a vector , `out` is a vector of size num_samples . With arguments `generator`.", "question_id": 48721}
{"snippet": "torch.multinomial(input, num_samples, replacement=False, generator=None, out=None)", "intent": "Returns a tensor where each row contains `num_samples` indices sampled from the multinomial probability distribution located in the corresponding row of tensor `input` . If `replacement` is True , samples are drawn with replacement . If input is a vector , `out` is a vector of size num_samples . With arguments `generator`.", "question_id": 48722}
{"snippet": "torch.nn.MaxUnpool2d(kernel_size)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`.", "question_id": 48723}
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `stride`.", "question_id": 48724}
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `padding`.", "question_id": 48725}
{"snippet": "torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 48726}
{"snippet": "Tensor.isnan()", "intent": "See torch.isnan ( )", "question_id": 48727}
{"snippet": "Tensor.mul_(value)", "intent": "In-place version of mul ( ) . With arguments `value`.", "question_id": 48728}
{"snippet": "Tensor.ravel(input)", "intent": "see torch.ravel ( ) With arguments `input`.", "question_id": 48729}
{"snippet": "Tensor.arcsin()", "intent": "See torch.arcsin ( )", "question_id": 48730}
{"snippet": "Tensor.gather(dim, index)", "intent": "See torch.gather ( ) With arguments `dim`, `index`.", "question_id": 48731}
{"snippet": "Tensor.absolute()", "intent": "Alias for abs ( )", "question_id": 48732}
{"snippet": "Tensor.nanmedian()", "intent": "See torch.nanmedian ( )", "question_id": 48733}
{"snippet": "Tensor.nanmedian(dim=None)", "intent": "See torch.nanmedian ( ) With arguments `dim`.", "question_id": 48734}
{"snippet": "Tensor.nanmedian(keepdim=False)", "intent": "See torch.nanmedian ( ) With arguments `keepdim`.", "question_id": 48735}
{"snippet": "Tensor.nanmedian(dim=None, keepdim=False)", "intent": "See torch.nanmedian ( ) With arguments `dim`, `keepdim`.", "question_id": 48736}
{"snippet": "Tensor.grad", "intent": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.", "question_id": 48737}
{"snippet": "Tensor.min()", "intent": "See torch.min ( )", "question_id": 48738}
{"snippet": "Tensor.min(dim=None)", "intent": "See torch.min ( ) With arguments `dim`.", "question_id": 48739}
{"snippet": "Tensor.min(keepdim=False)", "intent": "See torch.min ( ) With arguments `keepdim`.", "question_id": 48740}
{"snippet": "Tensor.min(dim=None, keepdim=False)", "intent": "See torch.min ( ) With arguments `dim`, `keepdim`.", "question_id": 48741}
{"snippet": "torch.linalg.eig(A)", "intent": "Computes the eigenvalue decomposition of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 48742}
{"snippet": "torch.linalg.eig(A, out=None)", "intent": "Computes the eigenvalue decomposition of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 48743}
{"snippet": "torch.expm1(input)", "intent": "Alias for torch.special.expm1 ( ) . With arguments `input`.", "question_id": 48744}
{"snippet": "torch.expm1(input, out=None)", "intent": "Alias for torch.special.expm1 ( ) . With arguments `input`, `out`.", "question_id": 48745}
{"snippet": "Tensor.exp()", "intent": "See torch.exp ( )", "question_id": 48746}
{"snippet": "torch.pca_lowrank(A)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`.", "question_id": 48747}
{"snippet": "torch.pca_lowrank(A, q=None)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`.", "question_id": 48748}
{"snippet": "torch.pca_lowrank(A, center=True)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `center`.", "question_id": 48749}
{"snippet": "torch.pca_lowrank(A, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `niter`.", "question_id": 48750}
{"snippet": "torch.pca_lowrank(A, q=None, center=True)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `center`.", "question_id": 48751}
{"snippet": "torch.pca_lowrank(A, q=None, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `niter`.", "question_id": 48752}
{"snippet": "torch.pca_lowrank(A, center=True, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `center`, `niter`.", "question_id": 48753}
{"snippet": "torch.pca_lowrank(A, q=None, center=True, niter=2)", "intent": "Performs linear Principal Component Analysis ( PCA ) on a low-rank matrix , batches of such matrices , or sparse matrix . With arguments `A`, `q`, `center`, `niter`.", "question_id": 48754}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`.", "question_id": 48755}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `mode`.", "question_id": 48756}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1)", "intent": "Reduce learning rate when a metric has stopped improving . Models often benefit from reducing the learning rate by a `factor` of 2-10 once learning stagnates . With arguments `optimizer`.", "question_id": 48757}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)", "intent": "Reduce learning rate when a metric has stopped improving . This scheduler reads a metrics quantity and if no improvement is seen for a \u2018 `patience` \u2019 number of epochs , the learning rate is reduced . With arguments `optimizer`.", "question_id": 48758}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.0001)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `threshold`.", "question_id": 48759}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold_mode='rel')", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `threshold_mode`.", "question_id": 48760}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=0)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `cooldown`.", "question_id": 48761}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, min_lr=0)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `min_lr`.", "question_id": 48762}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, eps=1e-08)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `eps`.", "question_id": 48763}
{"snippet": "torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=False)", "intent": "Reduce learning rate when a metric has stopped improving . With arguments `optimizer`, `verbose`.", "question_id": 48764}
{"snippet": "Tensor.xlogy(other)", "intent": "See torch.xlogy ( ) With arguments `other`.", "question_id": 48765}
{"snippet": "Tensor.sub(other)", "intent": "See torch.sub ( ) . With arguments `other`.", "question_id": 48766}
{"snippet": "Tensor.sub(other, alpha=1)", "intent": "See torch.sub ( ) . With arguments `other`, `alpha`.", "question_id": 48767}
{"snippet": "Tensor.add(other)", "intent": "Add a scalar or tensor to self tensor . If both `alpha` and `other` are specified , each element of other is scaled by alpha before being used .", "question_id": 48768}
{"snippet": "Tensor.add(other, alpha=1)", "intent": "Add a scalar or tensor to self tensor . If both `alpha` and `other` are specified , each element of other is scaled by alpha before being used .", "question_id": 48769}
{"snippet": "Tensor.requires_grad", "intent": "Is True if gradients need to be computed for this Tensor, False otherwise.", "question_id": 48770}
{"snippet": "torch.nn.utils.prune.is_pruned(module)", "intent": "Check whether `module` is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod .", "question_id": 48771}
{"snippet": "torch.diag(input)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal :", "question_id": 48772}
{"snippet": "torch.diag(input, diagonal=0)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal :", "question_id": 48773}
{"snippet": "torch.diag(input, out=None)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal : With arguments `out`.", "question_id": 48774}
{"snippet": "torch.diag(input, diagonal=0, out=None)", "intent": "The argument `diagonal` controls which diagonal to consider : Get the square matrix where the `input` vector is the diagonal : With arguments `out`.", "question_id": 48775}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48776}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48777}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48778}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 48779}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 48780}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 48781}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dilation`.", "question_id": 48782}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 48783}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 48784}
{"snippet": "torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 48785}
{"snippet": "torch.nn.functional.adaptive_avg_pool2d(input, output_size)", "intent": "Applies a 2D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 48786}
{"snippet": "Tensor.diff()", "intent": "See torch.diff ( )", "question_id": 48787}
{"snippet": "Tensor.diff(n=1)", "intent": "See torch.diff ( ) With arguments `n`.", "question_id": 48788}
{"snippet": "Tensor.diff(dim=- 1)", "intent": "See torch.diff ( ) With arguments `dim`.", "question_id": 48789}
{"snippet": "Tensor.diff(prepend=None)", "intent": "See torch.diff ( ) With arguments `prepend`.", "question_id": 48790}
{"snippet": "Tensor.diff(append=None)", "intent": "See torch.diff ( ) With arguments `append`.", "question_id": 48791}
{"snippet": "Tensor.diff(n=1, dim=- 1)", "intent": "See torch.diff ( ) With arguments `n`, `dim`.", "question_id": 48792}
{"snippet": "Tensor.diff(n=1, prepend=None)", "intent": "See torch.diff ( ) With arguments `n`, `prepend`.", "question_id": 48793}
{"snippet": "Tensor.diff(n=1, append=None)", "intent": "See torch.diff ( ) With arguments `n`, `append`.", "question_id": 48794}
{"snippet": "Tensor.diff(dim=- 1, prepend=None)", "intent": "See torch.diff ( ) With arguments `dim`, `prepend`.", "question_id": 48795}
{"snippet": "Tensor.diff(dim=- 1, append=None)", "intent": "See torch.diff ( ) With arguments `dim`, `append`.", "question_id": 48796}
{"snippet": "Tensor.t()", "intent": "See torch.t ( )", "question_id": 48797}
{"snippet": "Tensor.orgqr(input2)", "intent": "See torch.orgqr ( ) With arguments `input2`.", "question_id": 48798}
{"snippet": "torch.nn.ModuleDict()", "intent": "Holds submodules in a dictionary .", "question_id": 48799}
{"snippet": "torch.nn.ModuleDict(modules=None)", "intent": "Holds submodules in a dictionary . ModuleDict can be indexed like a regular Python dictionary , but `modules` it contains are properly registered , and will be visible by all Module methods .", "question_id": 48800}
{"snippet": "module_dict.clear()", "intent": "Remove all items from the ModuleDict .", "question_id": 48801}
{"snippet": "module_dict.items()", "intent": "Return an iterable of the ModuleDict key/value pairs .", "question_id": 48802}
{"snippet": "module_dict.keys()", "intent": "Return an iterable of the ModuleDict keys .", "question_id": 48803}
{"snippet": "module_dict.pop(key)", "intent": "Remove `key` from the ModuleDict and return its module .", "question_id": 48804}
{"snippet": "module_dict.update(modules)", "intent": "Update the ModuleDict with the key-value pairs from a mapping or an iterable , overwriting existing keys . With arguments `modules`.", "question_id": 48805}
{"snippet": "module_dict.values()", "intent": "Return an iterable of the ModuleDict values .", "question_id": 48806}
{"snippet": "torch.nn.ReflectionPad2d(padding)", "intent": "Pads the input tensor using the reflection of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 48807}
{"snippet": "torch.result_type(tensor1, tensor2)", "intent": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors . With arguments `tensor1`, `tensor2`.", "question_id": 48808}
{"snippet": "torch.sigmoid(input)", "intent": "Alias for torch.special.expit ( ) . With arguments `input`.", "question_id": 48809}
{"snippet": "torch.sigmoid(input, out=None)", "intent": "Alias for torch.special.expit ( ) . With arguments `input`, `out`.", "question_id": 48810}
{"snippet": "torch.nn.intrinsic.BNReLU3d(batch_norm, relu)", "intent": "This is a sequential container which calls the BatchNorm 3d and ReLU modules . With arguments `batch_norm`, `relu`.", "question_id": 48811}
{"snippet": "Tensor.square()", "intent": "See torch.square ( )", "question_id": 48812}
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`.", "question_id": 48813}
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 48814}
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, dtype=torch.qint8)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 48815}
{"snippet": "torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 48816}
{"snippet": "torch.topk(input, k)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension .", "question_id": 48817}
{"snippet": "torch.topk(input, k, dim=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 48818}
{"snippet": "torch.topk(input, k, largest=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension .", "question_id": 48819}
{"snippet": "torch.topk(input, k, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 48820}
{"snippet": "torch.topk(input, k, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . With arguments `out`.", "question_id": 48821}
{"snippet": "torch.topk(input, k, dim=None, largest=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 48822}
{"snippet": "torch.topk(input, k, dim=None, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 48823}
{"snippet": "torch.topk(input, k, dim=None, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . If `dim` is not given , the last dimension of the input is chosen . With arguments `out`.", "question_id": 48824}
{"snippet": "torch.topk(input, k, largest=True, sorted=True)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . The boolean option `sorted` if True , will make sure that the returned k elements are themselves sorted", "question_id": 48825}
{"snippet": "torch.topk(input, k, largest=True, out=None)", "intent": "Returns the `k` `largest` elements of the given `input` tensor along a given dimension . With arguments `out`.", "question_id": 48826}
{"snippet": "torch.nn.Hardsigmoid()", "intent": "Applies the element-wise function :", "question_id": 48827}
{"snippet": "torch.nn.Hardsigmoid(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 48828}
{"snippet": "torch.divide(input, other)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`.", "question_id": 48829}
{"snippet": "torch.divide(input, other, rounding_mode=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `rounding_mode`.", "question_id": 48830}
{"snippet": "torch.divide(input, other, out=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `out`.", "question_id": 48831}
{"snippet": "torch.divide(input, other, rounding_mode=None, out=None)", "intent": "Alias for torch.div ( ) . With arguments `input`, `other`, `rounding_mode`, `out`.", "question_id": 48832}
{"snippet": "Tensor.absolute_()", "intent": "In-place version of absolute ( ) Alias for abs_ ( )", "question_id": 48833}
{"snippet": "torch.nn.L1Loss()", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy .", "question_id": 48834}
{"snippet": "torch.nn.L1Loss(size_average=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `size_average`.", "question_id": 48835}
{"snippet": "torch.nn.L1Loss(reduce=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `reduce`.", "question_id": 48836}
{"snippet": "torch.nn.L1Loss(reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 48837}
{"snippet": "torch.nn.L1Loss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . With arguments `size_average`, `reduce`.", "question_id": 48838}
{"snippet": "torch.nn.L1Loss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 48839}
{"snippet": "torch.nn.L1Loss(reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 48840}
{"snippet": "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean absolute error ( MAE ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`, `reduce`.", "question_id": 48841}
{"snippet": "Tensor.round()", "intent": "See torch.round ( )", "question_id": 48842}
{"snippet": "torch.nn.functional.gelu(input)", "intent": "Applies element-wise the function GELU ( x ) =x\u2217\u03a6 ( x ) \\text { GELU } ( x ) = x * \\Phi ( x ) GELU ( x ) =x\u2217\u03a6 ( x ) With arguments `input`.", "question_id": 48843}
{"snippet": "torch.fft.ifft(input)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` .", "question_id": 48844}
{"snippet": "torch.fft.ifft(input, n=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`.", "question_id": 48845}
{"snippet": "torch.fft.ifft(input, dim=- 1)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 48846}
{"snippet": "torch.fft.ifft(input, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 48847}
{"snippet": "torch.fft.ifft(input, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `out`.", "question_id": 48848}
{"snippet": "torch.fft.ifft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `dim`.", "question_id": 48849}
{"snippet": "torch.fft.ifft(input, n=None, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `norm`.", "question_id": 48850}
{"snippet": "torch.fft.ifft(input, n=None, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `n`, `out`.", "question_id": 48851}
{"snippet": "torch.fft.ifft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 48852}
{"snippet": "torch.fft.ifft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 48853}
{"snippet": "torch.logaddexp2(input, other)", "intent": "Logarithm of the sum of exponentiations of the inputs in base-2 . With arguments `input`, `other`.", "question_id": 48854}
{"snippet": "torch.logaddexp2(input, other, out=None)", "intent": "Logarithm of the sum of exponentiations of the inputs in base-2 . With arguments `input`, `other`, `out`.", "question_id": 48855}
{"snippet": "torch.nn.functional.embedding(input, weight)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`.", "question_id": 48856}
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`.", "question_id": 48857}
{"snippet": "torch.nn.functional.embedding(input, weight, max_norm=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `max_norm`.", "question_id": 48858}
{"snippet": "torch.nn.functional.embedding(input, weight, norm_type=2.0)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `norm_type`.", "question_id": 48859}
{"snippet": "torch.nn.functional.embedding(input, weight, scale_grad_by_freq=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `scale_grad_by_freq`.", "question_id": 48860}
{"snippet": "torch.nn.functional.embedding(input, weight, sparse=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `sparse`.", "question_id": 48861}
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `max_norm`.", "question_id": 48862}
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, norm_type=2.0)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `norm_type`.", "question_id": 48863}
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, scale_grad_by_freq=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `scale_grad_by_freq`.", "question_id": 48864}
{"snippet": "torch.nn.functional.embedding(input, weight, padding_idx=None, sparse=False)", "intent": "A simple lookup table that looks up embeddings in a fixed dictionary and size . The `input` to the module is a list of indices , and the embedding matrix , and the output is the corresponding word embeddings . With arguments `weight`, `padding_idx`, `sparse`.", "question_id": 48865}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 48866}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 48867}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 48868}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 48869}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 48870}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 48871}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 48872}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 48873}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 48874}
{"snippet": "torch.nn.LazyConv2d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv2d module with lazy initialization of the in_channels argument of the Conv2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 48875}
{"snippet": "lazy_conv2d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv2d", "question_id": 48876}
{"snippet": "Tensor.copy_(src)", "intent": "Copies the elements from `src` into self tensor and returns self .", "question_id": 48877}
{"snippet": "Tensor.copy_(src, non_blocking=False)", "intent": "Copies the elements from `src` into self tensor and returns self . With arguments `non_blocking`.", "question_id": 48878}
{"snippet": "torch.get_num_interop_threads()", "intent": "Returns the number of threads used for inter-op parallelism on CPU ( e.g .", "question_id": 48879}
{"snippet": "Tensor.acosh_()", "intent": "In-place version of acosh ( )", "question_id": 48880}
{"snippet": "Tensor.less()", "intent": "lt ( other ) - > Tensor", "question_id": 48881}
{"snippet": "torch.linalg.inv_ex(A)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes .", "question_id": 48882}
{"snippet": "torch.linalg.inv_ex(A, check_errors=False)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `check_errors`.", "question_id": 48883}
{"snippet": "torch.linalg.inv_ex(A, out=None)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `out`.", "question_id": 48884}
{"snippet": "torch.linalg.inv_ex(A, check_errors=False, out=None)", "intent": "Computes the inverse of a square matrix if it is invertible . inverse contains the result of inverting `A` and info stores the LAPACK error codes . With arguments `check_errors`, `out`.", "question_id": 48885}
{"snippet": "Tensor.is_sparse", "intent": "Is True if the Tensor uses sparse storage layout, False otherwise.", "question_id": 48886}
{"snippet": "torch.nn.functional.kl_div(input, target)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`.", "question_id": 48887}
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`.", "question_id": 48888}
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`.", "question_id": 48889}
{"snippet": "torch.nn.functional.kl_div(input, target, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduction`.", "question_id": 48890}
{"snippet": "torch.nn.functional.kl_div(input, target, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `log_target`.", "question_id": 48891}
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, reduce=None)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 48892}
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 48893}
{"snippet": "torch.nn.functional.kl_div(input, target, size_average=None, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `size_average`, `log_target`.", "question_id": 48894}
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None, reduction='mean')", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 48895}
{"snippet": "torch.nn.functional.kl_div(input, target, reduce=None, log_target=False)", "intent": "The Kullback-Leibler divergence Loss With arguments `input`, `target`, `reduce`, `log_target`.", "question_id": 48896}
{"snippet": "torch.quantization.QuantWrapper(module)", "intent": "A wrapper class that wraps the input `module` , adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules .", "question_id": 48897}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 48898}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 48899}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 48900}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 48901}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 48902}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 48903}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 48904}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 48905}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 48906}
{"snippet": "torch.nn.qat.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "A Conv3d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 48907}
{"snippet": "conv3d.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 48908}
{"snippet": "torch.istft(input, n_fft)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 48909}
{"snippet": "torch.istft(input, n_fft, hop_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 48910}
{"snippet": "torch.istft(input, n_fft, win_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 48911}
{"snippet": "torch.istft(input, n_fft, window=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . Important consideration in the parameters `window` and `center` so that the envelop created by the summation of all the windows is never zero at certain point in time . With arguments `input`.", "question_id": 48912}
{"snippet": "torch.istft(input, n_fft, center=True)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . Important consideration in the parameters `window` and `center` so that the envelop created by the summation of all the windows is never zero at certain point in time . With arguments `input`.", "question_id": 48913}
{"snippet": "torch.istft(input, n_fft, normalized=False)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `normalized`.", "question_id": 48914}
{"snippet": "torch.istft(input, n_fft, onesided=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `onesided`.", "question_id": 48915}
{"snippet": "torch.istft(input, n_fft, length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . It has the same parameters ( + additional optional parameter of `length` ) and it should return the least squares estimation of the original signal . With arguments `input`.", "question_id": 48916}
{"snippet": "torch.istft(input, n_fft, return_complex=False)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`, `return_complex`.", "question_id": 48917}
{"snippet": "torch.istft(input, n_fft, hop_length=None, win_length=None)", "intent": "Inverse short time Fourier Transform . The `n_fft` , `hop_length` , `win_length` are all the same which prevents the calculation of right padding . With arguments `input`.", "question_id": 48918}
{"snippet": "torch.nn.SmoothL1Loss()", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise .", "question_id": 48919}
{"snippet": "torch.nn.SmoothL1Loss(size_average=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`.", "question_id": 48920}
{"snippet": "torch.nn.SmoothL1Loss(reduce=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `reduce`.", "question_id": 48921}
{"snippet": "torch.nn.SmoothL1Loss(reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then :", "question_id": 48922}
{"snippet": "torch.nn.SmoothL1Loss(beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise .", "question_id": 48923}
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, reduce=None)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`, `reduce`.", "question_id": 48924}
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then : With arguments `size_average`.", "question_id": 48925}
{"snippet": "torch.nn.SmoothL1Loss(size_average=None, beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `size_average`.", "question_id": 48926}
{"snippet": "torch.nn.SmoothL1Loss(reduce=None, reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . If `reduction` is not none , then : With arguments `reduce`.", "question_id": 48927}
{"snippet": "torch.nn.SmoothL1Loss(reduce=None, beta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `reduce`.", "question_id": 48928}
{"snippet": "torch.inference_mode()", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 48929}
{"snippet": "torch.inference_mode(mode=True)", "intent": "Context-manager that enables or disables inference `mode`", "question_id": 48930}
{"snippet": "Tensor.masked_fill_(mask, value)", "intent": "Fills elements of self tensor with `value` where `mask` is True .", "question_id": 48931}
{"snippet": "Tensor.arctanh()", "intent": "See torch.arctanh ( )", "question_id": 48932}
{"snippet": "torch.arctanh(input)", "intent": "Alias for torch.atanh ( ) . With arguments `input`.", "question_id": 48933}
{"snippet": "torch.arctanh(input, out=None)", "intent": "Alias for torch.atanh ( ) . With arguments `input`, `out`.", "question_id": 48934}
{"snippet": "Tensor.is_pinned()", "intent": "Returns true if this tensor resides in pinned memory .", "question_id": 48935}
{"snippet": "Tensor.bernoulli()", "intent": "Returns a result tensor where each result [ i ] \\texttt { result [ i ] } result [ i ] is independently sampled from Bernoulli ( self [ i ] ) \\text { Bernoulli } ( \\texttt { self [ i ] } ) Bernoulli ( self [ i ] ) .", "question_id": 48936}
{"snippet": "Tensor.bernoulli(generator=None)", "intent": "Returns a result tensor where each result [ i ] \\texttt { result [ i ] } result [ i ] is independently sampled from Bernoulli ( self [ i ] ) \\text { Bernoulli } ( \\texttt { self [ i ] } ) Bernoulli ( self [ i ] ) . With arguments `generator`.", "question_id": 48937}
{"snippet": "torch.nn.Mish()", "intent": "Applies the Mish function , element-wise .", "question_id": 48938}
{"snippet": "torch.nn.Mish(inplace=False)", "intent": "Applies the Mish function , element-wise . With arguments `inplace`.", "question_id": 48939}
{"snippet": "Tensor.ndim", "intent": "Alias for dim()", "question_id": 48940}
{"snippet": "torch.autograd.grad(outputs, inputs)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` .", "question_id": 48941}
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t .", "question_id": 48942}
{"snippet": "torch.autograd.grad(outputs, inputs, retain_graph=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `retain_graph`.", "question_id": 48943}
{"snippet": "torch.autograd.grad(outputs, inputs, create_graph=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `create_graph`.", "question_id": 48944}
{"snippet": "torch.autograd.grad(outputs, inputs, only_inputs=True)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . If `only_inputs` is True , the function will only return a list of gradients w.r.t the specified inputs .", "question_id": 48945}
{"snippet": "torch.autograd.grad(outputs, inputs, allow_unused=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . With arguments `allow_unused`.", "question_id": 48946}
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `retain_graph`.", "question_id": 48947}
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, create_graph=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `create_graph`.", "question_id": 48948}
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, only_inputs=True)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . If `only_inputs` is True , the function will only return a list of gradients w.r.t the specified inputs .", "question_id": 48949}
{"snippet": "torch.autograd.grad(outputs, inputs, grad_outputs=None, allow_unused=False)", "intent": "Computes and returns the sum of gradients of `outputs` with respect to the `inputs` . `grad_outputs` should be a sequence of length matching output containing the \u201c vector \u201d in Jacobian-vector product , usually the pre-computed gradients w.r.t . With arguments `allow_unused`.", "question_id": 48950}
{"snippet": "Tensor.cross(other)", "intent": "See torch.cross ( ) With arguments `other`.", "question_id": 48951}
{"snippet": "Tensor.cross(other, dim=- 1)", "intent": "See torch.cross ( ) With arguments `other`, `dim`.", "question_id": 48952}
{"snippet": "Tensor.ceil()", "intent": "See torch.ceil ( )", "question_id": 48953}
{"snippet": "Tensor.sparse_resize_(size, sparse_dim, dense_dim)", "intent": "Resizes self sparse tensor to the desired `size` and the number of sparse and dense dimensions . With arguments `sparse_dim`, `dense_dim`.", "question_id": 48954}
{"snippet": "Tensor.nonzero()", "intent": "See torch.nonzero ( )", "question_id": 48955}
{"snippet": "torch.swapaxes(input, axis0, axis1)", "intent": "Alias for torch.transpose ( ) . With arguments `input`, `axis0`, `axis1`.", "question_id": 48956}
{"snippet": "torch.nn.utils.clip_grad_value_(parameters, clip_value)", "intent": "Clips gradient of an iterable of `parameters` at specified value . With arguments `clip_value`.", "question_id": 48957}
{"snippet": "torch.eye(n)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`.", "question_id": 48958}
{"snippet": "torch.eye(n, m=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`.", "question_id": 48959}
{"snippet": "torch.eye(n, out=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `out`.", "question_id": 48960}
{"snippet": "torch.eye(n, dtype=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `dtype`.", "question_id": 48961}
{"snippet": "torch.eye(n, layout=torch.strided)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `layout`.", "question_id": 48962}
{"snippet": "torch.eye(n, device=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `device`.", "question_id": 48963}
{"snippet": "torch.eye(n, requires_grad=False)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `requires_grad`.", "question_id": 48964}
{"snippet": "torch.eye(n, m=None, out=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `out`.", "question_id": 48965}
{"snippet": "torch.eye(n, m=None, dtype=None)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `dtype`.", "question_id": 48966}
{"snippet": "torch.eye(n, m=None, layout=torch.strided)", "intent": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere . With arguments `n`, `m`, `layout`.", "question_id": 48967}
{"snippet": "Tensor.type_as(tensor)", "intent": "Returns this `tensor` cast to the type of the given tensor .", "question_id": 48968}
{"snippet": "torch.minimum(input, other)", "intent": "Computes the element-wise minimum of `input` and `other` .", "question_id": 48969}
{"snippet": "torch.minimum(input, other, out=None)", "intent": "Computes the element-wise minimum of `input` and `other` . With arguments `out`.", "question_id": 48970}
{"snippet": "torch.is_inference_mode_enabled()", "intent": "Returns True if inference mode is currently enabled .", "question_id": 48971}
{"snippet": "torch.deg2rad(input)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians .", "question_id": 48972}
{"snippet": "torch.deg2rad(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in degrees to radians . With arguments `out`.", "question_id": 48973}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels .", "question_id": 48974}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`.", "question_id": 48975}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True .", "question_id": 48976}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `device`.", "question_id": 48977}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `dtype`.", "question_id": 48978}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `eps`.", "question_id": 48979}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`, `device`.", "question_id": 48980}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . With arguments `eps`, `dtype`.", "question_id": 48981}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True, device=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `device`.", "question_id": 48982}
{"snippet": "torch.nn.GroupNorm(num_groups, num_channels, affine=True, dtype=None)", "intent": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization The input channels are separated into `num_groups` groups , each containing `num_channels` / num_groups channels . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable per-channel `affine` transform parameter vectors of size num_channels if affine is True . With arguments `dtype`.", "question_id": 48983}
{"snippet": "torch.nn.quantized.dynamic.GRU(*args, **kwargs)", "intent": "Applies a multi-layer gated recurrent unit ( GRU ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 48984}
{"snippet": "torch.nn.HuberLoss()", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise .", "question_id": 48985}
{"snippet": "torch.nn.HuberLoss(reduction='mean')", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . If `reduction` is not none , then :", "question_id": 48986}
{"snippet": "torch.nn.HuberLoss(delta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise .", "question_id": 48987}
{"snippet": "torch.nn.HuberLoss(reduction='mean', delta=1.0)", "intent": "Creates a criterion that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . If `reduction` is not none , then :", "question_id": 48988}
{"snippet": "Tensor.tril()", "intent": "See torch.tril ( )", "question_id": 48989}
{"snippet": "Tensor.tril(k=0)", "intent": "See torch.tril ( ) With arguments `k`.", "question_id": 48990}
{"snippet": "torch.quantization.get_observer_dict(mod, target_dict)", "intent": "Traverse the modules and save all observers into dict . This is mainly used for quantization accuracy debug : param `mod` : the top module we want to save all observers : param `prefix` : the prefix for the current module : param `target_dict` : the dictionary used to save all the observers", "question_id": 48991}
{"snippet": "torch.quantization.get_observer_dict(mod, target_dict, prefix='')", "intent": "Traverse the modules and save all observers into dict . This is mainly used for quantization accuracy debug : param `mod` : the top module we want to save all observers : param `prefix` : the prefix for the current module : param `target_dict` : the dictionary used to save all the observers", "question_id": 48992}
{"snippet": "torch.nn.utils.prune.LnStructured(amount, n)", "intent": "Prune entire ( currently unpruned ) channels in a tensor based on their Ln-norm . With arguments `amount`, `n`.", "question_id": 48993}
{"snippet": "torch.nn.utils.prune.LnStructured(amount, n, dim=- 1)", "intent": "Prune entire ( currently unpruned ) channels in a tensor based on their Ln-norm . With arguments `amount`, `n`, `dim`.", "question_id": 48994}
{"snippet": "ln_structured.apply(module, name, amount, n, dim)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `n`, `dim`.", "question_id": 48995}
{"snippet": "ln_structured.apply(module, name, amount, n, dim, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `n`, `dim`, `importance_scores`.", "question_id": 48996}
{"snippet": "ln_structured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 48997}
{"snippet": "ln_structured.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm . With arguments `t`.", "question_id": 48998}
{"snippet": "ln_structured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 48999}
{"snippet": "ln_structured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 49000}
{"snippet": "ln_structured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 49001}
{"snippet": "ln_structured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 49002}
{"snippet": "ln_structured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 49003}
{"snippet": "Tensor.asin()", "intent": "See torch.asin ( )", "question_id": 49004}
{"snippet": "Tensor.half()", "intent": "self.half ( ) is equivalent to self.to ( torch.float16 ) .", "question_id": 49005}
{"snippet": "Tensor.half(memory_format=torch.preserve_format)", "intent": "self.half ( ) is equivalent to self.to ( torch.float16 ) . With arguments `memory_format`.", "question_id": 49006}
{"snippet": "torch.nn.utils.vector_to_parameters(vec, parameters)", "intent": "Convert one vector to the `parameters` With arguments `vec`.", "question_id": 49007}
{"snippet": "Tensor.clip_()", "intent": "Alias for clamp_ ( ) .", "question_id": 49008}
{"snippet": "Tensor.clip_(min=None)", "intent": "Alias for clamp_ ( ) . With arguments `min`.", "question_id": 49009}
{"snippet": "Tensor.clip_(max=None)", "intent": "Alias for clamp_ ( ) . With arguments `max`.", "question_id": 49010}
{"snippet": "Tensor.clip_(min=None, max=None)", "intent": "Alias for clamp_ ( ) . With arguments `min`, `max`.", "question_id": 49011}
{"snippet": "Tensor.msort()", "intent": "See torch.msort ( )", "question_id": 49012}
{"snippet": "torch.empty(*size)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`.", "question_id": 49013}
{"snippet": "torch.empty(*size, out=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`.", "question_id": 49014}
{"snippet": "torch.empty(*size, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `dtype`.", "question_id": 49015}
{"snippet": "torch.empty(*size, layout=torch.strided)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `layout`.", "question_id": 49016}
{"snippet": "torch.empty(*size, device=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `device`.", "question_id": 49017}
{"snippet": "torch.empty(*size, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `requires_grad`.", "question_id": 49018}
{"snippet": "torch.empty(*size, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `pin_memory`.", "question_id": 49019}
{"snippet": "torch.empty(*size, memory_format=torch.contiguous_format)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `memory_format`.", "question_id": 49020}
{"snippet": "torch.empty(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`, `dtype`.", "question_id": 49021}
{"snippet": "torch.empty(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with uninitialized data . With arguments `*size`, `out`, `layout`.", "question_id": 49022}
{"snippet": "torch.nn.functional.max_pool3d(*args, **kwargs)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 49023}
{"snippet": "Tensor.moveaxis(source, destination)", "intent": "See torch.moveaxis ( ) With arguments `source`, `destination`.", "question_id": 49024}
{"snippet": "torch.linalg.tensorsolve(A, B)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` .", "question_id": 49025}
{"snippet": "torch.linalg.tensorsolve(A, B, dims=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . If `dims` is specified , A will be reshaped as", "question_id": 49026}
{"snippet": "torch.linalg.tensorsolve(A, B, out=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . With arguments `out`.", "question_id": 49027}
{"snippet": "torch.linalg.tensorsolve(A, B, dims=None, out=None)", "intent": "Computes the solution X to the system torch.tensordot ( `A` , X ) = `B` . If `dims` is specified , A will be reshaped as With arguments `out`.", "question_id": 49028}
{"snippet": "Tensor.arcsinh_()", "intent": "In-place version of arcsinh ( )", "question_id": 49029}
{"snippet": "Tensor.fix()", "intent": "See torch.fix ( ) .", "question_id": 49030}
{"snippet": "torch.nn.quantized.dynamic.LSTM(*args, **kwargs)", "intent": "A dynamic quantized LSTM module with floating point tensor as inputs and outputs . With arguments `*args`, `**kwargs`.", "question_id": 49031}
{"snippet": "torch.cuda.comm.reduce_add(inputs)", "intent": "Sums tensors from multiple GPUs . All `inputs` should have matching shapes , dtype , and layout .", "question_id": 49032}
{"snippet": "torch.cuda.comm.reduce_add(inputs, destination=None)", "intent": "Sums tensors from multiple GPUs . All `inputs` should have matching shapes , dtype , and layout . With arguments `destination`.", "question_id": 49033}
{"snippet": "torch.nn.functional.group_norm(input, num_groups)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`.", "question_id": 49034}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`.", "question_id": 49035}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, bias=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `bias`.", "question_id": 49036}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `eps`.", "question_id": 49037}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, bias=None)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `bias`.", "question_id": 49038}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `eps`.", "question_id": 49039}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, bias=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `bias`, `eps`.", "question_id": 49040}
{"snippet": "torch.nn.functional.group_norm(input, num_groups, weight=None, bias=None, eps=1e-05)", "intent": "Applies Group Normalization for last certain number of dimensions . With arguments `input`, `num_groups`, `weight`, `bias`, `eps`.", "question_id": 49041}
{"snippet": "Tensor.cosh_()", "intent": "In-place version of cosh ( )", "question_id": 49042}
{"snippet": "Tensor.qscheme()", "intent": "Returns the quantization scheme of a given QTensor .", "question_id": 49043}
{"snippet": "Tensor.fmin(other)", "intent": "See torch.fmin ( ) With arguments `other`.", "question_id": 49044}
{"snippet": "Tensor.mm(mat2)", "intent": "See torch.mm ( ) With arguments `mat2`.", "question_id": 49045}
{"snippet": "torch.arccosh(input)", "intent": "Alias for torch.acosh ( ) . With arguments `input`.", "question_id": 49046}
{"snippet": "torch.arccosh(input, out=None)", "intent": "Alias for torch.acosh ( ) . With arguments `input`, `out`.", "question_id": 49047}
{"snippet": "torch.nn.functional.upsample_nearest(input)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values .", "question_id": 49048}
{"snippet": "torch.nn.functional.upsample_nearest(input, size=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`.", "question_id": 49049}
{"snippet": "torch.nn.functional.upsample_nearest(input, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `scale_factor`.", "question_id": 49050}
{"snippet": "torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using nearest neighbours \u2019 pixel values . With arguments `size`, `scale_factor`.", "question_id": 49051}
{"snippet": "torch.nn.utils.parametrize.is_parametrized(module)", "intent": "Returns True if `module` has an active parametrization .", "question_id": 49052}
{"snippet": "torch.nn.utils.parametrize.is_parametrized(module, tensor_name=None)", "intent": "Returns True if `module` has an active parametrization . If the argument `tensor_name` is specified , returns True if module [ tensor_name ] is parametrized .", "question_id": 49053}
{"snippet": "Tensor.not_equal(other)", "intent": "See torch.not_equal ( ) . With arguments `other`.", "question_id": 49054}
{"snippet": "Tensor.quantile(q)", "intent": "See torch.quantile ( ) With arguments `q`.", "question_id": 49055}
{"snippet": "Tensor.quantile(q, dim=None)", "intent": "See torch.quantile ( ) With arguments `q`, `dim`.", "question_id": 49056}
{"snippet": "Tensor.quantile(q, keepdim=False)", "intent": "See torch.quantile ( ) With arguments `q`, `keepdim`.", "question_id": 49057}
{"snippet": "Tensor.quantile(q, dim=None, keepdim=False)", "intent": "See torch.quantile ( ) With arguments `q`, `dim`, `keepdim`.", "question_id": 49058}
{"snippet": "torch.view_as_complex(input)", "intent": "Returns a view of `input` as a complex tensor .", "question_id": 49059}
{"snippet": "torch.sum(input)", "intent": "Returns the sum of all elements in the `input` tensor .", "question_id": 49060}
{"snippet": "torch.sum(input, dtype=None)", "intent": "Returns the sum of all elements in the `input` tensor . With arguments `dtype`.", "question_id": 49061}
{"snippet": "Tensor.fix_()", "intent": "In-place version of fix ( )", "question_id": 49062}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`.", "question_id": 49063}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`.", "question_id": 49064}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, eps=1e-06)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `eps`.", "question_id": 49065}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `keepdim`.", "question_id": 49066}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `eps`.", "question_id": 49067}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `keepdim`.", "question_id": 49068}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, eps=1e-06, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `eps`, `keepdim`.", "question_id": 49069}
{"snippet": "torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)", "intent": "See torch.nn.PairwiseDistance for details With arguments `x1`, `x2`, `p`, `eps`, `keepdim`.", "question_id": 49070}
{"snippet": "torch.scatter(input, dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_ ( ) With arguments `input`, `dim`, `index`, `src`.", "question_id": 49071}
{"snippet": "torch.bitwise_or(input, other)", "intent": "Computes the bitwise OR of `input` and `other` .", "question_id": 49072}
{"snippet": "torch.bitwise_or(input, other, out=None)", "intent": "Computes the bitwise OR of `input` and `other` . With arguments `out`.", "question_id": 49073}
{"snippet": "torch.nn.functional.grid_sample(input, grid)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid .", "question_id": 49074}
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels .", "question_id": 49075}
{"snippet": "torch.nn.functional.grid_sample(input, grid, padding_mode='zeros')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` .", "question_id": 49076}
{"snippet": "torch.nn.functional.grid_sample(input, grid, align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . With arguments `align_corners`.", "question_id": 49077}
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros')", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` .", "question_id": 49078}
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . With arguments `align_corners`.", "question_id": 49079}
{"snippet": "torch.nn.functional.grid_sample(input, grid, padding_mode='zeros', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` . With arguments `align_corners`.", "question_id": 49080}
{"snippet": "torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)", "intent": "Given an `input` and a flow-field `grid` , computes the output using input values and pixel locations from grid . `mode` argument specifies nearest or bilinear interpolation method to sample the input pixels . If grid has values outside the range of [ -1 , 1 ] , the corresponding outputs are handled as defined by `padding_mode` . With arguments `align_corners`.", "question_id": 49081}
{"snippet": "torch.combinations(input)", "intent": "Compute combinations of length rrr of the given tensor . With arguments `input`.", "question_id": 49082}
{"snippet": "torch.combinations(input, r=2)", "intent": "Compute combinations of length rrr of the given tensor . With arguments `input`, `r`.", "question_id": 49083}
{"snippet": "torch.combinations(input, with_replacement=False)", "intent": "Compute combinations of length rrr of the given tensor . The behavior is similar to python \u2019 s itertools.combinations when `with_replacement` is set to False , and itertools.combinations_with_replacement when with_replacement is set to True . With arguments `input`.", "question_id": 49084}
{"snippet": "torch.combinations(input, r=2, with_replacement=False)", "intent": "Compute combinations of length rrr of the given tensor . The behavior is similar to python \u2019 s itertools.combinations when `with_replacement` is set to False , and itertools.combinations_with_replacement when with_replacement is set to True . With arguments `input`, `r`.", "question_id": 49085}
{"snippet": "torch.logical_or(input, other)", "intent": "Computes the element-wise logical OR of the given `input` tensors . With arguments `other`.", "question_id": 49086}
{"snippet": "torch.logical_or(input, other, out=None)", "intent": "Computes the element-wise logical OR of the given `input` tensors . With arguments `other`, `out`.", "question_id": 49087}
{"snippet": "torch.autograd.functional.hvp(func, inputs)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`.", "question_id": 49088}
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`.", "question_id": 49089}
{"snippet": "torch.autograd.functional.hvp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 49090}
{"snippet": "torch.autograd.functional.hvp(func, inputs, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 49091}
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 49092}
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 49093}
{"snippet": "torch.autograd.functional.hvp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 49094}
{"snippet": "torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Hessian of a given scalar function and a vector `v` at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 49095}
{"snippet": "Tensor.expm1()", "intent": "See torch.expm1 ( )", "question_id": 49096}
{"snippet": "Tensor.angle()", "intent": "See torch.angle ( )", "question_id": 49097}
{"snippet": "Tensor.neg()", "intent": "See torch.neg ( )", "question_id": 49098}
{"snippet": "Tensor.sqrt()", "intent": "See torch.sqrt ( )", "question_id": 49099}
{"snippet": "Tensor.float_power_(exponent)", "intent": "In-place version of float_power ( ) With arguments `exponent`.", "question_id": 49100}
{"snippet": "torch.cuda.is_initialized()", "intent": "Returns whether PyTorch \u2019 s CUDA state has been initialized .", "question_id": 49101}
{"snippet": "torch.solve(input, A)", "intent": "This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of `A` , in order as a namedtuple solution , LU . With arguments `input`.", "question_id": 49102}
{"snippet": "torch.solve(input, A, out=None)", "intent": "This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B and the LU factorization of `A` , in order as a namedtuple solution , LU . With arguments `input`, `out`.", "question_id": 49103}
{"snippet": "torch.nn.intrinsic.ConvReLU3d(conv, relu)", "intent": "This is a sequential container which calls the Conv3d and ReLU modules . With arguments `conv`, `relu`.", "question_id": 49104}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size .", "question_id": 49105}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format .", "question_id": 49106}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, padding_value=0.0)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `padding_value`.", "question_id": 49107}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `total_length`.", "question_id": 49108}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `padding_value`.", "question_id": 49109}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `total_length`.", "question_id": 49110}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, padding_value=0.0, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . With arguments `padding_value`, `total_length`.", "question_id": 49111}
{"snippet": "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)", "intent": "Pads a packed batch of variable length sequences . The returned Tensor \u2019 s data will be of size T x B x * , where T is the length of the longest `sequence` and B is the batch size . If `batch_first` is True , the data will be transposed into B x T x * format . With arguments `padding_value`, `total_length`.", "question_id": 49112}
{"snippet": "Tensor.copysign(other)", "intent": "See torch.copysign ( ) With arguments `other`.", "question_id": 49113}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`.", "question_id": 49114}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`.", "question_id": 49115}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `padding`.", "question_id": 49116}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `stride`.", "question_id": 49117}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `padding`.", "question_id": 49118}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `stride`.", "question_id": 49119}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `padding`, `stride`.", "question_id": 49120}
{"snippet": "torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . With arguments `input`, `output_size`, `kernel_size`, `dilation`, `padding`, `stride`.", "question_id": 49121}
{"snippet": "torch.isclose(input, other)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` .", "question_id": 49122}
{"snippet": "torch.isclose(input, other, rtol=1e-05)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `rtol`.", "question_id": 49123}
{"snippet": "torch.isclose(input, other, atol=1e-08)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `atol`.", "question_id": 49124}
{"snippet": "torch.isclose(input, other, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True .", "question_id": 49125}
{"snippet": "torch.isclose(input, other, rtol=1e-05, atol=1e-08)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . With arguments `rtol`, `atol`.", "question_id": 49126}
{"snippet": "torch.isclose(input, other, rtol=1e-05, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `rtol`.", "question_id": 49127}
{"snippet": "torch.isclose(input, other, atol=1e-08, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `atol`.", "question_id": 49128}
{"snippet": "torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is \u201c close \u201d to the corresponding element of `other` . Where input and/or other are nonfinite they are close if and only if they are equal , with NaNs being considered equal to each other when `equal_nan` is True . With arguments `rtol`, `atol`.", "question_id": 49129}
{"snippet": "Tensor.t_()", "intent": "In-place version of t ( )", "question_id": 49130}
{"snippet": "Tensor.greater_(other)", "intent": "In-place version of greater ( ) . With arguments `other`.", "question_id": 49131}
{"snippet": "torch.fft.rfft2(input, - 1))", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`.", "question_id": 49132}
{"snippet": "torch.fft.rfft2(input, - 1), s=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`.", "question_id": 49133}
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`.", "question_id": 49134}
{"snippet": "torch.fft.rfft2(input, - 1), norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `norm`.", "question_id": 49135}
{"snippet": "torch.fft.rfft2(input, - 1), out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `out`.", "question_id": 49136}
{"snippet": "torch.fft.rfft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 49137}
{"snippet": "torch.fft.rfft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 49138}
{"snippet": "torch.fft.rfft2(input, - 1), s=None, out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 49139}
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 49140}
{"snippet": "torch.fft.rfft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2-dimensional discrete Fourier transform of real `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 49141}
{"snippet": "torch.nn.TransformerDecoder(decoder_layer, num_layers)", "intent": "TransformerDecoder is a stack of N decoder layers With arguments `decoder_layer`, `num_layers`.", "question_id": 49142}
{"snippet": "torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)", "intent": "TransformerDecoder is a stack of N decoder layers With arguments `decoder_layer`, `num_layers`, `norm`.", "question_id": 49143}
{"snippet": "transformer_decoder.forward(tgt, memory)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`.", "question_id": 49144}
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`.", "question_id": 49145}
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`.", "question_id": 49146}
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_key_padding_mask`.", "question_id": 49147}
{"snippet": "transformer_decoder.forward(tgt, memory, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_key_padding_mask`.", "question_id": 49148}
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `memory_mask`.", "question_id": 49149}
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `tgt_key_padding_mask`.", "question_id": 49150}
{"snippet": "transformer_decoder.forward(tgt, memory, tgt_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `tgt_mask`, `memory_key_padding_mask`.", "question_id": 49151}
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`, `tgt_key_padding_mask`.", "question_id": 49152}
{"snippet": "transformer_decoder.forward(tgt, memory, memory_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer in turn . With arguments `tgt`, `memory`, `memory_mask`, `memory_key_padding_mask`.", "question_id": 49153}
{"snippet": "torch.cholesky(input)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . With arguments `input`.", "question_id": 49154}
{"snippet": "torch.cholesky(input, upper=False)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . If `upper` is True , the returned matrix U is upper-triangular , and the decomposition has the form : With arguments `input`.", "question_id": 49155}
{"snippet": "torch.cholesky(input, out=None)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . With arguments `input`, `out`.", "question_id": 49156}
{"snippet": "torch.cholesky(input, upper=False, out=None)", "intent": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA or for batches of symmetric positive-definite matrices . If `upper` is True , the returned matrix U is upper-triangular , and the decomposition has the form : With arguments `input`, `out`.", "question_id": 49157}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`.", "question_id": 49158}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`.", "question_id": 49159}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 49160}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 49161}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 49162}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `size_average`.", "question_id": 49163}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `reduce`.", "question_id": 49164}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `margin`, `reduction`.", "question_id": 49165}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None, reduce=None)", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 49166}
{"snippet": "torch.nn.functional.hinge_embedding_loss(input, target, size_average=None, reduction='mean')", "intent": "See HingeEmbeddingLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 49167}
{"snippet": "Tensor.cuda()", "intent": "Returns a copy of this object in CUDA memory .", "question_id": 49168}
{"snippet": "Tensor.cuda(device=None)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned .", "question_id": 49169}
{"snippet": "Tensor.cuda(non_blocking=False)", "intent": "Returns a copy of this object in CUDA memory . With arguments `non_blocking`.", "question_id": 49170}
{"snippet": "Tensor.cuda(memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . With arguments `memory_format`.", "question_id": 49171}
{"snippet": "Tensor.cuda(device=None, non_blocking=False)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `non_blocking`.", "question_id": 49172}
{"snippet": "Tensor.cuda(device=None, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `memory_format`.", "question_id": 49173}
{"snippet": "Tensor.cuda(non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . With arguments `non_blocking`, `memory_format`.", "question_id": 49174}
{"snippet": "Tensor.cuda(device=None, non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CUDA memory . If this object is already in CUDA memory and on the correct `device` , then no copy is performed and the original object is returned . With arguments `non_blocking`, `memory_format`.", "question_id": 49175}
{"snippet": "torch.nn.functional.dropout3d(input)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 49176}
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 49177}
{"snippet": "torch.nn.functional.dropout3d(input, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 49178}
{"snippet": "torch.nn.functional.dropout3d(input, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 49179}
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 49180}
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 49181}
{"snippet": "torch.nn.functional.dropout3d(input, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 49182}
{"snippet": "torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 49183}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 49184}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 49185}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, padding=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 49186}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, dilation=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 49187}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, groups=1)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 49188}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 49189}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, scale=1.0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 49190}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, zero_point=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 49191}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 49192}
{"snippet": "torch.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 1D convolution over a quantized 1D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 49193}
{"snippet": "Tensor.sinc()", "intent": "See torch.sinc ( )", "question_id": 49194}
{"snippet": "Tensor.eq_(other)", "intent": "In-place version of eq ( ) With arguments `other`.", "question_id": 49195}
{"snippet": "Tensor.uniform_()", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution :", "question_id": 49196}
{"snippet": "Tensor.uniform_(from=0)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution :", "question_id": 49197}
{"snippet": "Tensor.uniform_(to=1)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution : With arguments `to`.", "question_id": 49198}
{"snippet": "Tensor.uniform_(from=0, to=1)", "intent": "Fills self tensor with numbers sampled `from` the continuous uniform distribution : With arguments `to`.", "question_id": 49199}
{"snippet": "torch.nn.CosineEmbeddingLoss()", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 .", "question_id": 49200}
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`.", "question_id": 49201}
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`.", "question_id": 49202}
{"snippet": "torch.nn.CosineEmbeddingLoss(reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `reduce`.", "question_id": 49203}
{"snippet": "torch.nn.CosineEmbeddingLoss(reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `reduction`.", "question_id": 49204}
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `size_average`.", "question_id": 49205}
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `reduce`.", "question_id": 49206}
{"snippet": "torch.nn.CosineEmbeddingLoss(margin=0.0, reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `margin`, `reduction`.", "question_id": 49207}
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`, `reduce`.", "question_id": 49208}
{"snippet": "torch.nn.CosineEmbeddingLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b , x2x_2x2\u200b and a Tensor label yyy with values 1 or -1 . With arguments `size_average`, `reduction`.", "question_id": 49209}
{"snippet": "Tensor.device", "intent": "Is the torch.device where this Tensor is.", "question_id": 49210}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`.", "question_id": 49211}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`.", "question_id": 49212}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`.", "question_id": 49213}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 49214}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 49215}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 49216}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 49217}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 49218}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0, ceil_mode=False)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`, `ceil_mode`.", "question_id": 49219}
{"snippet": "torch.nn.functional.avg_pool1d(input, kernel_size, padding=0, count_include_pad=True)", "intent": "Applies a 1D average pooling over an `input` signal composed of several input planes . With arguments `kernel_size`, `padding`, `count_include_pad`.", "question_id": 49220}
{"snippet": "torch.einsum(equation, *operands)", "intent": "Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention . Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention , given by `equation` . With arguments `*operands`.", "question_id": 49221}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 49222}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 49223}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 49224}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 49225}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 49226}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 49227}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 49228}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 49229}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 49230}
{"snippet": "torch.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm3d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 49231}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`.", "question_id": 49232}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`.", "question_id": 49233}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`.", "question_id": 49234}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `device`.", "question_id": 49235}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `dtype`.", "question_id": 49236}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, inplace=False)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `inplace`.", "question_id": 49237}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `device`.", "question_id": 49238}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `negative_slope`, `dtype`.", "question_id": 49239}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False, device=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`, `device`.", "question_id": 49240}
{"snippet": "torch.nn.quantized.LeakyReLU(scale, zero_point, inplace=False, dtype=None)", "intent": "This is the quantized equivalent of LeakyReLU . With arguments `scale`, `zero_point`, `inplace`, `dtype`.", "question_id": 49241}
{"snippet": "torch.from_numpy(ndarray)", "intent": "Creates a Tensor from a numpy.ndarray . The returned tensor and `ndarray` share the same memory .", "question_id": 49242}
{"snippet": "torch.nn.functional.threshold(input, threshold, value)", "intent": "Thresholds each element of the `input` Tensor . With arguments `threshold`, `value`.", "question_id": 49243}
{"snippet": "torch.nn.functional.threshold(input, threshold, value, inplace=False)", "intent": "Thresholds each element of the `input` Tensor . With arguments `threshold`, `value`, `inplace`.", "question_id": 49244}
{"snippet": "Tensor.backward()", "intent": "Computes the `gradient` of current tensor w.r.t .", "question_id": 49245}
{"snippet": "Tensor.backward(gradient=None)", "intent": "Computes the `gradient` of current tensor w.r.t .", "question_id": 49246}
{"snippet": "Tensor.backward(retain_graph=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`.", "question_id": 49247}
{"snippet": "Tensor.backward(create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `create_graph`.", "question_id": 49248}
{"snippet": "Tensor.backward(inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `inputs`.", "question_id": 49249}
{"snippet": "Tensor.backward(gradient=None, retain_graph=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`.", "question_id": 49250}
{"snippet": "Tensor.backward(gradient=None, create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `create_graph`.", "question_id": 49251}
{"snippet": "Tensor.backward(gradient=None, inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `inputs`.", "question_id": 49252}
{"snippet": "Tensor.backward(retain_graph=None, create_graph=False)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`, `create_graph`.", "question_id": 49253}
{"snippet": "Tensor.backward(retain_graph=None, inputs=None)", "intent": "Computes the `gradient` of current tensor w.r.t . With arguments `retain_graph`, `inputs`.", "question_id": 49254}
{"snippet": "Tensor.retain_grad()", "intent": "Enables .grad attribute for non-leaf Tensors .", "question_id": 49255}
{"snippet": "torch.nn.KLDivLoss()", "intent": "The Kullback-Leibler divergence loss measure", "question_id": 49256}
{"snippet": "torch.nn.KLDivLoss(size_average=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `size_average`.", "question_id": 49257}
{"snippet": "torch.nn.KLDivLoss(reduce=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `reduce`.", "question_id": 49258}
{"snippet": "torch.nn.KLDivLoss(reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as :", "question_id": 49259}
{"snippet": "torch.nn.KLDivLoss(log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True .", "question_id": 49260}
{"snippet": "torch.nn.KLDivLoss(size_average=None, reduce=None)", "intent": "The Kullback-Leibler divergence loss measure With arguments `size_average`, `reduce`.", "question_id": 49261}
{"snippet": "torch.nn.KLDivLoss(size_average=None, reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 49262}
{"snippet": "torch.nn.KLDivLoss(size_average=None, log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True . With arguments `size_average`.", "question_id": 49263}
{"snippet": "torch.nn.KLDivLoss(reduce=None, reduction='mean')", "intent": "The Kullback-Leibler divergence loss measure with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 49264}
{"snippet": "torch.nn.KLDivLoss(reduce=None, log_target=False)", "intent": "The Kullback-Leibler divergence loss measure The targets are interpreted as probabilities by default , but could be considered as log-probabilities with `log_target` set to True . With arguments `reduce`.", "question_id": 49265}
{"snippet": "torch.nn.quantized.functional.threshold(input, threshold, value)", "intent": "Applies the quantized version of the `threshold` function element-wise : With arguments `input`, `value`.", "question_id": 49266}
{"snippet": "torch.nn.quantized.functional.upsample(input)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 49267}
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 49268}
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 49269}
{"snippet": "torch.nn.quantized.functional.upsample(input, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 49270}
{"snippet": "torch.nn.quantized.functional.upsample(input, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 49271}
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 49272}
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 49273}
{"snippet": "torch.nn.quantized.functional.upsample(input, size=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 49274}
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 49275}
{"snippet": "torch.nn.quantized.functional.upsample(input, scale_factor=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 49276}
{"snippet": "torch.bmm(input, mat2)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` .", "question_id": 49277}
{"snippet": "torch.bmm(input, mat2, deterministic=False)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . With arguments `deterministic`.", "question_id": 49278}
{"snippet": "torch.bmm(input, mat2, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . If input is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , mat2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor .", "question_id": 49279}
{"snippet": "torch.bmm(input, mat2, deterministic=False, out=None)", "intent": "Performs a batch matrix-matrix product of matrices stored in `input` and `mat2` . If input is a ( b\u00d7n\u00d7m ) ( b \\times n \\times m ) ( b\u00d7n\u00d7m ) tensor , mat2 is a ( b\u00d7m\u00d7p ) ( b \\times m \\times p ) ( b\u00d7m\u00d7p ) tensor , `out` will be a ( b\u00d7n\u00d7p ) ( b \\times n \\times p ) ( b\u00d7n\u00d7p ) tensor . With arguments `deterministic`.", "question_id": 49280}
{"snippet": "torch.rot90(input, k, dims)", "intent": "Rotate a n-D tensor by 90 degrees in the plane specified by `dims` axis . Rotation direction is from the first towards the second axis if `k` > 0 , and from the second towards the first for k < 0 . With arguments `input`.", "question_id": 49281}
{"snippet": "torch.abs(input)", "intent": "Computes the absolute value of each element in `input` .", "question_id": 49282}
{"snippet": "torch.abs(input, out=None)", "intent": "Computes the absolute value of each element in `input` . With arguments `out`.", "question_id": 49283}
{"snippet": "torch.linalg.vector_norm(A)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( )", "question_id": 49284}
{"snippet": "torch.linalg.vector_norm(A, ord=2)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed .", "question_id": 49285}
{"snippet": "torch.linalg.vector_norm(A, dim=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dim`.", "question_id": 49286}
{"snippet": "torch.linalg.vector_norm(A, keepdim=False)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `keepdim`.", "question_id": 49287}
{"snippet": "torch.linalg.vector_norm(A, dtype=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dtype`.", "question_id": 49288}
{"snippet": "torch.linalg.vector_norm(A, out=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `out`.", "question_id": 49289}
{"snippet": "torch.linalg.vector_norm(A, ord=2, dim=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `dim`.", "question_id": 49290}
{"snippet": "torch.linalg.vector_norm(A, ord=2, keepdim=False)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `keepdim`.", "question_id": 49291}
{"snippet": "torch.linalg.vector_norm(A, ord=2, dtype=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `dtype`.", "question_id": 49292}
{"snippet": "torch.linalg.vector_norm(A, ord=2, out=None)", "intent": "Computes a vector norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the vector norm that is computed . With arguments `out`.", "question_id": 49293}
{"snippet": "Tensor.element_size()", "intent": "Returns the size in bytes of an individual element .", "question_id": 49294}
{"snippet": "torch.isfinite(input)", "intent": "Returns a new tensor with boolean elements representing if each element is finite or not . With arguments `input`.", "question_id": 49295}
{"snippet": "torch.kthvalue(input, k)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` .", "question_id": 49296}
{"snippet": "torch.kthvalue(input, k, dim=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` .", "question_id": 49297}
{"snippet": "torch.kthvalue(input, k, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 .", "question_id": 49298}
{"snippet": "torch.kthvalue(input, k, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 49299}
{"snippet": "torch.kthvalue(input, k, dim=None, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 .", "question_id": 49300}
{"snippet": "torch.kthvalue(input, k, dim=None, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 49301}
{"snippet": "torch.kthvalue(input, k, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 49302}
{"snippet": "torch.kthvalue(input, k, dim=None, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the `k` th smallest element of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , both the values and indices tensors are the same size as input , except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 49303}
{"snippet": "torch.copysign(input, other)", "intent": "Create a new floating-point tensor with the magnitude of `input` and the sign of `other` , elementwise .", "question_id": 49304}
{"snippet": "torch.copysign(input, other, out=None)", "intent": "Create a new floating-point tensor with the magnitude of `input` and the sign of `other` , elementwise . With arguments `out`.", "question_id": 49305}
{"snippet": "Tensor.log2_()", "intent": "In-place version of log2 ( )", "question_id": 49306}
{"snippet": "torch.nn.utils.prune.custom_from_mask(module, name, mask)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by applying the pre-computed `mask` in mask .", "question_id": 49307}
{"snippet": "torch.swapdims(input, dim0, dim1)", "intent": "Alias for torch.transpose ( ) . With arguments `input`, `dim0`, `dim1`.", "question_id": 49308}
{"snippet": "Tensor.erfc_()", "intent": "In-place version of erfc ( )", "question_id": 49309}
{"snippet": "Tensor.normal_()", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 49310}
{"snippet": "Tensor.normal_(mean=0)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 49311}
{"snippet": "Tensor.normal_(std=1)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 49312}
{"snippet": "Tensor.normal_(generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 49313}
{"snippet": "Tensor.normal_(mean=0, std=1)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` .", "question_id": 49314}
{"snippet": "Tensor.normal_(mean=0, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 49315}
{"snippet": "Tensor.normal_(std=1, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 49316}
{"snippet": "Tensor.normal_(mean=0, std=1, generator=None)", "intent": "Fills self tensor with elements samples from the normal distribution parameterized by `mean` and `std` . With arguments `generator`.", "question_id": 49317}
{"snippet": "Tensor.new_zeros(size)", "intent": "Returns a Tensor of `size` size filled with 0 .", "question_id": 49318}
{"snippet": "Tensor.new_zeros(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`.", "question_id": 49319}
{"snippet": "Tensor.new_zeros(size, device=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `device`.", "question_id": 49320}
{"snippet": "Tensor.new_zeros(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `requires_grad`.", "question_id": 49321}
{"snippet": "Tensor.new_zeros(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `device`.", "question_id": 49322}
{"snippet": "Tensor.new_zeros(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `requires_grad`.", "question_id": 49323}
{"snippet": "Tensor.new_zeros(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `device`, `requires_grad`.", "question_id": 49324}
{"snippet": "Tensor.new_zeros(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 0 . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 49325}
{"snippet": "torch.nn.GELU", "intent": "Applies the Gaussian Error Linear Units function:", "question_id": 49326}
{"snippet": "Tensor.nanquantile(q)", "intent": "See torch.nanquantile ( ) With arguments `q`.", "question_id": 49327}
{"snippet": "Tensor.nanquantile(q, dim=None)", "intent": "See torch.nanquantile ( ) With arguments `q`, `dim`.", "question_id": 49328}
{"snippet": "Tensor.nanquantile(q, keepdim=False)", "intent": "See torch.nanquantile ( ) With arguments `q`, `keepdim`.", "question_id": 49329}
{"snippet": "Tensor.nanquantile(q, dim=None, keepdim=False)", "intent": "See torch.nanquantile ( ) With arguments `q`, `dim`, `keepdim`.", "question_id": 49330}
{"snippet": "torch.log10(input)", "intent": "Returns a new tensor with the logarithm to the base 10 of the elements of `input` .", "question_id": 49331}
{"snippet": "torch.log10(input, out=None)", "intent": "Returns a new tensor with the logarithm to the base 10 of the elements of `input` . With arguments `out`.", "question_id": 49332}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 49333}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 49334}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 49335}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 49336}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 49337}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 49338}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 49339}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 49340}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 49341}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU3d module is a fused module of Conv3d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 49342}
{"snippet": "torch.nn.LazyLinear(out_features)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`.", "question_id": 49343}
{"snippet": "torch.nn.LazyLinear(out_features, bias=True)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`.", "question_id": 49344}
{"snippet": "torch.nn.LazyLinear(out_features, device=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `device`.", "question_id": 49345}
{"snippet": "torch.nn.LazyLinear(out_features, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `dtype`.", "question_id": 49346}
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, device=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `device`.", "question_id": 49347}
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `dtype`.", "question_id": 49348}
{"snippet": "torch.nn.LazyLinear(out_features, device=None, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . With arguments `out_features`, `device`, `dtype`.", "question_id": 49349}
{"snippet": "torch.nn.LazyLinear(out_features, bias=True, device=None, dtype=None)", "intent": "A torch.nn.Linear module where in_features is inferred . In this module , the weight and `bias` are of torch.nn.UninitializedParameter class . With arguments `out_features`, `device`, `dtype`.", "question_id": 49350}
{"snippet": "lazy_linear.cls_to_become", "intent": "alias of torch.nn.modules.linear.Linear", "question_id": 49351}
{"snippet": "torch.cuda.ipc_collect()", "intent": "Force collects GPU memory after it has been released by CUDA IPC .", "question_id": 49352}
{"snippet": "torch.tile(input, reps)", "intent": "Constructs a tensor by repeating the elements of `input` . The `reps` argument specifies the number of repetitions in each dimension .", "question_id": 49353}
{"snippet": "torch.nn.CosineSimilarity()", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` .", "question_id": 49354}
{"snippet": "torch.nn.CosineSimilarity(dim=1)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` .", "question_id": 49355}
{"snippet": "torch.nn.CosineSimilarity(eps=1e-08)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` . With arguments `eps`.", "question_id": 49356}
{"snippet": "torch.nn.CosineSimilarity(dim=1, eps=1e-08)", "intent": "Returns cosine similarity between x1x_1x1\u200b and x2x_2x2\u200b , computed along `dim` . With arguments `eps`.", "question_id": 49357}
{"snippet": "torch.autograd.profiler.load_nvprof(path)", "intent": "Opens an nvprof trace file and parses autograd annotations . With arguments `path`.", "question_id": 49358}
{"snippet": "Tensor.subtract_(other)", "intent": "In-place version of subtract ( ) . With arguments `other`.", "question_id": 49359}
{"snippet": "Tensor.subtract_(other, alpha=1)", "intent": "In-place version of subtract ( ) . With arguments `other`, `alpha`.", "question_id": 49360}
{"snippet": "torch.nn.MaxPool1d(kernel_size)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 49361}
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 49362}
{"snippet": "torch.nn.MaxPool1d(kernel_size, padding=0)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . If `padding` is non-zero , then the input is implicitly padded with negative infinity on both sides for padding number of points . With arguments `kernel_size`.", "question_id": 49363}
{"snippet": "torch.nn.MaxPool1d(kernel_size, dilation=1)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 49364}
{"snippet": "torch.nn.MaxPool1d(kernel_size, return_indices=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 49365}
{"snippet": "torch.nn.MaxPool1d(kernel_size, ceil_mode=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 49366}
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, padding=0)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . If `padding` is non-zero , then the input is implicitly padded with negative infinity on both sides for padding number of points . With arguments `kernel_size`.", "question_id": 49367}
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`.", "question_id": 49368}
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`, `return_indices`.", "question_id": 49369}
{"snippet": "torch.nn.MaxPool1d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . `dilation` is the `stride` between the elements within the sliding window . With arguments `kernel_size`, `ceil_mode`.", "question_id": 49370}
{"snippet": "torch.less(input, other)", "intent": "Alias for torch.lt ( ) . With arguments `input`, `other`.", "question_id": 49371}
{"snippet": "torch.less(input, other, out=None)", "intent": "Alias for torch.lt ( ) . With arguments `input`, `other`, `out`.", "question_id": 49372}
{"snippet": "Tensor.logaddexp2(other)", "intent": "See torch.logaddexp2 ( ) With arguments `other`.", "question_id": 49373}
{"snippet": "torch.true_divide(dividend, divisor, out)", "intent": "Alias for torch.div ( ) with rounding_mode=None . With arguments `dividend`, `divisor`, `out`.", "question_id": 49374}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 49375}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`.", "question_id": 49376}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, max_norm=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 49377}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 49378}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 49379}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, sparse=False)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 49380}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, _weight=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 49381}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, device=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `device`.", "question_id": 49382}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, dtype=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 49383}
{"snippet": "torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None)", "intent": "A simple lookup table that stores embeddings of a fixed dictionary and size . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `max_norm`.", "question_id": 49384}
{"snippet": "embedding.from_pretrained(embeddings)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`.", "question_id": 49385}
{"snippet": "embedding.from_pretrained(embeddings, freeze=True)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`.", "question_id": 49386}
{"snippet": "embedding.from_pretrained(embeddings, padding_idx=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `padding_idx`.", "question_id": 49387}
{"snippet": "embedding.from_pretrained(embeddings, max_norm=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `max_norm`.", "question_id": 49388}
{"snippet": "embedding.from_pretrained(embeddings, norm_type=2.0)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `norm_type`.", "question_id": 49389}
{"snippet": "embedding.from_pretrained(embeddings, scale_grad_by_freq=False)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `scale_grad_by_freq`.", "question_id": 49390}
{"snippet": "embedding.from_pretrained(embeddings, sparse=False)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `sparse`.", "question_id": 49391}
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, padding_idx=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `padding_idx`.", "question_id": 49392}
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, max_norm=None)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `max_norm`.", "question_id": 49393}
{"snippet": "embedding.from_pretrained(embeddings, freeze=True, norm_type=2.0)", "intent": "Creates Embedding instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `norm_type`.", "question_id": 49394}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`.", "question_id": 49395}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`.", "question_id": 49396}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, device=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `device`.", "question_id": 49397}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `dtype`.", "question_id": 49398}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, device=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `device`.", "question_id": 49399}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `dtype`.", "question_id": 49400}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, device=None, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `device`, `dtype`.", "question_id": 49401}
{"snippet": "torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True, device=None, dtype=None)", "intent": "Applies a bilinear transformation to the incoming data : y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b With arguments `in1_features`, `in2_features`, `out_features`, `bias`, `device`, `dtype`.", "question_id": 49402}
{"snippet": "Tensor.scatter_(dim, index, src)", "intent": "Writes all values from the tensor `src` into self at the indices specified in the `index` tensor . For each value in src , its output index is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim .", "question_id": 49403}
{"snippet": "Tensor.scatter_(dim, index, src, reduce=None)", "intent": "Writes all values from the tensor `src` into self at the indices specified in the `index` tensor . For each value in src , its output index is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim . Additionally accepts an optional `reduce` argument that allows specification of an optional reduction operation , which is applied to all values in the tensor src into self at the indicies specified in the index .", "question_id": 49404}
{"snippet": "torch.log2(input)", "intent": "Returns a new tensor with the logarithm to the base 2 of the elements of `input` .", "question_id": 49405}
{"snippet": "torch.log2(input, out=None)", "intent": "Returns a new tensor with the logarithm to the base 2 of the elements of `input` . With arguments `out`.", "question_id": 49406}
{"snippet": "Tensor.atan_()", "intent": "In-place version of atan ( )", "question_id": 49407}
{"snippet": "Tensor.new_empty(size)", "intent": "Returns a Tensor of `size` size filled with uninitialized data .", "question_id": 49408}
{"snippet": "Tensor.new_empty(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`.", "question_id": 49409}
{"snippet": "Tensor.new_empty(size, device=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `device`.", "question_id": 49410}
{"snippet": "Tensor.new_empty(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `requires_grad`.", "question_id": 49411}
{"snippet": "Tensor.new_empty(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `device`.", "question_id": 49412}
{"snippet": "Tensor.new_empty(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `requires_grad`.", "question_id": 49413}
{"snippet": "Tensor.new_empty(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `device`, `requires_grad`.", "question_id": 49414}
{"snippet": "Tensor.new_empty(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with uninitialized data . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 49415}
{"snippet": "torch.nn.utils.prune.RandomStructured(amount)", "intent": "Prune entire ( currently unpruned ) channels in a tensor at random . With arguments `amount`.", "question_id": 49416}
{"snippet": "torch.nn.utils.prune.RandomStructured(amount, dim=- 1)", "intent": "Prune entire ( currently unpruned ) channels in a tensor at random . With arguments `amount`, `dim`.", "question_id": 49417}
{"snippet": "random_structured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 49418}
{"snippet": "random_structured.apply(module, name, amount, dim=- 1)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `dim`.", "question_id": 49419}
{"snippet": "random_structured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 49420}
{"snippet": "random_structured.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor . With arguments `t`.", "question_id": 49421}
{"snippet": "random_structured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 49422}
{"snippet": "random_structured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 49423}
{"snippet": "random_structured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 49424}
{"snippet": "random_structured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 49425}
{"snippet": "random_structured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 49426}
{"snippet": "torch.clone(input)", "intent": "Returns a copy of `input` .", "question_id": 49427}
{"snippet": "torch.clone(input, memory_format=torch.preserve_format)", "intent": "Returns a copy of `input` . With arguments `memory_format`.", "question_id": 49428}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`.", "question_id": 49429}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`.", "question_id": 49430}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `last_epoch`.", "question_id": 49431}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `verbose`.", "question_id": 49432}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `last_epoch`.", "question_id": 49433}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `verbose`.", "question_id": 49434}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, last_epoch=- 1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `last_epoch`, `verbose`.", "question_id": 49435}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=- 1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr and TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart in SGDR : With arguments `optimizer`, `T_max`, `eta_min`, `last_epoch`, `verbose`.", "question_id": 49436}
{"snippet": "cosine_annealing_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 49437}
{"snippet": "cosine_annealing_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 49438}
{"snippet": "cosine_annealing_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 49439}
{"snippet": "cosine_annealing_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 49440}
{"snippet": "cosine_annealing_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 49441}
{"snippet": "torch.ceil(input)", "intent": "Returns a new tensor with the ceil of the elements of `input` , the smallest integer greater than or equal to each element .", "question_id": 49442}
{"snippet": "torch.ceil(input, out=None)", "intent": "Returns a new tensor with the ceil of the elements of `input` , the smallest integer greater than or equal to each element . With arguments `out`.", "question_id": 49443}
{"snippet": "torch.nn.utils.prune.PruningContainer(*args)", "intent": "Container holding a sequence of pruning methods for iterative pruning . With arguments `*args`.", "question_id": 49444}
{"snippet": "pruning_container.add_pruning_method(method)", "intent": "Adds a child pruning `method` to the container .", "question_id": 49445}
{"snippet": "pruning_container.apply(module, name, *args, **kwargs)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`.", "question_id": 49446}
{"snippet": "pruning_container.apply(module, name, *args, **kwargs, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`, `importance_scores`.", "question_id": 49447}
{"snippet": "pruning_container.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 49448}
{"snippet": "pruning_container.compute_mask(t, default_mask)", "intent": "Applies the latest method by computing the new partial masks and returning its combination with the `default_mask` . Which portions of the tensor `t` the new mask will be calculated from depends on the PRUNING_TYPE ( handled by the type handler ) :", "question_id": 49449}
{"snippet": "pruning_container.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 49450}
{"snippet": "pruning_container.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 49451}
{"snippet": "pruning_container.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 49452}
{"snippet": "pruning_container.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 49453}
{"snippet": "pruning_container.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 49454}
{"snippet": "torch.nn.functional.tanhshrink(input)", "intent": "Applies element-wise , Tanhshrink ( x ) =x\u2212Tanh ( x ) \\text { Tanhshrink } ( x ) = x - \\text { Tanh } ( x ) Tanhshrink ( x ) =x\u2212Tanh ( x ) With arguments `input`.", "question_id": 49455}
{"snippet": "torch.nn.ReLU6()", "intent": "Applies the element-wise function :", "question_id": 49456}
{"snippet": "torch.nn.ReLU6(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 49457}
{"snippet": "Tensor.asin_()", "intent": "In-place version of asin ( )", "question_id": 49458}
{"snippet": "torch.nn.CELU()", "intent": "Applies the element-wise function :", "question_id": 49459}
{"snippet": "torch.nn.CELU(alpha=1.0)", "intent": "Applies the element-wise function : With arguments `alpha`.", "question_id": 49460}
{"snippet": "torch.nn.CELU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 49461}
{"snippet": "torch.nn.CELU(alpha=1.0, inplace=False)", "intent": "Applies the element-wise function : With arguments `alpha`, `inplace`.", "question_id": 49462}
{"snippet": "torch.flipud(input)", "intent": "Flip tensor in the up/down direction , returning a new tensor . With arguments `input`.", "question_id": 49463}
{"snippet": "Tensor.logical_xor_()", "intent": "In-place version of logical_xor ( )", "question_id": 49464}
{"snippet": "torch.nn.functional.elu(input)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`.", "question_id": 49465}
{"snippet": "torch.nn.functional.elu(input, alpha=1.0)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `alpha`.", "question_id": 49466}
{"snippet": "torch.nn.functional.elu(input, inplace=False)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `inplace`.", "question_id": 49467}
{"snippet": "torch.nn.functional.elu(input, alpha=1.0, inplace=False)", "intent": "Applies element-wise , ELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) \\text { ELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) . With arguments `input`, `alpha`, `inplace`.", "question_id": 49468}
{"snippet": "torch.maximum(input, other)", "intent": "Computes the element-wise maximum of `input` and `other` .", "question_id": 49469}
{"snippet": "torch.maximum(input, other, out=None)", "intent": "Computes the element-wise maximum of `input` and `other` . With arguments `out`.", "question_id": 49470}
{"snippet": "Tensor.coalesce()", "intent": "Returns a coalesced copy of self if self is an uncoalesced tensor .", "question_id": 49471}
{"snippet": "Tensor.reciprocal()", "intent": "See torch.reciprocal ( )", "question_id": 49472}
{"snippet": "torch.isneginf(input)", "intent": "Tests if each element of `input` is negative infinity or not .", "question_id": 49473}
{"snippet": "torch.isneginf(input, out=None)", "intent": "Tests if each element of `input` is negative infinity or not . With arguments `out`.", "question_id": 49474}
{"snippet": "Tensor.mv(vec)", "intent": "See torch.mv ( ) With arguments `vec`.", "question_id": 49475}
{"snippet": "torch.isnan(input)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is NaN or not .", "question_id": 49476}
{"snippet": "Tensor.count_nonzero()", "intent": "See torch.count_nonzero ( )", "question_id": 49477}
{"snippet": "Tensor.count_nonzero(dim=None)", "intent": "See torch.count_nonzero ( ) With arguments `dim`.", "question_id": 49478}
{"snippet": "Tensor.logcumsumexp(dim)", "intent": "See torch.logcumsumexp ( ) With arguments `dim`.", "question_id": 49479}
{"snippet": "torch.remainder(input, other)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the divisor `other` . With arguments `input`.", "question_id": 49480}
{"snippet": "torch.remainder(input, other, out=None)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the divisor `other` . With arguments `input`, `out`.", "question_id": 49481}
{"snippet": "Optimizer.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 49482}
{"snippet": "torch.nn.functional.leaky_relu_(input)", "intent": "In-place version of leaky_relu ( ) . With arguments `input`.", "question_id": 49483}
{"snippet": "torch.nn.functional.leaky_relu_(input, negative_slope=0.01)", "intent": "In-place version of leaky_relu ( ) . With arguments `input`, `negative_slope`.", "question_id": 49484}
{"snippet": "torch.nn.ReplicationPad2d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 49485}
{"snippet": "torch.nn.functional.adaptive_avg_pool1d(input, output_size)", "intent": "Applies a 1D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 49486}
{"snippet": "torch.jit.script_if_tracing(fn)", "intent": "Compiles `fn` when it is first called during tracing .", "question_id": 49487}
{"snippet": "Tensor.transpose(dim0, dim1)", "intent": "See torch.transpose ( ) With arguments `dim0`, `dim1`.", "question_id": 49488}
{"snippet": "Tensor.multiply_(value)", "intent": "In-place version of multiply ( ) . With arguments `value`.", "question_id": 49489}
{"snippet": "torch.jit.freeze(mod)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . With arguments `mod`.", "question_id": 49490}
{"snippet": "torch.jit.freeze(mod, preserved_attrs=None)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . By default , forward will be preserved , as well as attributes & methods specified in `preserved_attrs` . With arguments `mod`.", "question_id": 49491}
{"snippet": "torch.jit.freeze(mod, optimize_numerics=True)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . With arguments `mod`, `optimize_numerics`.", "question_id": 49492}
{"snippet": "torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)", "intent": "Freezing a ScriptModule will clone it and attempt to inline the cloned module \u2019 s submodules , parameters , and attributes as constants in the TorchScript IR Graph . By default , forward will be preserved , as well as attributes & methods specified in `preserved_attrs` . With arguments `mod`, `optimize_numerics`.", "question_id": 49493}
{"snippet": "torch.unsqueeze(input, dim)", "intent": "Returns a new tensor with a dimension of size one inserted at the specified position . A `dim` value within the range [ -input.dim ( ) - 1 , input.dim ( ) + 1 ) can be used . With arguments `input`.", "question_id": 49494}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 49495}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 49496}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, padding=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 49497}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, dilation=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 49498}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, groups=1)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 49499}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 49500}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, scale=1.0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 49501}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, zero_point=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 49502}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 49503}
{"snippet": "torch.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 3D convolution over a quantized 3D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 49504}
{"snippet": "torch.addcmul(input, tensor1, tensor2)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 49505}
{"snippet": "torch.addcmul(input, tensor1, tensor2, value=1)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` .", "question_id": 49506}
{"snippet": "torch.addcmul(input, tensor1, tensor2, out=None)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 49507}
{"snippet": "torch.addcmul(input, tensor1, tensor2, value=1, out=None)", "intent": "Performs the element-wise multiplication of `tensor1` by `tensor2` , multiply the result by the scalar `value` and add it to `input` . With arguments `out`.", "question_id": 49508}
{"snippet": "torch.atan(input)", "intent": "Returns a new tensor with the arctangent of the elements of `input` .", "question_id": 49509}
{"snippet": "torch.atan(input, out=None)", "intent": "Returns a new tensor with the arctangent of the elements of `input` . With arguments `out`.", "question_id": 49510}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`.", "question_id": 49511}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`.", "question_id": 49512}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`.", "question_id": 49513}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `device`.", "question_id": 49514}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `dtype`.", "question_id": 49515}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `momentum`.", "question_id": 49516}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `device`.", "question_id": 49517}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, eps=1e-05, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `eps`, `dtype`.", "question_id": 49518}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1, device=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`, `device`.", "question_id": 49519}
{"snippet": "torch.nn.quantized.BatchNorm3d(num_features, momentum=0.1, dtype=None)", "intent": "This is the quantized version of BatchNorm3d . With arguments `num_features`, `momentum`, `dtype`.", "question_id": 49520}
{"snippet": "Tensor.q_per_channel_zero_points()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns a tensor of zero_points of the underlying quantizer .", "question_id": 49521}
{"snippet": "torch.bitwise_and(input, other)", "intent": "Computes the bitwise AND of `input` and `other` .", "question_id": 49522}
{"snippet": "torch.bitwise_and(input, other, out=None)", "intent": "Computes the bitwise AND of `input` and `other` . With arguments `out`.", "question_id": 49523}
{"snippet": "Tensor.rsqrt()", "intent": "See torch.rsqrt ( )", "question_id": 49524}
{"snippet": "torch.sign(input)", "intent": "Returns a new tensor with the signs of the elements of `input` .", "question_id": 49525}
{"snippet": "torch.sign(input, out=None)", "intent": "Returns a new tensor with the signs of the elements of `input` . With arguments `out`.", "question_id": 49526}
{"snippet": "torch.linalg.svdvals(A)", "intent": "Computes the singular values of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 49527}
{"snippet": "torch.linalg.svdvals(A, out=None)", "intent": "Computes the singular values of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 49528}
{"snippet": "torch.cuda.StreamContext(stream)", "intent": "Context-manager that selects a given `stream` .", "question_id": 49529}
{"snippet": "Tensor.erfc()", "intent": "See torch.erfc ( )", "question_id": 49530}
{"snippet": "torch.fft.fft(input)", "intent": "Computes the one dimensional discrete Fourier transform of `input` .", "question_id": 49531}
{"snippet": "torch.fft.fft(input, n=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`.", "question_id": 49532}
{"snippet": "torch.fft.fft(input, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 49533}
{"snippet": "torch.fft.fft(input, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 49534}
{"snippet": "torch.fft.fft(input, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `out`.", "question_id": 49535}
{"snippet": "torch.fft.fft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `dim`.", "question_id": 49536}
{"snippet": "torch.fft.fft(input, n=None, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `norm`.", "question_id": 49537}
{"snippet": "torch.fft.fft(input, n=None, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `n`, `out`.", "question_id": 49538}
{"snippet": "torch.fft.fft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 49539}
{"snippet": "torch.fft.fft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 49540}
{"snippet": "Tensor.sum()", "intent": "See torch.sum ( )", "question_id": 49541}
{"snippet": "Tensor.sum(dim=None)", "intent": "See torch.sum ( ) With arguments `dim`.", "question_id": 49542}
{"snippet": "Tensor.sum(keepdim=False)", "intent": "See torch.sum ( ) With arguments `keepdim`.", "question_id": 49543}
{"snippet": "Tensor.sum(dtype=None)", "intent": "See torch.sum ( ) With arguments `dtype`.", "question_id": 49544}
{"snippet": "Tensor.sum(dim=None, keepdim=False)", "intent": "See torch.sum ( ) With arguments `dim`, `keepdim`.", "question_id": 49545}
{"snippet": "Tensor.sum(dim=None, dtype=None)", "intent": "See torch.sum ( ) With arguments `dim`, `dtype`.", "question_id": 49546}
{"snippet": "Tensor.sum(keepdim=False, dtype=None)", "intent": "See torch.sum ( ) With arguments `keepdim`, `dtype`.", "question_id": 49547}
{"snippet": "Tensor.sum(dim=None, keepdim=False, dtype=None)", "intent": "See torch.sum ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 49548}
{"snippet": "torch.nn.quantized.ELU(scale, zero_point)", "intent": "This is the quantized equivalent of ELU . With arguments `scale`, `zero_point`.", "question_id": 49549}
{"snippet": "torch.nn.quantized.ELU(scale, zero_point, alpha=1.0)", "intent": "This is the quantized equivalent of ELU . With arguments `scale`, `zero_point`, `alpha`.", "question_id": 49550}
{"snippet": "torch.hstack(tensors)", "intent": "Stack `tensors` in sequence horizontally ( column wise ) .", "question_id": 49551}
{"snippet": "torch.hstack(tensors, out=None)", "intent": "Stack `tensors` in sequence horizontally ( column wise ) . With arguments `out`.", "question_id": 49552}
{"snippet": "Tensor.mul(value)", "intent": "See torch.mul ( ) . With arguments `value`.", "question_id": 49553}
{"snippet": "torch.nn.functional.hardshrink(input)", "intent": "Applies the hard shrinkage function element-wise With arguments `input`.", "question_id": 49554}
{"snippet": "torch.nn.functional.hardshrink(input, lambd=0.5)", "intent": "Applies the hard shrinkage function element-wise With arguments `input`, `lambd`.", "question_id": 49555}
{"snippet": "torch.nn.functional.gumbel_softmax(logits)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`.", "question_id": 49556}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`.", "question_id": 49557}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`.", "question_id": 49558}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `eps`.", "question_id": 49559}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `dim`.", "question_id": 49560}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `hard`.", "question_id": 49561}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `eps`.", "question_id": 49562}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, tau=1, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `tau`, `dim`.", "question_id": 49563}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False, eps=1e-10)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`, `eps`.", "question_id": 49564}
{"snippet": "torch.nn.functional.gumbel_softmax(logits, hard=False, dim=- 1)", "intent": "Samples from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretizes . With arguments `logits`, `hard`, `dim`.", "question_id": 49565}
{"snippet": "torch.bitwise_xor(input, other)", "intent": "Computes the bitwise XOR of `input` and `other` .", "question_id": 49566}
{"snippet": "torch.bitwise_xor(input, other, out=None)", "intent": "Computes the bitwise XOR of `input` and `other` . With arguments `out`.", "question_id": 49567}
{"snippet": "torch.imag(input)", "intent": "Returns a new tensor containing imaginary values of the self tensor . With arguments `input`.", "question_id": 49568}
{"snippet": "torch.trunc(input)", "intent": "Returns a new tensor with the truncated integer values of the elements of `input` .", "question_id": 49569}
{"snippet": "torch.trunc(input, out=None)", "intent": "Returns a new tensor with the truncated integer values of the elements of `input` . With arguments `out`.", "question_id": 49570}
{"snippet": "torch.jit.trace_module(mod, inputs)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`.", "question_id": 49571}
{"snippet": "torch.jit.trace_module(mod, inputs, optimize=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `optimize`.", "question_id": 49572}
{"snippet": "torch.jit.trace_module(mod, inputs, check_trace=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_trace`.", "question_id": 49573}
{"snippet": "torch.jit.trace_module(mod, inputs, check_inputs=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_inputs`.", "question_id": 49574}
{"snippet": "torch.jit.trace_module(mod, inputs, check_tolerance=1e-05)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `check_tolerance`.", "question_id": 49575}
{"snippet": "torch.jit.trace_module(mod, inputs, strict=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `strict`.", "question_id": 49576}
{"snippet": "torch.jit.trace_module(mod, inputs, _force_outplace=False)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_force_outplace`.", "question_id": 49577}
{"snippet": "torch.jit.trace_module(mod, inputs, _module_class=None)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_module_class`.", "question_id": 49578}
{"snippet": "torch.jit.trace_module(mod, inputs, _compilation_unit=<torch.jit.CompilationUnit object>)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `_compilation_unit`.", "question_id": 49579}
{"snippet": "torch.jit.trace_module(mod, inputs, optimize=None, check_trace=True)", "intent": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation . With trace_module , you can specify a dictionary of method names to example `inputs` to trace ( see the inputs ) argument below . With arguments `mod`, `optimize`, `check_trace`.", "question_id": 49580}
{"snippet": "torch.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers .", "question_id": 49581}
{"snippet": "Tensor.flatten(input)", "intent": "see torch.flatten ( ) With arguments `input`.", "question_id": 49582}
{"snippet": "Tensor.flatten(input, start_dim=0)", "intent": "see torch.flatten ( ) With arguments `input`, `start_dim`.", "question_id": 49583}
{"snippet": "Tensor.flatten(input, end_dim=- 1)", "intent": "see torch.flatten ( ) With arguments `input`, `end_dim`.", "question_id": 49584}
{"snippet": "Tensor.flatten(input, start_dim=0, end_dim=- 1)", "intent": "see torch.flatten ( ) With arguments `input`, `start_dim`, `end_dim`.", "question_id": 49585}
{"snippet": "Tensor.sort()", "intent": "See torch.sort ( )", "question_id": 49586}
{"snippet": "Tensor.sort(dim=- 1)", "intent": "See torch.sort ( ) With arguments `dim`.", "question_id": 49587}
{"snippet": "Tensor.sort(descending=False)", "intent": "See torch.sort ( ) With arguments `descending`.", "question_id": 49588}
{"snippet": "Tensor.sort(dim=- 1, descending=False)", "intent": "See torch.sort ( ) With arguments `dim`, `descending`.", "question_id": 49589}
{"snippet": "torch.nn.TransformerEncoder(encoder_layer, num_layers)", "intent": "TransformerEncoder is a stack of N encoder layers With arguments `encoder_layer`, `num_layers`.", "question_id": 49590}
{"snippet": "torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)", "intent": "TransformerEncoder is a stack of N encoder layers With arguments `encoder_layer`, `num_layers`, `norm`.", "question_id": 49591}
{"snippet": "transformer_encoder.forward(src)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`.", "question_id": 49592}
{"snippet": "transformer_encoder.forward(src, mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `mask`.", "question_id": 49593}
{"snippet": "transformer_encoder.forward(src, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `src_key_padding_mask`.", "question_id": 49594}
{"snippet": "transformer_encoder.forward(src, mask=None, src_key_padding_mask=None)", "intent": "Pass the input through the encoder layers in turn . With arguments `src`, `mask`, `src_key_padding_mask`.", "question_id": 49595}
{"snippet": "torch.ormqr(input, tau, other)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) .", "question_id": 49596}
{"snippet": "torch.ormqr(input, tau, other, left=True)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) .", "question_id": 49597}
{"snippet": "torch.ormqr(input, tau, other, transpose=False)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op .", "question_id": 49598}
{"snippet": "torch.ormqr(input, tau, other, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . With arguments `out`.", "question_id": 49599}
{"snippet": "torch.ormqr(input, tau, other, left=True, transpose=False)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op .", "question_id": 49600}
{"snippet": "torch.ormqr(input, tau, other, left=True, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . With arguments `out`.", "question_id": 49601}
{"snippet": "torch.ormqr(input, tau, other, transpose=False, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op . With arguments `out`.", "question_id": 49602}
{"snippet": "torch.ormqr(input, tau, other, left=True, transpose=False, out=None)", "intent": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix . Multiplies a m\u00d7nm \\times nm\u00d7n matrix C ( given by `other` ) with a matrix Q , where Q is represented using Householder reflectors ( `input` , `tau` ) . If `left` is True then op ( Q ) times C is computed , otherwise the result is C times op ( Q ) . If `transpose` is True then op is the conjugate transpose operation , otherwise it \u2019 s a no-op . With arguments `out`.", "question_id": 49603}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 49604}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 49605}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 49606}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 49607}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 49608}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 49609}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 49610}
{"snippet": "torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 49611}
{"snippet": "torch.fft.ihfft(input)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain .", "question_id": 49612}
{"snippet": "torch.fft.ihfft(input, n=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`.", "question_id": 49613}
{"snippet": "torch.fft.ihfft(input, dim=- 1)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`.", "question_id": 49614}
{"snippet": "torch.fft.ihfft(input, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `norm`.", "question_id": 49615}
{"snippet": "torch.fft.ihfft(input, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `out`.", "question_id": 49616}
{"snippet": "torch.fft.ihfft(input, n=None, dim=- 1)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `dim`.", "question_id": 49617}
{"snippet": "torch.fft.ihfft(input, n=None, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `norm`.", "question_id": 49618}
{"snippet": "torch.fft.ihfft(input, n=None, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `n`, `out`.", "question_id": 49619}
{"snippet": "torch.fft.ihfft(input, dim=- 1, norm=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`, `norm`.", "question_id": 49620}
{"snippet": "torch.fft.ihfft(input, dim=- 1, out=None)", "intent": "Computes the inverse of hfft ( ) . `input` must be a real-valued signal , interpreted in the Fourier domain . With arguments `dim`, `out`.", "question_id": 49621}
{"snippet": "torch.ldexp(input, other)", "intent": "Multiplies `input` by 2 * * : attr : `other` .", "question_id": 49622}
{"snippet": "torch.ldexp(input, other, out=None)", "intent": "Multiplies `input` by 2 * * : attr : `other` . With arguments `out`.", "question_id": 49623}
{"snippet": "torch.negative(input)", "intent": "Alias for torch.neg ( ) With arguments `input`.", "question_id": 49624}
{"snippet": "torch.negative(input, out=None)", "intent": "Alias for torch.neg ( ) With arguments `input`, `out`.", "question_id": 49625}
{"snippet": "Tensor.where(condition, y)", "intent": "self.where ( `condition` , `y` ) is equivalent to torch.where ( condition , self , y ) .", "question_id": 49626}
{"snippet": "torch.hann_window(window_length)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 49627}
{"snippet": "torch.hann_window(window_length, periodic=True)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 49628}
{"snippet": "torch.hann_window(window_length, dtype=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 49629}
{"snippet": "torch.hann_window(window_length, layout=torch.strided)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 49630}
{"snippet": "torch.hann_window(window_length, device=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 49631}
{"snippet": "torch.hann_window(window_length, requires_grad=False)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 49632}
{"snippet": "torch.hann_window(window_length, periodic=True, dtype=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 49633}
{"snippet": "torch.hann_window(window_length, periodic=True, layout=torch.strided)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 49634}
{"snippet": "torch.hann_window(window_length, periodic=True, device=None)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 49635}
{"snippet": "torch.hann_window(window_length, periodic=True, requires_grad=False)", "intent": "Hann window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 49636}
{"snippet": "Tensor.addcdiv_(tensor1, tensor2)", "intent": "In-place version of addcdiv ( ) With arguments `tensor1`, `tensor2`.", "question_id": 49637}
{"snippet": "Tensor.addcdiv_(tensor1, tensor2, value=1)", "intent": "In-place version of addcdiv ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 49638}
{"snippet": "torch.nn.Hardshrink()", "intent": "Applies the hard shrinkage function element-wise :", "question_id": 49639}
{"snippet": "torch.nn.Hardshrink(lambd=0.5)", "intent": "Applies the hard shrinkage function element-wise : With arguments `lambd`.", "question_id": 49640}
{"snippet": "torch.matrix_power(input, n)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `input`, `n`.", "question_id": 49641}
{"snippet": "torch.matrix_power(input, n, out=None)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `input`, `n`, `out`.", "question_id": 49642}
{"snippet": "torch.stack(tensors)", "intent": "Concatenates a sequence of `tensors` along a new dimension .", "question_id": 49643}
{"snippet": "torch.stack(tensors, dim=0)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `dim`.", "question_id": 49644}
{"snippet": "torch.stack(tensors, out=None)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `out`.", "question_id": 49645}
{"snippet": "torch.stack(tensors, dim=0, out=None)", "intent": "Concatenates a sequence of `tensors` along a new dimension . With arguments `dim`, `out`.", "question_id": 49646}
{"snippet": "torch.nn.UpsamplingBilinear2d()", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels .", "question_id": 49647}
{"snippet": "torch.nn.UpsamplingBilinear2d(size=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 49648}
{"snippet": "torch.nn.UpsamplingBilinear2d(scale_factor=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 49649}
{"snippet": "torch.nn.UpsamplingBilinear2d(size=None, scale_factor=None)", "intent": "Applies a 2D bilinear upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 49650}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss()", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) .", "question_id": 49651}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 49652}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 49653}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . With arguments `swap`.", "question_id": 49654}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 49655}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 .", "question_id": 49656}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . With arguments `swap`.", "question_id": 49657}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(distance_function=None, reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 49658}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0, swap=False)", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . With arguments `swap`.", "question_id": 49659}
{"snippet": "torch.nn.TripletMarginWithDistanceLoss(margin=1.0, reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given input tensors aaa , ppp , and nnn ( representing anchor , positive , and negative examples , respectively ) , and a nonnegative , real-valued function ( \u201c distance function \u201d ) used to compute the relationship between the anchor and positive example ( \u201c positive distance \u201d ) and the anchor and negative example ( \u201c negative distance \u201d ) . where NNN is the batch size ; ddd is a nonnegative , real-valued function quantifying the closeness of two tensors , referred to as the `distance_function` ; and marginmarginmargin is a nonnegative `margin` representing the minimum difference between the positive and negative distances that is required for the loss to be 0 . The unreduced loss ( i.e. , with `reduction` set to 'none ' ) can be described as :", "question_id": 49660}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) .", "question_id": 49661}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`.", "question_id": 49662}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_pivots=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_pivots`.", "question_id": 49663}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `out`.", "question_id": 49664}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `unpack_pivots`.", "question_id": 49665}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `out`.", "question_id": 49666}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_pivots=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_pivots`, `out`.", "question_id": 49667}
{"snippet": "torch.lu_unpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True, out=None)", "intent": "Unpacks the data and pivots from a LU factorization of a tensor into tensors L and U and a permutation tensor P such that `LU_data` , `LU_pivots` = ( P @ L @ U ) .lu ( ) . With arguments `unpack_data`, `unpack_pivots`, `out`.", "question_id": 49668}
{"snippet": "torch.nn.InstanceNorm2d(num_features)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 49669}
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 49670}
{"snippet": "torch.nn.InstanceNorm2d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 49671}
{"snippet": "torch.nn.InstanceNorm2d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 49672}
{"snippet": "torch.nn.InstanceNorm2d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 49673}
{"snippet": "torch.nn.InstanceNorm2d(num_features, device=None)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 49674}
{"snippet": "torch.nn.InstanceNorm2d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 49675}
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 49676}
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 49677}
{"snippet": "torch.nn.InstanceNorm2d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 49678}
{"snippet": "torch.fft.irfftn(input)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) .", "question_id": 49679}
{"snippet": "torch.fft.irfftn(input, s=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` .", "question_id": 49680}
{"snippet": "torch.fft.irfftn(input, dim=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`.", "question_id": 49681}
{"snippet": "torch.fft.irfftn(input, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `norm`.", "question_id": 49682}
{"snippet": "torch.fft.irfftn(input, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `out`.", "question_id": 49683}
{"snippet": "torch.fft.irfftn(input, s=None, dim=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `dim`.", "question_id": 49684}
{"snippet": "torch.fft.irfftn(input, s=None, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `norm`.", "question_id": 49685}
{"snippet": "torch.fft.irfftn(input, s=None, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `out`.", "question_id": 49686}
{"snippet": "torch.fft.irfftn(input, dim=None, norm=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`, `norm`.", "question_id": 49687}
{"snippet": "torch.fft.irfftn(input, dim=None, out=None)", "intent": "Computes the inverse of rfftn ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfftn ( ) . With arguments `dim`, `out`.", "question_id": 49688}
{"snippet": "Tensor.resize_(*sizes)", "intent": "Resizes self tensor to the specified size . With arguments `*sizes`.", "question_id": 49689}
{"snippet": "Tensor.resize_(*sizes, memory_format=torch.contiguous_format)", "intent": "Resizes self tensor to the specified size . With arguments `*sizes`, `memory_format`.", "question_id": 49690}
{"snippet": "torch.cuda.max_memory_cached()", "intent": "Deprecated ; see max_memory_reserved ( ) .", "question_id": 49691}
{"snippet": "torch.cuda.max_memory_cached(device=None)", "intent": "Deprecated ; see max_memory_reserved ( ) . With arguments `device`.", "question_id": 49692}
{"snippet": "torch.nn.LazyBatchNorm2d()", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) .", "question_id": 49693}
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 49694}
{"snippet": "torch.nn.LazyBatchNorm2d(momentum=0.1)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 49695}
{"snippet": "torch.nn.LazyBatchNorm2d(affine=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 49696}
{"snippet": "torch.nn.LazyBatchNorm2d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 49697}
{"snippet": "torch.nn.LazyBatchNorm2d(device=None)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 49698}
{"snippet": "torch.nn.LazyBatchNorm2d(dtype=None)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 49699}
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 49700}
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 49701}
{"snippet": "torch.nn.LazyBatchNorm2d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm2d module with lazy initialization of the num_features argument of the BatchNorm2d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 49702}
{"snippet": "lazy_batch_norm2d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm2d", "question_id": 49703}
{"snippet": "torch.xlogy(input, other)", "intent": "Computes `input` * log ( `other` ) with the following cases .", "question_id": 49704}
{"snippet": "torch.xlogy(input, other, out=None)", "intent": "Computes `input` * log ( `other` ) with the following cases . With arguments `out`.", "question_id": 49705}
{"snippet": "Tensor.log_()", "intent": "In-place version of log ( )", "question_id": 49706}
{"snippet": "torch.autograd.functional.jacobian(func, inputs)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`.", "question_id": 49707}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`.", "question_id": 49708}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, strict=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `strict`.", "question_id": 49709}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `vectorize`.", "question_id": 49710}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `strict`.", "question_id": 49711}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `vectorize`.", "question_id": 49712}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, strict=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `strict`, `vectorize`.", "question_id": 49713}
{"snippet": "torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False)", "intent": "Function that computes the Jacobian of a given function . With arguments `func`, `inputs`, `create_graph`, `strict`, `vectorize`.", "question_id": 49714}
{"snippet": "torch.quantization.observer.get_observer_state_dict(mod)", "intent": "Returns the state dict corresponding to the observer stats . With arguments `mod`.", "question_id": 49715}
{"snippet": "torch.autograd.backward(tensors)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves .", "question_id": 49716}
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` .", "question_id": 49717}
{"snippet": "torch.autograd.backward(tensors, retain_graph=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `retain_graph`.", "question_id": 49718}
{"snippet": "torch.autograd.backward(tensors, create_graph=False)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `create_graph`.", "question_id": 49719}
{"snippet": "torch.autograd.backward(tensors, grad_variables=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `grad_variables`.", "question_id": 49720}
{"snippet": "torch.autograd.backward(tensors, inputs=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . With arguments `inputs`.", "question_id": 49721}
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `retain_graph`.", "question_id": 49722}
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, create_graph=False)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `create_graph`.", "question_id": 49723}
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, grad_variables=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `grad_variables`.", "question_id": 49724}
{"snippet": "torch.autograd.backward(tensors, grad_tensors=None, inputs=None)", "intent": "Computes the sum of gradients of given `tensors` with respect to graph leaves . their data has more than one element ) and require gradient , then the Jacobian-vector product would be computed , in this case the function additionally requires specifying `grad_tensors` . With arguments `inputs`.", "question_id": 49725}
{"snippet": "torch.polar(abs, angle)", "intent": "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value `abs` and `angle` angle .", "question_id": 49726}
{"snippet": "torch.polar(abs, angle, out=None)", "intent": "Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value `abs` and `angle` angle . With arguments `out`.", "question_id": 49727}
{"snippet": "torch.nn.Transformer()", "intent": "A transformer model .", "question_id": 49728}
{"snippet": "torch.nn.Transformer(d_model=512)", "intent": "A transformer model . With arguments `d_model`.", "question_id": 49729}
{"snippet": "torch.nn.Transformer(nhead=8)", "intent": "A transformer model . With arguments `nhead`.", "question_id": 49730}
{"snippet": "torch.nn.Transformer(num_encoder_layers=6)", "intent": "A transformer model . With arguments `num_encoder_layers`.", "question_id": 49731}
{"snippet": "torch.nn.Transformer(num_decoder_layers=6)", "intent": "A transformer model . With arguments `num_decoder_layers`.", "question_id": 49732}
{"snippet": "torch.nn.Transformer(dim_feedforward=2048)", "intent": "A transformer model . With arguments `dim_feedforward`.", "question_id": 49733}
{"snippet": "torch.nn.Transformer(dropout=0.1)", "intent": "A transformer model . With arguments `dropout`.", "question_id": 49734}
{"snippet": "torch.nn.Transformer(activation='relu')", "intent": "A transformer model . With arguments `activation`.", "question_id": 49735}
{"snippet": "torch.nn.Transformer(custom_encoder=None)", "intent": "A transformer model . With arguments `custom_encoder`.", "question_id": 49736}
{"snippet": "torch.nn.Transformer(custom_decoder=None)", "intent": "A transformer model . With arguments `custom_decoder`.", "question_id": 49737}
{"snippet": "transformer.forward(src, tgt)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`.", "question_id": 49738}
{"snippet": "transformer.forward(src, tgt, src_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`.", "question_id": 49739}
{"snippet": "transformer.forward(src, tgt, tgt_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `tgt_mask`.", "question_id": 49740}
{"snippet": "transformer.forward(src, tgt, memory_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `memory_mask`.", "question_id": 49741}
{"snippet": "transformer.forward(src, tgt, src_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_key_padding_mask`.", "question_id": 49742}
{"snippet": "transformer.forward(src, tgt, tgt_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `tgt_key_padding_mask`.", "question_id": 49743}
{"snippet": "transformer.forward(src, tgt, memory_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `memory_key_padding_mask`.", "question_id": 49744}
{"snippet": "transformer.forward(src, tgt, src_mask=None, tgt_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `tgt_mask`.", "question_id": 49745}
{"snippet": "transformer.forward(src, tgt, src_mask=None, memory_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `memory_mask`.", "question_id": 49746}
{"snippet": "transformer.forward(src, tgt, src_mask=None, src_key_padding_mask=None)", "intent": "Take in and process masked source/target sequences . With arguments `src`, `tgt`, `src_mask`, `src_key_padding_mask`.", "question_id": 49747}
{"snippet": "transformer.generate_square_subsequent_mask(sz)", "intent": "Generate a square mask for the sequence . With arguments `sz`.", "question_id": 49748}
{"snippet": "Tensor.unsqueeze_(dim)", "intent": "In-place version of unsqueeze ( ) With arguments `dim`.", "question_id": 49749}
{"snippet": "torch.fmin(input, other)", "intent": "Computes the element-wise minimum of `input` and `other` .", "question_id": 49750}
{"snippet": "torch.fmin(input, other, out=None)", "intent": "Computes the element-wise minimum of `input` and `other` . With arguments `out`.", "question_id": 49751}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 49752}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 49753}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 49754}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 49755}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 49756}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 49757}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 49758}
{"snippet": "torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool2d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 49759}
{"snippet": "Tensor.flipud()", "intent": "See torch.flipud ( )", "question_id": 49760}
{"snippet": "torch.acos(input)", "intent": "Computes the inverse cosine of each element in `input` .", "question_id": 49761}
{"snippet": "torch.acos(input, out=None)", "intent": "Computes the inverse cosine of each element in `input` . With arguments `out`.", "question_id": 49762}
{"snippet": "torch.cuda.memory_snapshot()", "intent": "Returns a snapshot of the CUDA memory allocator state across all devices .", "question_id": 49763}
{"snippet": "torch.cuda.set_per_process_memory_fraction(fraction)", "intent": "Set memory `fraction` for a process .", "question_id": 49764}
{"snippet": "torch.cuda.set_per_process_memory_fraction(fraction, device=None)", "intent": "Set memory `fraction` for a process . The fraction is used to limit an caching allocator to allocated memory on a CUDA `device` .", "question_id": 49765}
{"snippet": "torch.cholesky_solve(input, input2)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . With arguments `input`, `input2`.", "question_id": 49766}
{"snippet": "torch.cholesky_solve(input, input2, upper=False)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . If `upper` is False , uuu is and lower triangular and c is returned such that : With arguments `input`, `input2`.", "question_id": 49767}
{"snippet": "torch.cholesky_solve(input, input2, out=None)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . With arguments `input`, `input2`, `out`.", "question_id": 49768}
{"snippet": "torch.cholesky_solve(input, input2, upper=False, out=None)", "intent": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu . If `upper` is False , uuu is and lower triangular and c is returned such that : With arguments `input`, `input2`, `out`.", "question_id": 49769}
{"snippet": "Tensor.allclose(other)", "intent": "See torch.allclose ( ) With arguments `other`.", "question_id": 49770}
{"snippet": "Tensor.allclose(other, rtol=1e-05)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`.", "question_id": 49771}
{"snippet": "Tensor.allclose(other, atol=1e-08)", "intent": "See torch.allclose ( ) With arguments `other`, `atol`.", "question_id": 49772}
{"snippet": "Tensor.allclose(other, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `equal_nan`.", "question_id": 49773}
{"snippet": "Tensor.allclose(other, rtol=1e-05, atol=1e-08)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `atol`.", "question_id": 49774}
{"snippet": "Tensor.allclose(other, rtol=1e-05, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `equal_nan`.", "question_id": 49775}
{"snippet": "Tensor.allclose(other, atol=1e-08, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `atol`, `equal_nan`.", "question_id": 49776}
{"snippet": "Tensor.allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "See torch.allclose ( ) With arguments `other`, `rtol`, `atol`, `equal_nan`.", "question_id": 49777}
{"snippet": "torch.linalg.det(A)", "intent": "Computes the determinant of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 49778}
{"snippet": "torch.linalg.det(A, out=None)", "intent": "Computes the determinant of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 49779}
{"snippet": "Tensor.logical_or_()", "intent": "In-place version of logical_or ( )", "question_id": 49780}
{"snippet": "torch.linalg.solve(A, B)", "intent": "Computes the solution of a square system of linear equations with a unique solution . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb { K } ^ { n \\times k } X\u2208Kn\u00d7k of the linear system associated to A\u2208Kn\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { n \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Kn\u00d7n , B\u2208Km\u00d7k , which is defined as With arguments `A`.", "question_id": 49781}
{"snippet": "torch.linalg.solve(A, B, out=None)", "intent": "Computes the solution of a square system of linear equations with a unique solution . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , this function computes the solution X\u2208Kn\u00d7kX \\in \\mathbb { K } ^ { n \\times k } X\u2208Kn\u00d7k of the linear system associated to A\u2208Kn\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { n \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Kn\u00d7n , B\u2208Km\u00d7k , which is defined as With arguments `A`, `out`.", "question_id": 49782}
{"snippet": "torch.nn.SiLU()", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise .", "question_id": 49783}
{"snippet": "torch.nn.SiLU(inplace=False)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `inplace`.", "question_id": 49784}
{"snippet": "torch.nn.functional.alpha_dropout(input)", "intent": "Applies alpha dropout to the `input` .", "question_id": 49785}
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5)", "intent": "Applies alpha dropout to the `input` . With arguments `p`.", "question_id": 49786}
{"snippet": "torch.nn.functional.alpha_dropout(input, training=False)", "intent": "Applies alpha dropout to the `input` . With arguments `training`.", "question_id": 49787}
{"snippet": "torch.nn.functional.alpha_dropout(input, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `inplace`.", "question_id": 49788}
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, training=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `training`.", "question_id": 49789}
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `inplace`.", "question_id": 49790}
{"snippet": "torch.nn.functional.alpha_dropout(input, training=False, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `training`, `inplace`.", "question_id": 49791}
{"snippet": "torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)", "intent": "Applies alpha dropout to the `input` . With arguments `p`, `training`, `inplace`.", "question_id": 49792}
{"snippet": "torch.tril(input)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 49793}
{"snippet": "torch.tril(input, diagonal=0)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The lower triangular part of the matrix is defined as the elements on and below the `diagonal` .", "question_id": 49794}
{"snippet": "torch.tril(input, out=None)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 .", "question_id": 49795}
{"snippet": "torch.tril(input, diagonal=0, out=None)", "intent": "Returns the lower triangular part of the matrix ( 2-D tensor ) or batch of matrices `input` , the other elements of the result tensor `out` are set to 0 . The lower triangular part of the matrix is defined as the elements on and below the `diagonal` .", "question_id": 49796}
{"snippet": "torch.diag_embed(input)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 49797}
{"snippet": "torch.diag_embed(input, offset=0)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 49798}
{"snippet": "torch.diag_embed(input, dim1=- 2)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 49799}
{"snippet": "torch.diag_embed(input, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 49800}
{"snippet": "torch.diag_embed(input, offset=0, dim1=- 2)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 49801}
{"snippet": "torch.diag_embed(input, offset=0, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 49802}
{"snippet": "torch.diag_embed(input, dim1=- 2, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` .", "question_id": 49803}
{"snippet": "torch.diag_embed(input, offset=0, dim1=- 2, dim2=- 1)", "intent": "Creates a tensor whose diagonals of certain 2D planes ( specified by `dim1` and `dim2` ) are filled by `input` . The argument `offset` controls which diagonal to consider :", "question_id": 49804}
{"snippet": "Tensor.is_leaf", "intent": "All Tensors that have requires_grad which is False will be leaf Tensors by convention.", "question_id": 49805}
{"snippet": "torch.flip(input, dims)", "intent": "Reverse the order of a n-D tensor along given axis in `dims` . With arguments `input`.", "question_id": 49806}
{"snippet": "Tensor.apply_(callable)", "intent": "Applies the function `callable` to each element in the tensor , replacing each element with the value returned by callable .", "question_id": 49807}
{"snippet": "torch.add(input, other)", "intent": "Adds the scalar `other` to each element of the `input` input and returns a new resulting tensor .", "question_id": 49808}
{"snippet": "torch.add(input, other, out=None)", "intent": "Adds the scalar `other` to each element of the `input` input and returns a new resulting tensor . With arguments `out`.", "question_id": 49809}
{"snippet": "torch.multiply(input, other)", "intent": "Alias for torch.mul ( ) . With arguments `input`, `other`.", "question_id": 49810}
{"snippet": "torch.multiply(input, other, out=None)", "intent": "Alias for torch.mul ( ) . With arguments `input`, `other`, `out`.", "question_id": 49811}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`.", "question_id": 49812}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`.", "question_id": 49813}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`.", "question_id": 49814}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `verbose`.", "question_id": 49815}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`.", "question_id": 49816}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `verbose`.", "question_id": 49817}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 49818}
{"snippet": "torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` once the number of epoch reaches one of the `milestones` . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 49819}
{"snippet": "multi_step_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 49820}
{"snippet": "multi_step_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 49821}
{"snippet": "multi_step_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 49822}
{"snippet": "multi_step_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 49823}
{"snippet": "multi_step_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 49824}
{"snippet": "Tensor.scatter_add(dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_add_ ( ) With arguments `dim`, `index`, `src`.", "question_id": 49825}
{"snippet": "Tensor.movedim(source, destination)", "intent": "See torch.movedim ( ) With arguments `source`, `destination`.", "question_id": 49826}
{"snippet": "torch.cat(tensors)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension .", "question_id": 49827}
{"snippet": "torch.cat(tensors, dim=0)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `dim`.", "question_id": 49828}
{"snippet": "torch.cat(tensors, out=None)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `out`.", "question_id": 49829}
{"snippet": "torch.cat(tensors, dim=0, out=None)", "intent": "Concatenates the given sequence of seq `tensors` in the given dimension . With arguments `dim`, `out`.", "question_id": 49830}
{"snippet": "torch.cuda.current_blas_handle()", "intent": "Returns cublasHandle_t pointer to current cuBLAS handle", "question_id": 49831}
{"snippet": "Tensor.log10()", "intent": "See torch.log10 ( )", "question_id": 49832}
{"snippet": "torch.fft.ifftshift(input)", "intent": "Inverse of fftshift ( ) . With arguments `input`.", "question_id": 49833}
{"snippet": "torch.fft.ifftshift(input, dim=None)", "intent": "Inverse of fftshift ( ) . With arguments `input`, `dim`.", "question_id": 49834}
{"snippet": "Tensor.gt_(other)", "intent": "In-place version of gt ( ) . With arguments `other`.", "question_id": 49835}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`.", "question_id": 49836}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`.", "question_id": 49837}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `padding`.", "question_id": 49838}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 49839}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `count_include_pad`.", "question_id": 49840}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `divisor_override`.", "question_id": 49841}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 49842}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 49843}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 49844}
{"snippet": "torch.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps . The number of output features is equal to the number of `input` planes . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 49845}
{"snippet": "torch.set_warn_always(b)", "intent": "When this flag is False ( default ) then some PyTorch warnings may only appear once per process . With arguments `b`.", "question_id": 49846}
{"snippet": "torch.exp(input)", "intent": "Returns a new tensor with the exponential of the elements of the `input` tensor input .", "question_id": 49847}
{"snippet": "torch.exp(input, out=None)", "intent": "Returns a new tensor with the exponential of the elements of the `input` tensor input . With arguments `out`.", "question_id": 49848}
{"snippet": "Tensor.lerp(end, weight)", "intent": "See torch.lerp ( ) With arguments `end`, `weight`.", "question_id": 49849}
{"snippet": "torch.nn.utils.prune.BasePruningMethod", "intent": "Abstract base class for creation of new pruning techniques.", "question_id": 49850}
{"snippet": "base_pruning_method.apply(module, name, *args, **kwargs)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`.", "question_id": 49851}
{"snippet": "base_pruning_method.apply(module, name, *args, **kwargs, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `*args`, `**kwargs`, `importance_scores`.", "question_id": 49852}
{"snippet": "base_pruning_method.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 49853}
{"snippet": "base_pruning_method.compute_mask(t, default_mask)", "intent": "Computes and returns a mask for the input tensor t. Starting from a base `default_mask` ( which should be a mask of ones if the tensor has not been pruned yet ) , generate a random mask to apply on top of the default_mask according to the specific pruning method recipe . With arguments `t`.", "question_id": 49854}
{"snippet": "base_pruning_method.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 49855}
{"snippet": "base_pruning_method.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 49856}
{"snippet": "base_pruning_method.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 49857}
{"snippet": "base_pruning_method.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 49858}
{"snippet": "base_pruning_method.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 49859}
{"snippet": "torch.linalg.eigh(A)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 49860}
{"snippet": "torch.linalg.eigh(A, UPLO='L')", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`.", "question_id": 49861}
{"snippet": "torch.linalg.eigh(A, out=None)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 49862}
{"snippet": "torch.linalg.eigh(A, UPLO='L', out=None)", "intent": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`, `out`.", "question_id": 49863}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`.", "question_id": 49864}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`.", "question_id": 49865}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`.", "question_id": 49866}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `swap`.", "question_id": 49867}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `reduction`.", "question_id": 49868}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, margin=1.0)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `margin`.", "question_id": 49869}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `swap`.", "question_id": 49870}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, distance_function=None, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `distance_function`, `reduction`.", "question_id": 49871}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0, swap=False)", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`, `swap`.", "question_id": 49872}
{"snippet": "torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, margin=1.0, reduction='mean')", "intent": "See TripletMarginWithDistanceLoss for details . With arguments `anchor`, `positive`, `negative`, `margin`, `reduction`.", "question_id": 49873}
{"snippet": "torch.take_along_dim(input, indices, dim)", "intent": "Selects values from `input` at the 1-dimensional `indices` from indices along the given `dim` .", "question_id": 49874}
{"snippet": "torch.take_along_dim(input, indices, dim, out=None)", "intent": "Selects values from `input` at the 1-dimensional `indices` from indices along the given `dim` . With arguments `out`.", "question_id": 49875}
{"snippet": "Tensor.ge(other)", "intent": "See torch.ge ( ) . With arguments `other`.", "question_id": 49876}
{"snippet": "Tensor.cumsum(dim)", "intent": "See torch.cumsum ( ) With arguments `dim`.", "question_id": 49877}
{"snippet": "Tensor.cumsum(dim, dtype=None)", "intent": "See torch.cumsum ( ) With arguments `dim`, `dtype`.", "question_id": 49878}
{"snippet": "Tensor.addmm(mat1, mat2)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`.", "question_id": 49879}
{"snippet": "Tensor.addmm(mat1, mat2, beta=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 49880}
{"snippet": "Tensor.addmm(mat1, mat2, alpha=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 49881}
{"snippet": "Tensor.addmm(mat1, mat2, beta=1, alpha=1)", "intent": "See torch.addmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 49882}
{"snippet": "Tensor.take_along_dim(indices, dim)", "intent": "See torch.take_along_dim ( ) With arguments `indices`, `dim`.", "question_id": 49883}
{"snippet": "torch.max(input)", "intent": "Returns the maximum value of all elements in the `input` tensor .", "question_id": 49884}
{"snippet": "Tensor.remainder_(divisor)", "intent": "In-place version of remainder ( ) With arguments `divisor`.", "question_id": 49885}
{"snippet": "torch.cuda.device_count()", "intent": "Returns the number of GPUs available .", "question_id": 49886}
{"snippet": "Tensor.repeat(*sizes)", "intent": "Repeats this tensor along the specified dimensions . With arguments `*sizes`.", "question_id": 49887}
{"snippet": "torch.randn(*size)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`.", "question_id": 49888}
{"snippet": "torch.randn(*size, out=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`.", "question_id": 49889}
{"snippet": "torch.randn(*size, dtype=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `dtype`.", "question_id": 49890}
{"snippet": "torch.randn(*size, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `layout`.", "question_id": 49891}
{"snippet": "torch.randn(*size, device=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `device`.", "question_id": 49892}
{"snippet": "torch.randn(*size, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `requires_grad`.", "question_id": 49893}
{"snippet": "torch.randn(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `dtype`.", "question_id": 49894}
{"snippet": "torch.randn(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `layout`.", "question_id": 49895}
{"snippet": "torch.randn(*size, out=None, device=None)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `device`.", "question_id": 49896}
{"snippet": "torch.randn(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 ( also called the standard normal distribution ) . With arguments `*size`, `out`, `requires_grad`.", "question_id": 49897}
{"snippet": "torch.cos(input)", "intent": "Returns a new tensor with the cosine of the elements of `input` .", "question_id": 49898}
{"snippet": "torch.cos(input, out=None)", "intent": "Returns a new tensor with the cosine of the elements of `input` . With arguments `out`.", "question_id": 49899}
{"snippet": "Tensor.floor_divide_(value)", "intent": "In-place version of floor_divide ( ) With arguments `value`.", "question_id": 49900}
{"snippet": "torch.nn.functional.tanh(input)", "intent": "Applies element-wise , Tanh ( x ) =tanh\u2061 ( x ) =exp\u2061 ( x ) \u2212exp\u2061 ( \u2212x ) exp\u2061 ( x ) +exp\u2061 ( \u2212x ) \\text { Tanh } ( x ) = \\tanh ( x ) = \\frac { \\exp ( x ) - \\exp ( -x ) } { \\exp ( x ) + \\exp ( -x ) } Tanh ( x ) =tanh ( x ) =exp ( x ) +exp ( \u2212x ) exp ( x ) \u2212exp ( \u2212x ) \u200b With arguments `input`.", "question_id": 49901}
{"snippet": "torch.nn.quantized.functional.adaptive_avg_pool2d(input, output_size)", "intent": "Applies a 2D adaptive average pooling over a quantized `input` signal composed of several quantized input planes . With arguments `output_size`.", "question_id": 49902}
{"snippet": "torch.le(input, other)", "intent": "Computes input\u2264other\\text { `input` } \\leq \\text { `other` } input\u2264other element-wise .", "question_id": 49903}
{"snippet": "torch.le(input, other, out=None)", "intent": "Computes input\u2264other\\text { `input` } \\leq \\text { `other` } input\u2264other element-wise . With arguments `out`.", "question_id": 49904}
{"snippet": "Tensor.trace()", "intent": "See torch.trace ( )", "question_id": 49905}
{"snippet": "torch.atleast_1d(*tensors)", "intent": "Returns a 1-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 49906}
{"snippet": "torch.mvlgamma(input, p)", "intent": "Computes the multivariate log-gamma function ) with dimension ppp element-wise , given by where C=log\u2061 ( \u03c0 ) \u00d7p ( p\u22121 ) 4C = \\log ( \\pi ) \\times \\frac { `p` ( p - 1 ) } { 4 } C=log ( \u03c0 ) \u00d74p ( p\u22121 ) \u200b and \u0393 ( \u22c5 ) \\Gamma ( \\cdot ) \u0393 ( \u22c5 ) is the Gamma function . With arguments `input`.", "question_id": 49907}
{"snippet": "Tensor.set_()", "intent": "Sets the underlying storage , `size` , and strides .", "question_id": 49908}
{"snippet": "Tensor.set_(source=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source .", "question_id": 49909}
{"snippet": "Tensor.set_(storage_offset=0)", "intent": "Sets the underlying storage , `size` , and strides . With arguments `storage_offset`.", "question_id": 49910}
{"snippet": "Tensor.set_(size=None)", "intent": "Sets the underlying storage , `size` , and strides .", "question_id": 49911}
{"snippet": "Tensor.set_(stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` .", "question_id": 49912}
{"snippet": "Tensor.set_(source=None, storage_offset=0)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source . With arguments `storage_offset`.", "question_id": 49913}
{"snippet": "Tensor.set_(source=None, size=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source .", "question_id": 49914}
{"snippet": "Tensor.set_(source=None, stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If `source` is a tensor , self tensor will share the same storage and have the same size and strides as source . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` .", "question_id": 49915}
{"snippet": "Tensor.set_(storage_offset=0, size=None)", "intent": "Sets the underlying storage , `size` , and strides . With arguments `storage_offset`.", "question_id": 49916}
{"snippet": "Tensor.set_(storage_offset=0, stride=None)", "intent": "Sets the underlying storage , `size` , and strides . If source is a Storage , the method sets the underlying storage , offset , size , and `stride` . With arguments `storage_offset`.", "question_id": 49917}
{"snippet": "torch.nn.SELU()", "intent": "Applied element-wise , as :", "question_id": 49918}
{"snippet": "torch.nn.SELU(inplace=False)", "intent": "Applied element-wise , as : With arguments `inplace`.", "question_id": 49919}
{"snippet": "Tensor.masked_select(mask)", "intent": "See torch.masked_select ( ) With arguments `mask`.", "question_id": 49920}
{"snippet": "Tensor.numel()", "intent": "See torch.numel ( )", "question_id": 49921}
{"snippet": "torch.nn.intrinsic.ConvBn2d(conv, bn)", "intent": "This is a sequential container which calls the Conv 2d and Batch Norm 2d modules . With arguments `conv`, `bn`.", "question_id": 49922}
{"snippet": "Tensor.isfinite()", "intent": "See torch.isfinite ( )", "question_id": 49923}
{"snippet": "torch.allclose(input, other)", "intent": "This function checks if all `input` and `other` satisfy the condition :", "question_id": 49924}
{"snippet": "torch.allclose(input, other, rtol=1e-05)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`.", "question_id": 49925}
{"snippet": "torch.allclose(input, other, atol=1e-08)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `atol`.", "question_id": 49926}
{"snippet": "torch.allclose(input, other, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `equal_nan`.", "question_id": 49927}
{"snippet": "torch.allclose(input, other, rtol=1e-05, atol=1e-08)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `atol`.", "question_id": 49928}
{"snippet": "torch.allclose(input, other, rtol=1e-05, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `equal_nan`.", "question_id": 49929}
{"snippet": "torch.allclose(input, other, atol=1e-08, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `atol`, `equal_nan`.", "question_id": 49930}
{"snippet": "torch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "This function checks if all `input` and `other` satisfy the condition : With arguments `rtol`, `atol`, `equal_nan`.", "question_id": 49931}
{"snippet": "torch.nn.utils.weight_norm(module)", "intent": "Applies weight normalization to a parameter in the given `module` .", "question_id": 49932}
{"snippet": "torch.nn.utils.weight_norm(module, name='weight')", "intent": "Applies weight normalization to a parameter in the given `module` . This replaces the parameter specified by `name` ( e.g .", "question_id": 49933}
{"snippet": "torch.nn.utils.weight_norm(module, dim=0)", "intent": "Applies weight normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 49934}
{"snippet": "torch.nn.utils.weight_norm(module, name='weight', dim=0)", "intent": "Applies weight normalization to a parameter in the given `module` . This replaces the parameter specified by `name` ( e.g . With arguments `dim`.", "question_id": 49935}
{"snippet": "Tensor.matrix_power(n)", "intent": "Alias for torch.linalg.matrix_power ( ) With arguments `n`.", "question_id": 49936}
{"snippet": "Tensor.q_per_channel_axis()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns the index of dimension on which per-channel quantization is applied .", "question_id": 49937}
{"snippet": "Tensor.maximum(other)", "intent": "See torch.maximum ( ) With arguments `other`.", "question_id": 49938}
{"snippet": "Tensor.to(*args, **kwargs)", "intent": "Performs Tensor dtype and/or device conversion . With arguments `*args`, `**kwargs`.", "question_id": 49939}
{"snippet": "to(dtype)", "intent": "Returns a Tensor with the specified `dtype`", "question_id": 49940}
{"snippet": "to(dtype, non_blocking=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`.", "question_id": 49941}
{"snippet": "to(dtype, copy=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `copy`.", "question_id": 49942}
{"snippet": "to(dtype, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `memory_format`.", "question_id": 49943}
{"snippet": "to(dtype, non_blocking=False, copy=False)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `copy`.", "question_id": 49944}
{"snippet": "to(dtype, non_blocking=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `memory_format`.", "question_id": 49945}
{"snippet": "to(dtype, copy=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `copy`, `memory_format`.", "question_id": 49946}
{"snippet": "to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `dtype` With arguments `non_blocking`, `copy`, `memory_format`.", "question_id": 49947}
{"snippet": "torch.to()", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 49948}
{"snippet": "torch.to(device=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 49949}
{"snippet": "torch.to(dtype=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 49950}
{"snippet": "torch.to(non_blocking=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 49951}
{"snippet": "torch.to(copy=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 49952}
{"snippet": "torch.to(memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . With arguments `memory_format`.", "question_id": 49953}
{"snippet": "torch.to(device=None, dtype=None)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` .", "question_id": 49954}
{"snippet": "torch.to(device=None, non_blocking=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 49955}
{"snippet": "torch.to(device=None, copy=False)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 49956}
{"snippet": "torch.to(device=None, memory_format=torch.preserve_format)", "intent": "Returns a Tensor with the specified `device` and ( optional ) `dtype` . With arguments `memory_format`.", "question_id": 49957}
{"snippet": "torch.to(other)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` .", "question_id": 49958}
{"snippet": "torch.to(other, non_blocking=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor .", "question_id": 49959}
{"snippet": "torch.to(other, copy=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 49960}
{"snippet": "torch.to(other, non_blocking=False, copy=False)", "intent": "Returns a Tensor with same torch.dtype and torch.device as the Tensor `other` . When `non_blocking` , tries to convert asynchronously with respect to the host if possible , e.g. , converting a CPU Tensor with pinned memory to a CUDA Tensor . When `copy` is set , a new Tensor is created even when the Tensor already matches the desired conversion .", "question_id": 49961}
{"snippet": "Tensor.sparse_mask(mask)", "intent": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor `mask` .", "question_id": 49962}
{"snippet": "Tensor.detach()", "intent": "Returns a new Tensor , detached from the current graph .", "question_id": 49963}
{"snippet": "Tensor.sinh_()", "intent": "In-place version of sinh ( )", "question_id": 49964}
{"snippet": "torch.vdot(input, other)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`.", "question_id": 49965}
{"snippet": "torch.vdot(input, other, out=None)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`, `out`.", "question_id": 49966}
{"snippet": "torch.Generator()", "intent": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers .", "question_id": 49967}
{"snippet": "torch.Generator(device='cpu')", "intent": "Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers . With arguments `device`.", "question_id": 49968}
{"snippet": "generator.device", "intent": "Generator.device -> device", "question_id": 49969}
{"snippet": "generator.get_state()", "intent": "Returns the Generator state as a torch.ByteTensor .", "question_id": 49970}
{"snippet": "generator.initial_seed()", "intent": "Returns the initial seed for generating random numbers .", "question_id": 49971}
{"snippet": "generator.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers .", "question_id": 49972}
{"snippet": "generator.seed()", "intent": "Gets a non-deterministic random number from std : :random_device or the current time and uses it to seed a Generator .", "question_id": 49973}
{"snippet": "generator.set_state(new_state)", "intent": "Sets the Generator state . With arguments `new_state`.", "question_id": 49974}
{"snippet": "Tensor.pow_(exponent)", "intent": "In-place version of pow ( ) With arguments `exponent`.", "question_id": 49975}
{"snippet": "Tensor.lt_(other)", "intent": "In-place version of lt ( ) . With arguments `other`.", "question_id": 49976}
{"snippet": "torch.cuda.reset_max_memory_allocated()", "intent": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given `device` .", "question_id": 49977}
{"snippet": "torch.cuda.reset_max_memory_allocated(device=None)", "intent": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given `device` .", "question_id": 49978}
{"snippet": "torch.nn.functional.threshold_(input, threshold, value)", "intent": "In-place version of `threshold` ( ) . With arguments `input`, `value`.", "question_id": 49979}
{"snippet": "torch.addmv(input, mat, vec)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result .", "question_id": 49980}
{"snippet": "torch.addmv(input, mat, vec, beta=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 49981}
{"snippet": "torch.addmv(input, mat, vec, alpha=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 49982}
{"snippet": "torch.addmv(input, mat, vec, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 49983}
{"snippet": "torch.addmv(input, mat, vec, beta=1, alpha=1)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively .", "question_id": 49984}
{"snippet": "torch.addmv(input, mat, vec, beta=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 49985}
{"snippet": "torch.addmv(input, mat, vec, alpha=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 49986}
{"snippet": "torch.addmv(input, mat, vec, beta=1, alpha=1, out=None)", "intent": "Performs a matrix-vector product of the matrix `mat` and the vector `vec` . The vector `input` is added to the final result . `alpha` and `beta` are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively . If mat is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size m , then input must be broadcastable with a 1-D tensor of size n and `out` will be 1-D tensor of size n .", "question_id": 49987}
{"snippet": "Tensor.ldexp_(other)", "intent": "In-place version of ldexp ( ) With arguments `other`.", "question_id": 49988}
{"snippet": "torch.nanquantile(input, q)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist .", "question_id": 49989}
{"snippet": "torch.nanquantile(input, q, dim=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`.", "question_id": 49990}
{"snippet": "torch.nanquantile(input, q, keepdim=False)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `keepdim`.", "question_id": 49991}
{"snippet": "torch.nanquantile(input, q, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `out`.", "question_id": 49992}
{"snippet": "torch.nanquantile(input, q, dim=None, keepdim=False)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `keepdim`.", "question_id": 49993}
{"snippet": "torch.nanquantile(input, q, dim=None, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `out`.", "question_id": 49994}
{"snippet": "torch.nanquantile(input, q, keepdim=False, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `keepdim`, `out`.", "question_id": 49995}
{"snippet": "torch.nanquantile(input, q, dim=None, keepdim=False, out=None)", "intent": "This is a variant of torch.quantile ( ) that \u201c ignores \u201d NaN values , computing the quantiles `q` as if NaN values in `input` did not exist . With arguments `dim`, `keepdim`, `out`.", "question_id": 49996}
{"snippet": "torch.jit.Attribute(value, type)", "intent": "This method is a pass-through function that returns `value` , mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with `type` of type .", "question_id": 49997}
{"snippet": "attribute.count(value, /)", "intent": "Return number of occurrences of `value` . With arguments `/`.", "question_id": 49998}
{"snippet": "attribute.index(value, /)", "intent": "Return first index of `value` . With arguments `/`.", "question_id": 49999}
{"snippet": "attribute.index(value, /, start=0)", "intent": "Return first index of `value` . With arguments `/`, `start`.", "question_id": 50000}
{"snippet": "attribute.index(value, /, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `stop`.", "question_id": 50001}
{"snippet": "attribute.index(value, /, start=0, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `start`, `stop`.", "question_id": 50002}
{"snippet": "attribute.type", "intent": "Alias for field number 1", "question_id": 50003}
{"snippet": "attribute.value", "intent": "Alias for field number 0", "question_id": 50004}
{"snippet": "torch.nansum(input)", "intent": "Returns the sum of all elements , treating Not a Numbers ( NaNs ) as zero . Returns the sum of each row of the `input` tensor in the given dimension dim , treating Not a Numbers ( NaNs ) as zero .", "question_id": 50005}
{"snippet": "torch.nansum(input, dtype=None)", "intent": "Returns the sum of all elements , treating Not a Numbers ( NaNs ) as zero . Returns the sum of each row of the `input` tensor in the given dimension dim , treating Not a Numbers ( NaNs ) as zero . With arguments `dtype`.", "question_id": 50006}
{"snippet": "Tensor.acos_()", "intent": "In-place version of acos ( )", "question_id": 50007}
{"snippet": "torch.linalg.norm(A)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( )", "question_id": 50008}
{"snippet": "torch.linalg.norm(A, ord=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed .", "question_id": 50009}
{"snippet": "torch.linalg.norm(A, dim=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) Using the `dim` argument to compute vector norms :", "question_id": 50010}
{"snippet": "torch.linalg.norm(A, keepdim=False)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `keepdim`.", "question_id": 50011}
{"snippet": "torch.linalg.norm(A, out=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `out`.", "question_id": 50012}
{"snippet": "torch.linalg.norm(A, dtype=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `dtype`.", "question_id": 50013}
{"snippet": "torch.linalg.norm(A, ord=None, dim=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . Using the `dim` argument to compute vector norms :", "question_id": 50014}
{"snippet": "torch.linalg.norm(A, ord=None, keepdim=False)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `keepdim`.", "question_id": 50015}
{"snippet": "torch.linalg.norm(A, ord=None, out=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `out`.", "question_id": 50016}
{"snippet": "torch.linalg.norm(A, ord=None, dtype=None)", "intent": "Computes a vector or matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the norm that is computed . With arguments `dtype`.", "question_id": 50017}
{"snippet": "Tensor.addr(vec1, vec2)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`.", "question_id": 50018}
{"snippet": "Tensor.addr(vec1, vec2, beta=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `beta`.", "question_id": 50019}
{"snippet": "Tensor.addr(vec1, vec2, alpha=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `alpha`.", "question_id": 50020}
{"snippet": "Tensor.addr(vec1, vec2, beta=1, alpha=1)", "intent": "See torch.addr ( ) With arguments `vec1`, `vec2`, `beta`, `alpha`.", "question_id": 50021}
{"snippet": "torch.quantization.observer.HistogramObserver()", "intent": "The module records the running histogram of tensor values along with min/max values .", "question_id": 50022}
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`.", "question_id": 50023}
{"snippet": "torch.quantization.observer.HistogramObserver(upsample_rate=128)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `upsample_rate`.", "question_id": 50024}
{"snippet": "torch.quantization.observer.HistogramObserver(dtype=torch.quint8)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `dtype`.", "question_id": 50025}
{"snippet": "torch.quantization.observer.HistogramObserver(qscheme=torch.per_tensor_affine)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `qscheme`.", "question_id": 50026}
{"snippet": "torch.quantization.observer.HistogramObserver(reduce_range=False)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `reduce_range`.", "question_id": 50027}
{"snippet": "torch.quantization.observer.HistogramObserver(factory_kwargs=None)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `factory_kwargs`.", "question_id": 50028}
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, upsample_rate=128)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `upsample_rate`.", "question_id": 50029}
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, dtype=torch.quint8)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `dtype`.", "question_id": 50030}
{"snippet": "torch.quantization.observer.HistogramObserver(bins=2048, qscheme=torch.per_tensor_affine)", "intent": "The module records the running histogram of tensor values along with min/max values . With arguments `bins`, `qscheme`.", "question_id": 50031}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 50032}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`.", "question_id": 50033}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`.", "question_id": 50034}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 50035}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `_random_samples`.", "question_id": 50036}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, output_ratio=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `output_ratio`.", "question_id": 50037}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `return_indices`.", "question_id": 50038}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_size=None, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `_random_samples`.", "question_id": 50039}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None, return_indices=False)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `return_indices`.", "question_id": 50040}
{"snippet": "torch.nn.FractionalMaxPool2d(kernel_size, output_ratio=None, _random_samples=None)", "intent": "Applies a 2D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `_random_samples`.", "question_id": 50041}
{"snippet": "torch.cuda.get_gencode_flags()", "intent": "Returns NVCC gencode flags this library was compiled with .", "question_id": 50042}
{"snippet": "Tensor.vdot(other)", "intent": "See torch.vdot ( ) With arguments `other`.", "question_id": 50043}
{"snippet": "torch.nn.functional.adaptive_max_pool3d(*args, **kwargs)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 50044}
{"snippet": "profile.total_average()", "intent": "Averages all events .", "question_id": 50045}
{"snippet": "torch.nn.quantized.Sigmoid(output_scale, output_zero_point)", "intent": "This is the quantized equivalent of Sigmoid . With arguments `output_scale`, `output_zero_point`.", "question_id": 50046}
{"snippet": "torch.logspace(start, end, steps)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base .", "question_id": 50047}
{"snippet": "torch.logspace(start, end, steps, base=10.0)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base .", "question_id": 50048}
{"snippet": "torch.logspace(start, end, steps, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `out`.", "question_id": 50049}
{"snippet": "torch.logspace(start, end, steps, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `dtype`.", "question_id": 50050}
{"snippet": "torch.logspace(start, end, steps, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `layout`.", "question_id": 50051}
{"snippet": "torch.logspace(start, end, steps, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `device`.", "question_id": 50052}
{"snippet": "torch.logspace(start, end, steps, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `requires_grad`.", "question_id": 50053}
{"snippet": "torch.logspace(start, end, steps, base=10.0, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `out`.", "question_id": 50054}
{"snippet": "torch.logspace(start, end, steps, base=10.0, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `dtype`.", "question_id": 50055}
{"snippet": "torch.logspace(start, end, steps, base=10.0, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from basestart { { \\text { { `base` } } } } ^ { { \\text { { `start` } } } } basestart to baseend { { \\text { { base } } } } ^ { { \\text { { `end` } } } } baseend , inclusive , on a logarithmic scale with base base . With arguments `layout`.", "question_id": 50056}
{"snippet": "torch.nn.parameter.UninitializedParameter()", "intent": "A parameter that is not initialized .", "question_id": 50057}
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True)", "intent": "A parameter that is not initialized . With arguments `requires_grad`.", "question_id": 50058}
{"snippet": "torch.nn.parameter.UninitializedParameter(device=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter .", "question_id": 50059}
{"snippet": "torch.nn.parameter.UninitializedParameter(dtype=None)", "intent": "A parameter that is not initialized . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g .", "question_id": 50060}
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . With arguments `requires_grad`.", "question_id": 50061}
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, dtype=None)", "intent": "A parameter that is not initialized . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 50062}
{"snippet": "torch.nn.parameter.UninitializedParameter(device=None, dtype=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g .", "question_id": 50063}
{"snippet": "torch.nn.parameter.UninitializedParameter(requires_grad=True, device=None, dtype=None)", "intent": "A parameter that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.nn.Parameter . The default device or `dtype` to use when the parameter is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 50064}
{"snippet": "uninitialized_parameter.cls_to_become", "intent": "alias of torch.nn.parameter.Parameter", "question_id": 50065}
{"snippet": "torch.nn.HingeEmbeddingLoss()", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) .", "question_id": 50066}
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`.", "question_id": 50067}
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 50068}
{"snippet": "torch.nn.HingeEmbeddingLoss(reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 50069}
{"snippet": "torch.nn.HingeEmbeddingLoss(reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 50070}
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `size_average`.", "question_id": 50071}
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduce`.", "question_id": 50072}
{"snippet": "torch.nn.HingeEmbeddingLoss(margin=1.0, reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `margin`, `reduction`.", "question_id": 50073}
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None, reduce=None)", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 50074}
{"snippet": "torch.nn.HingeEmbeddingLoss(size_average=None, reduction='mean')", "intent": "Measures the loss given an input tensor xxx and a labels tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 50075}
{"snippet": "Tensor.sparse_resize_and_clear_(size, sparse_dim, dense_dim)", "intent": "Removes all specified elements from a sparse tensor self and resizes self to the desired `size` and the number of sparse and dense dimensions . With arguments `sparse_dim`, `dense_dim`.", "question_id": 50076}
{"snippet": "torch.normal(mean, std)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution", "question_id": 50077}
{"snippet": "torch.normal(mean, std, generator=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `generator`.", "question_id": 50078}
{"snippet": "torch.normal(mean, std, out=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `out`.", "question_id": 50079}
{"snippet": "torch.normal(mean, std, generator=None, out=None)", "intent": "Returns a tensor of random numbers drawn from separate normal distributions whose `mean` and standard deviation are given . The `std` is a tensor with the standard deviation of each output element \u2019 s normal distribution With arguments `generator`, `out`.", "question_id": 50080}
{"snippet": "Tensor.sum_to_size(*size)", "intent": "Sum this tensor to size . With arguments `*size`.", "question_id": 50081}
{"snippet": "torch.nn.functional.relu(input)", "intent": "Applies the rectified linear unit function element-wise . With arguments `input`.", "question_id": 50082}
{"snippet": "torch.nn.functional.relu(input, inplace=False)", "intent": "Applies the rectified linear unit function element-wise . With arguments `input`, `inplace`.", "question_id": 50083}
{"snippet": "torch.linalg.eigvals(A)", "intent": "Computes the eigenvalues of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 50084}
{"snippet": "torch.linalg.eigvals(A, out=None)", "intent": "Computes the eigenvalues of a square matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 50085}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 50086}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 50087}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 50088}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 50089}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 50090}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 50091}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 50092}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 50093}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 50094}
{"snippet": "torch.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 1D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 50095}
{"snippet": "conv1d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 50096}
{"snippet": "Tensor.isinf()", "intent": "See torch.isinf ( )", "question_id": 50097}
{"snippet": "Tensor.neg_()", "intent": "In-place version of neg ( )", "question_id": 50098}
{"snippet": "Tensor.matrix_exp()", "intent": "See torch.matrix_exp ( )", "question_id": 50099}
{"snippet": "torch.randint(high, size, \\*)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`.", "question_id": 50100}
{"snippet": "torch.randint(high, size, \\*, low=0)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`.", "question_id": 50101}
{"snippet": "torch.randint(high, size, \\*, generator=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `generator`.", "question_id": 50102}
{"snippet": "torch.randint(high, size, \\*, out=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `out`.", "question_id": 50103}
{"snippet": "torch.randint(high, size, \\*, dtype=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `dtype`.", "question_id": 50104}
{"snippet": "torch.randint(high, size, \\*, layout=torch.strided)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `layout`.", "question_id": 50105}
{"snippet": "torch.randint(high, size, \\*, device=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `device`.", "question_id": 50106}
{"snippet": "torch.randint(high, size, \\*, requires_grad=False)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `requires_grad`.", "question_id": 50107}
{"snippet": "torch.randint(high, size, \\*, low=0, generator=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `generator`.", "question_id": 50108}
{"snippet": "torch.randint(high, size, \\*, low=0, out=None)", "intent": "Returns a tensor filled with random integers generated uniformly between `low` ( inclusive ) and `high` ( exclusive ) . The shape of the tensor is defined by the variable argument `size` . With arguments `\\*`, `out`.", "question_id": 50109}
{"snippet": "torch.linalg.matrix_rank(A)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 50110}
{"snippet": "torch.linalg.matrix_rank(A, tol=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold .", "question_id": 50111}
{"snippet": "torch.linalg.matrix_rank(A, hermitian=False)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`.", "question_id": 50112}
{"snippet": "torch.linalg.matrix_rank(A, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 50113}
{"snippet": "torch.linalg.matrix_rank(A, tol=None, hermitian=False)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `hermitian`.", "question_id": 50114}
{"snippet": "torch.linalg.matrix_rank(A, tol=None, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `out`.", "question_id": 50115}
{"snippet": "torch.linalg.matrix_rank(A, hermitian=False, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `hermitian`, `out`.", "question_id": 50116}
{"snippet": "torch.linalg.matrix_rank(A, tol=None, hermitian=False, out=None)", "intent": "Computes the numerical rank of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The matrix rank is computed as the number of singular values ( or eigenvalues in absolute value when hermitian= True ) that are greater than the specified `tol` threshold . With arguments `hermitian`, `out`.", "question_id": 50117}
{"snippet": "torch.lobpcg(A)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 50118}
{"snippet": "torch.lobpcg(A, k=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 50119}
{"snippet": "torch.lobpcg(A, B=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `B`.", "question_id": 50120}
{"snippet": "torch.lobpcg(A, X=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `X`.", "question_id": 50121}
{"snippet": "torch.lobpcg(A, n=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `n`.", "question_id": 50122}
{"snippet": "torch.lobpcg(A, iK=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `iK`.", "question_id": 50123}
{"snippet": "torch.lobpcg(A, niter=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `niter`.", "question_id": 50124}
{"snippet": "torch.lobpcg(A, tol=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . With arguments `tol`.", "question_id": 50125}
{"snippet": "torch.lobpcg(A, largest=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements .", "question_id": 50126}
{"snippet": "torch.lobpcg(A, method=None)", "intent": "Find the `k` `largest` ( or smallest ) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods . ( 2002 ) `A` Block Orthogonalization Procedure with Constant Synchronization Requirements . This function is a front-end to the following LOBPCG algorithms selectable via `method` argument :", "question_id": 50127}
{"snippet": "Tensor.topk(k)", "intent": "See torch.topk ( ) With arguments `k`.", "question_id": 50128}
{"snippet": "Tensor.topk(k, dim=None)", "intent": "See torch.topk ( ) With arguments `k`, `dim`.", "question_id": 50129}
{"snippet": "Tensor.topk(k, largest=True)", "intent": "See torch.topk ( ) With arguments `k`, `largest`.", "question_id": 50130}
{"snippet": "Tensor.topk(k, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `sorted`.", "question_id": 50131}
{"snippet": "Tensor.topk(k, dim=None, largest=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `largest`.", "question_id": 50132}
{"snippet": "Tensor.topk(k, dim=None, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `sorted`.", "question_id": 50133}
{"snippet": "Tensor.topk(k, largest=True, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `largest`, `sorted`.", "question_id": 50134}
{"snippet": "Tensor.topk(k, dim=None, largest=True, sorted=True)", "intent": "See torch.topk ( ) With arguments `k`, `dim`, `largest`, `sorted`.", "question_id": 50135}
{"snippet": "torch.optim.LBFGS(params)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`.", "question_id": 50136}
{"snippet": "torch.optim.LBFGS(params, lr=1)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`.", "question_id": 50137}
{"snippet": "torch.optim.LBFGS(params, max_iter=20)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `max_iter`.", "question_id": 50138}
{"snippet": "torch.optim.LBFGS(params, max_eval=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `max_eval`.", "question_id": 50139}
{"snippet": "torch.optim.LBFGS(params, tolerance_grad=1e-07)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `tolerance_grad`.", "question_id": 50140}
{"snippet": "torch.optim.LBFGS(params, tolerance_change=1e-09)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `tolerance_change`.", "question_id": 50141}
{"snippet": "torch.optim.LBFGS(params, history_size=100)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `history_size`.", "question_id": 50142}
{"snippet": "torch.optim.LBFGS(params, line_search_fn=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `line_search_fn`.", "question_id": 50143}
{"snippet": "torch.optim.LBFGS(params, lr=1, max_iter=20)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`, `max_iter`.", "question_id": 50144}
{"snippet": "torch.optim.LBFGS(params, lr=1, max_eval=None)", "intent": "Implements L-BFGS algorithm , heavily inspired by minFunc . With arguments `params`, `lr`, `max_eval`.", "question_id": 50145}
{"snippet": "lbfgs.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 50146}
{"snippet": "lbfgs.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 50147}
{"snippet": "lbfgs.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 50148}
{"snippet": "lbfgs.step(closure)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 50149}
{"snippet": "lbfgs.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 50150}
{"snippet": "lbfgs.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 50151}
{"snippet": "torch.nn.Dropout()", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 50152}
{"snippet": "torch.nn.Dropout(p=0.5)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution .", "question_id": 50153}
{"snippet": "torch.nn.Dropout(inplace=False)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 50154}
{"snippet": "torch.nn.Dropout(p=0.5, inplace=False)", "intent": "During training , randomly zeroes some of the elements of the input tensor with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 50155}
{"snippet": "Tensor.reciprocal_()", "intent": "In-place version of reciprocal ( )", "question_id": 50156}
{"snippet": "torch.logdet(input)", "intent": "Calculates log determinant of a square matrix or batches of square matrices . With arguments `input`.", "question_id": 50157}
{"snippet": "Tensor.erfinv()", "intent": "See torch.erfinv ( )", "question_id": 50158}
{"snippet": "torch.cuda.comm.broadcast(tensor)", "intent": "Broadcasts a `tensor` to specified GPU `devices` .", "question_id": 50159}
{"snippet": "torch.cuda.comm.broadcast(tensor, devices=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` .", "question_id": 50160}
{"snippet": "torch.cuda.comm.broadcast(tensor, out=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` . With arguments `out`.", "question_id": 50161}
{"snippet": "torch.cuda.comm.broadcast(tensor, devices=None, out=None)", "intent": "Broadcasts a `tensor` to specified GPU `devices` . With arguments `out`.", "question_id": 50162}
{"snippet": "torch.outer(input, vec2)", "intent": "Outer product of `input` and `vec2` .", "question_id": 50163}
{"snippet": "torch.outer(input, vec2, out=None)", "intent": "Outer product of `input` and `vec2` . If input is a vector of size nnn and vec2 is a vector of size mmm , then `out` must be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 50164}
{"snippet": "torch.nn.functional.upsample(input)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 50165}
{"snippet": "torch.nn.functional.upsample(input, size=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 50166}
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 50167}
{"snippet": "torch.nn.functional.upsample(input, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 50168}
{"snippet": "torch.nn.functional.upsample(input, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 50169}
{"snippet": "torch.nn.functional.upsample(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor`", "question_id": 50170}
{"snippet": "torch.nn.functional.upsample(input, size=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 50171}
{"snippet": "torch.nn.functional.upsample(input, size=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 50172}
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None, mode='nearest')", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` The algorithm used for upsampling is determined by `mode` .", "question_id": 50173}
{"snippet": "torch.nn.functional.upsample(input, scale_factor=None, align_corners=None)", "intent": "Upsamples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 50174}
{"snippet": "Tensor.ger(vec2)", "intent": "See torch.ger ( ) With arguments `vec2`.", "question_id": 50175}
{"snippet": "torch.jit.script(obj)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`.", "question_id": 50176}
{"snippet": "torch.jit.script(obj, optimize=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`.", "question_id": 50177}
{"snippet": "torch.jit.script(obj, _frames_up=0)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_frames_up`.", "question_id": 50178}
{"snippet": "torch.jit.script(obj, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_rcb`.", "question_id": 50179}
{"snippet": "torch.jit.script(obj, optimize=None, _frames_up=0)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_frames_up`.", "question_id": 50180}
{"snippet": "torch.jit.script(obj, optimize=None, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_rcb`.", "question_id": 50181}
{"snippet": "torch.jit.script(obj, _frames_up=0, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `_frames_up`, `_rcb`.", "question_id": 50182}
{"snippet": "torch.jit.script(obj, optimize=None, _frames_up=0, _rcb=None)", "intent": "Scripting a function or nn.Module will inspect the source code , compile it as TorchScript code using the TorchScript compiler , and return a ScriptModule or ScriptFunction . With arguments `obj`, `optimize`, `_frames_up`, `_rcb`.", "question_id": 50183}
{"snippet": "torch.mv(input, vec)", "intent": "Performs a matrix-vector product of the matrix `input` and the vector `vec` .", "question_id": 50184}
{"snippet": "torch.mv(input, vec, out=None)", "intent": "Performs a matrix-vector product of the matrix `input` and the vector `vec` . If input is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , vec is a 1-D tensor of size mmm , `out` will be 1-D of size nnn .", "question_id": 50185}
{"snippet": "torch.sparse.mm(mat1, mat2)", "intent": "Performs a matrix multiplication of the sparse matrix `mat1` and the ( sparse or strided ) matrix `mat2` .", "question_id": 50186}
{"snippet": "torch.mul(input, other)", "intent": "Multiplies each element of the `input` input with the scalar `other` and returns a new resulting tensor .", "question_id": 50187}
{"snippet": "torch.mul(input, other, out=None)", "intent": "Multiplies each element of the `input` input with the scalar `other` and returns a new resulting tensor . With arguments `out`.", "question_id": 50188}
{"snippet": "Tensor.index_copy(tensor1, dim, index, tensor2)", "intent": "Out-of-place version of torch.Tensor.index_copy_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_copy_ ( ) . With arguments `dim`, `index`, `tensor2`.", "question_id": 50189}
{"snippet": "Tensor.argmin()", "intent": "See torch.argmin ( )", "question_id": 50190}
{"snippet": "Tensor.argmin(dim=None)", "intent": "See torch.argmin ( ) With arguments `dim`.", "question_id": 50191}
{"snippet": "Tensor.argmin(keepdim=False)", "intent": "See torch.argmin ( ) With arguments `keepdim`.", "question_id": 50192}
{"snippet": "Tensor.argmin(dim=None, keepdim=False)", "intent": "See torch.argmin ( ) With arguments `dim`, `keepdim`.", "question_id": 50193}
{"snippet": "torch.fft.irfft2(input, - 1))", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`.", "question_id": 50194}
{"snippet": "torch.fft.irfft2(input, - 1), s=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`.", "question_id": 50195}
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`.", "question_id": 50196}
{"snippet": "torch.fft.irfft2(input, - 1), norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `norm`.", "question_id": 50197}
{"snippet": "torch.fft.irfft2(input, - 1), out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `out`.", "question_id": 50198}
{"snippet": "torch.fft.irfft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `dim`.", "question_id": 50199}
{"snippet": "torch.fft.irfft2(input, - 1), s=None, norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `norm`.", "question_id": 50200}
{"snippet": "torch.fft.irfft2(input, - 1), s=None, out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . So , it is recommended to always pass the signal shape `s` . With arguments `- 1)`, `out`.", "question_id": 50201}
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`, `norm`.", "question_id": 50202}
{"snippet": "torch.fft.irfft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the inverse of rfft2 ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft2 ( ) . With arguments `- 1)`, `dim`, `out`.", "question_id": 50203}
{"snippet": "Tensor.is_floating_point()", "intent": "Returns True if the data type of self is a floating point data type .", "question_id": 50204}
{"snippet": "Tensor.tolist()", "intent": "Returns the tensor as a ( nested ) list .", "question_id": 50205}
{"snippet": "Tensor.type(**kwargs)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`.", "question_id": 50206}
{"snippet": "Tensor.type(**kwargs, dtype=None)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`.", "question_id": 50207}
{"snippet": "Tensor.type(**kwargs, non_blocking=False)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`, `non_blocking`.", "question_id": 50208}
{"snippet": "Tensor.type(**kwargs, dtype=None, non_blocking=False)", "intent": "Returns the type if `dtype` is not provided , else casts this object to the specified type . With arguments `**kwargs`, `non_blocking`.", "question_id": 50209}
{"snippet": "Tensor.logical_or()", "intent": "See torch.logical_or ( )", "question_id": 50210}
{"snippet": "torch.argmax(input)", "intent": "Returns the indices of the maximum value of all elements in the `input` tensor .", "question_id": 50211}
{"snippet": "torch.scatter_add(input, dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_add_ ( ) With arguments `input`, `dim`, `index`, `src`.", "question_id": 50212}
{"snippet": "torch.nn.quantized.QFunctional", "intent": "Wrapper class for quantized operations.", "question_id": 50213}
{"snippet": "torch.quantization.observer.load_observer_state_dict(mod, obs_dict)", "intent": "Given input model and a state_dict containing model observer stats , load the stats back into the model . With arguments `mod`, `obs_dict`.", "question_id": 50214}
{"snippet": "torch.nn.MaxPool2d(kernel_size)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as :", "question_id": 50215}
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be :", "question_id": 50216}
{"snippet": "torch.nn.MaxPool2d(kernel_size, padding=0)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 50217}
{"snippet": "torch.nn.MaxPool2d(kernel_size, dilation=1)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : `dilation` controls the spacing between the kernel points .", "question_id": 50218}
{"snippet": "torch.nn.MaxPool2d(kernel_size, return_indices=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `return_indices`.", "question_id": 50219}
{"snippet": "torch.nn.MaxPool2d(kernel_size, ceil_mode=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 50220}
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 50221}
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : `dilation` controls the spacing between the kernel points .", "question_id": 50222}
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `return_indices`.", "question_id": 50223}
{"snippet": "torch.nn.MaxPool2d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `ceil_mode`.", "question_id": 50224}
{"snippet": "Tensor.exponential_()", "intent": "Fills self tensor with elements drawn from the exponential distribution :", "question_id": 50225}
{"snippet": "Tensor.exponential_(lambd=1)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `lambd`.", "question_id": 50226}
{"snippet": "Tensor.exponential_(generator=None)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `generator`.", "question_id": 50227}
{"snippet": "Tensor.exponential_(lambd=1, generator=None)", "intent": "Fills self tensor with elements drawn from the exponential distribution : With arguments `lambd`, `generator`.", "question_id": 50228}
{"snippet": "torch.get_default_dtype()", "intent": "Get the current default floating point torch.dtype .", "question_id": 50229}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs)", "intent": "Simulate the quantize and dequantize operations in training time . With arguments `**observer_kwargs`.", "question_id": 50230}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>)", "intent": "Simulate the quantize and dequantize operations in training time . With arguments `**observer_kwargs`, `observer`.", "question_id": 50231}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_min=0)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 50232}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 50233}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 50234}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 50235}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, quant_min=0, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`.", "question_id": 50236}
{"snippet": "torch.quantization.fake_quantize.FakeQuantize(**observer_kwargs, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255)", "intent": "Simulate the quantize and dequantize operations in training time . x_out = ( clamp ( round ( x/scale + zero_point ) , `quant_min` , `quant_max` ) -zero_point ) * scale With arguments `**observer_kwargs`, `observer`.", "question_id": 50237}
{"snippet": "torch.nn.intrinsic.ConvBnReLU3d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 3d , Batch Norm 3d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 50238}
{"snippet": "torch.nn.utils.parametrize.ParametrizationList(modules, original)", "intent": "A sequential container that holds and manages the `original` parameter or buffer of a parametrized torch.nn.Module . With arguments `modules`.", "question_id": 50239}
{"snippet": "parametrization_list.set_original_(value)", "intent": "This method is called when assigning to a parametrized tensor . With arguments `value`.", "question_id": 50240}
{"snippet": "torch.fmod(input, other)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the dividend `input` . With arguments `other`.", "question_id": 50241}
{"snippet": "torch.fmod(input, other, out=None)", "intent": "Computes the element-wise remainder of division . The remainder has the same sign as the dividend `input` . With arguments `other`, `out`.", "question_id": 50242}
{"snippet": "Tensor.median()", "intent": "See torch.median ( )", "question_id": 50243}
{"snippet": "Tensor.median(dim=None)", "intent": "See torch.median ( ) With arguments `dim`.", "question_id": 50244}
{"snippet": "Tensor.median(keepdim=False)", "intent": "See torch.median ( ) With arguments `keepdim`.", "question_id": 50245}
{"snippet": "Tensor.median(dim=None, keepdim=False)", "intent": "See torch.median ( ) With arguments `dim`, `keepdim`.", "question_id": 50246}
{"snippet": "torch.quantization.observer.RecordingObserver(**kwargs)", "intent": "The module is mainly for debug and records the tensor values during runtime . With arguments `**kwargs`.", "question_id": 50247}
{"snippet": "torch.diagonal(input)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 50248}
{"snippet": "torch.diagonal(input, offset=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 50249}
{"snippet": "torch.diagonal(input, dim1=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 50250}
{"snippet": "torch.diagonal(input, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 50251}
{"snippet": "torch.diagonal(input, offset=0, dim1=0)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 50252}
{"snippet": "torch.diagonal(input, offset=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 50253}
{"snippet": "torch.diagonal(input, dim1=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape .", "question_id": 50254}
{"snippet": "torch.diagonal(input, offset=0, dim1=0, dim2=1)", "intent": "Returns a partial view of `input` with the its diagonal elements with respect to `dim1` and `dim2` appended as a dimension at the end of the shape . The argument `offset` controls which diagonal to consider :", "question_id": 50255}
{"snippet": "torch.cuda.init()", "intent": "Initialize PyTorch \u2019 s CUDA state .", "question_id": 50256}
{"snippet": "torch.nn.functional.cross_entropy(input, target)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`.", "question_id": 50257}
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`.", "question_id": 50258}
{"snippet": "torch.nn.functional.cross_entropy(input, target, size_average=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `size_average`.", "question_id": 50259}
{"snippet": "torch.nn.functional.cross_entropy(input, target, ignore_index=- 100)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `ignore_index`.", "question_id": 50260}
{"snippet": "torch.nn.functional.cross_entropy(input, target, reduce=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `reduce`.", "question_id": 50261}
{"snippet": "torch.nn.functional.cross_entropy(input, target, reduction='mean')", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `reduction`.", "question_id": 50262}
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 50263}
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, ignore_index=- 100)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `ignore_index`.", "question_id": 50264}
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, reduce=None)", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `reduce`.", "question_id": 50265}
{"snippet": "torch.nn.functional.cross_entropy(input, target, weight=None, reduction='mean')", "intent": "This criterion combines log_softmax and nll_loss in a single function . With arguments `input`, `target`, `weight`, `reduction`.", "question_id": 50266}
{"snippet": "Tensor.arctan()", "intent": "See torch.arctan ( )", "question_id": 50267}
{"snippet": "torch.clip(input)", "intent": "Alias for torch.clamp ( ) . With arguments `input`.", "question_id": 50268}
{"snippet": "torch.clip(input, min=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`.", "question_id": 50269}
{"snippet": "torch.clip(input, max=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `max`.", "question_id": 50270}
{"snippet": "torch.clip(input, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `out`.", "question_id": 50271}
{"snippet": "torch.clip(input, min=None, max=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `max`.", "question_id": 50272}
{"snippet": "torch.clip(input, min=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `out`.", "question_id": 50273}
{"snippet": "torch.clip(input, max=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `max`, `out`.", "question_id": 50274}
{"snippet": "torch.clip(input, min=None, max=None, out=None)", "intent": "Alias for torch.clamp ( ) . With arguments `input`, `min`, `max`, `out`.", "question_id": 50275}
{"snippet": "torch.nn.functional.pixel_unshuffle(input, downscale_factor)", "intent": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) to a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) , where r is the `downscale_factor` . With arguments `input`.", "question_id": 50276}
{"snippet": "torch.ne(input, other)", "intent": "Computes input\u2260other\\text { `input` } \\neq \\text { `other` } input\ue020=other element-wise .", "question_id": 50277}
{"snippet": "torch.ne(input, other, out=None)", "intent": "Computes input\u2260other\\text { `input` } \\neq \\text { `other` } input\ue020=other element-wise . With arguments `out`.", "question_id": 50278}
{"snippet": "torch.sinc(input)", "intent": "Computes the normalized sinc of `input` .", "question_id": 50279}
{"snippet": "torch.sinc(input, out=None)", "intent": "Computes the normalized sinc of `input` . With arguments `out`.", "question_id": 50280}
{"snippet": "torch.rand(*size)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`.", "question_id": 50281}
{"snippet": "torch.rand(*size, out=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`.", "question_id": 50282}
{"snippet": "torch.rand(*size, dtype=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `dtype`.", "question_id": 50283}
{"snippet": "torch.rand(*size, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `layout`.", "question_id": 50284}
{"snippet": "torch.rand(*size, device=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `device`.", "question_id": 50285}
{"snippet": "torch.rand(*size, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `requires_grad`.", "question_id": 50286}
{"snippet": "torch.rand(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `dtype`.", "question_id": 50287}
{"snippet": "torch.rand(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `layout`.", "question_id": 50288}
{"snippet": "torch.rand(*size, out=None, device=None)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `device`.", "question_id": 50289}
{"snippet": "torch.rand(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with random numbers from a uniform distribution on the interval [ 0,1 ) [ 0 , 1 ) [ 0,1 ) With arguments `*size`, `out`, `requires_grad`.", "question_id": 50290}
{"snippet": "torch.nonzero(input)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` .", "question_id": 50291}
{"snippet": "torch.nonzero(input, out=None)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` . If input has nnn dimensions , then the resulting indices tensor `out` is of size ( z\u00d7n ) ( z \\times n ) ( z\u00d7n ) , where zzz is the total number of non-zero elements in the input tensor .", "question_id": 50292}
{"snippet": "torch.nonzero(input, as_tuple=False)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` .", "question_id": 50293}
{"snippet": "torch.nonzero(input, out=None, as_tuple=False)", "intent": "When `as_tuple` is `` False `` ( default ) : Returns a tensor containing the indices of all non-zero elements of `input` . If input has nnn dimensions , then the resulting indices tensor `out` is of size ( z\u00d7n ) ( z \\times n ) ( z\u00d7n ) , where zzz is the total number of non-zero elements in the input tensor .", "question_id": 50294}
{"snippet": "torch.nn.ReplicationPad3d(padding)", "intent": "Pads the input tensor using replication of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 50295}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`.", "question_id": 50296}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`.", "question_id": 50297}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, stride=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `stride`.", "question_id": 50298}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `padding`.", "question_id": 50299}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `output_padding`.", "question_id": 50300}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, groups=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `groups`.", "question_id": 50301}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `dilation`.", "question_id": 50302}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `stride`.", "question_id": 50303}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `padding`.", "question_id": 50304}
{"snippet": "torch.nn.functional.conv_transpose3d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d With arguments `weight`, `bias`, `output_padding`.", "question_id": 50305}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`.", "question_id": 50306}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`.", "question_id": 50307}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, padding=0)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `padding`.", "question_id": 50308}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, dilation=1)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `dilation`.", "question_id": 50309}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 50310}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, return_indices=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 50311}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 50312}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, dilation=1)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `dilation`.", "question_id": 50313}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 50314}
{"snippet": "torch.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, return_indices=False)", "intent": "Applies a 2D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `return_indices`.", "question_id": 50315}
{"snippet": "Tensor.greater_equal(other)", "intent": "See torch.greater_equal ( ) . With arguments `other`.", "question_id": 50316}
{"snippet": "torch.transpose(input, dim0, dim1)", "intent": "Returns a tensor that is a transposed version of `input` . The given dimensions `dim0` and `dim1` are swapped .", "question_id": 50317}
{"snippet": "torch.get_num_threads()", "intent": "Returns the number of threads used for parallelizing CPU operations", "question_id": 50318}
{"snippet": "torch.nn.functional.pdist(input)", "intent": "Computes the p-norm distance between every pair of row vectors in the `input` .", "question_id": 50319}
{"snippet": "torch.nn.functional.pdist(input, p=2)", "intent": "Computes the p-norm distance between every pair of row vectors in the `input` . This function is equivalent to scipy.spatial.distance.pdist ( input , \u2018 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) .", "question_id": 50320}
{"snippet": "torch.prod(input)", "intent": "Returns the product of all elements in the `input` tensor .", "question_id": 50321}
{"snippet": "torch.prod(input, dtype=None)", "intent": "Returns the product of all elements in the `input` tensor . With arguments `dtype`.", "question_id": 50322}
{"snippet": "Tensor.broadcast_to(shape)", "intent": "See torch.broadcast_to ( ) . With arguments `shape`.", "question_id": 50323}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 50324}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 50325}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 50326}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 50327}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 50328}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 50329}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 50330}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 50331}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 50332}
{"snippet": "torch.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU , attached with FakeQuantize modules for weight for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 50333}
{"snippet": "torch.as_strided(input, size, stride)", "intent": "Create a view of an existing torch.Tensor `input` with specified `size` , `stride` and `storage_offset` .", "question_id": 50334}
{"snippet": "torch.as_strided(input, size, stride, storage_offset=0)", "intent": "Create a view of an existing torch.Tensor `input` with specified `size` , `stride` and `storage_offset` .", "question_id": 50335}
{"snippet": "Tensor.std(dim)", "intent": "See torch.std ( ) With arguments `dim`.", "question_id": 50336}
{"snippet": "Tensor.std(dim, unbiased=True)", "intent": "See torch.std ( ) With arguments `dim`, `unbiased`.", "question_id": 50337}
{"snippet": "Tensor.std(dim, keepdim=False)", "intent": "See torch.std ( ) With arguments `dim`, `keepdim`.", "question_id": 50338}
{"snippet": "Tensor.std(dim, unbiased=True, keepdim=False)", "intent": "See torch.std ( ) With arguments `dim`, `unbiased`, `keepdim`.", "question_id": 50339}
{"snippet": "torch.logcumsumexp(input, dim)", "intent": "Returns the logarithm of the cumulative summation of the exponentiation of elements of `input` in the dimension `dim` .", "question_id": 50340}
{"snippet": "torch.logcumsumexp(input, dim, out=None)", "intent": "Returns the logarithm of the cumulative summation of the exponentiation of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 50341}
{"snippet": "Tensor.sign_()", "intent": "In-place version of sign ( )", "question_id": 50342}
{"snippet": "torch.row_stack(tensors)", "intent": "Alias of torch.vstack ( ) . With arguments `tensors`.", "question_id": 50343}
{"snippet": "torch.row_stack(tensors, out=None)", "intent": "Alias of torch.vstack ( ) . With arguments `tensors`, `out`.", "question_id": 50344}
{"snippet": "torch.sparse.addmm(mat, mat1, mat2)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`.", "question_id": 50345}
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, beta=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `beta`.", "question_id": 50346}
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, alpha=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `alpha`.", "question_id": 50347}
{"snippet": "torch.sparse.addmm(mat, mat1, mat2, beta=1.0, alpha=1.0)", "intent": "This function does exact same thing as torch.addmm ( ) in the forward , except that it supports backward for sparse matrix `mat1` . With arguments `mat`, `mat2`, `beta`, `alpha`.", "question_id": 50348}
{"snippet": "torch.quantization.prepare_qat(model)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version .", "question_id": 50349}
{"snippet": "torch.quantization.prepare_qat(model, mapping=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `mapping`.", "question_id": 50350}
{"snippet": "torch.quantization.prepare_qat(model, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `inplace`.", "question_id": 50351}
{"snippet": "torch.quantization.prepare_qat(model, mapping=None, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training and converts it to quantized version . With arguments `mapping`, `inplace`.", "question_id": 50352}
{"snippet": "torch.cuda.seed()", "intent": "Sets the seed for generating random numbers to a random number for the current GPU .", "question_id": 50353}
{"snippet": "torch.cuda.nvtx.mark(msg)", "intent": "Describe an instantaneous event that occurred at some point . With arguments `msg`.", "question_id": 50354}
{"snippet": "torch.repeat_interleave(input, repeats)", "intent": "Repeat elements of a tensor . If the `repeats` is tensor ( [ n1 , n2 , n3 , \u2026 ] ) , then the output will be tensor ( [ 0 , 0 , \u2026 , 1 , 1 , \u2026 , 2 , 2 , \u2026 , \u2026 ] ) where 0 appears n1 times , 1 appears n2 times , 2 appears n3 times , etc . With arguments `input`.", "question_id": 50355}
{"snippet": "torch.repeat_interleave(input, repeats, dim=None)", "intent": "Repeat elements of a tensor . If the `repeats` is tensor ( [ n1 , n2 , n3 , \u2026 ] ) , then the output will be tensor ( [ 0 , 0 , \u2026 , 1 , 1 , \u2026 , 2 , 2 , \u2026 , \u2026 ] ) where 0 appears n1 times , 1 appears n2 times , 2 appears n3 times , etc . With arguments `input`, `dim`.", "question_id": 50356}
{"snippet": "Tensor.atanh()", "intent": "See torch.atanh ( )", "question_id": 50357}
{"snippet": "Tensor.bitwise_xor_()", "intent": "In-place version of bitwise_xor ( )", "question_id": 50358}
{"snippet": "torch.meshgrid(*tensors)", "intent": "Take NNN tensors , each of which can be either scalar or 1-dimensional vector , and create NNN N-dimensional grids , where the iii th grid is defined by expanding the iii th input over dimensions defined by other inputs . With arguments `*tensors`.", "question_id": 50359}
{"snippet": "Tensor.atan2_(other)", "intent": "In-place version of atan2 ( ) With arguments `other`.", "question_id": 50360}
{"snippet": "torch.split(tensor, split_size_or_sections)", "intent": "Splits the `tensor` into chunks . If `split_size_or_sections` is an integer type , then tensor will be split into equally sized chunks ( if possible ) .", "question_id": 50361}
{"snippet": "torch.split(tensor, split_size_or_sections, dim=0)", "intent": "Splits the `tensor` into chunks . If `split_size_or_sections` is an integer type , then tensor will be split into equally sized chunks ( if possible ) . Last chunk will be smaller if the tensor size along the given dimension `dim` is not divisible by split_size .", "question_id": 50362}
{"snippet": "Tensor.flip(dims)", "intent": "See torch.flip ( ) With arguments `dims`.", "question_id": 50363}
{"snippet": "torch.quantization.qconfig.QConfigDynamic()", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights .", "question_id": 50364}
{"snippet": "torch.quantization.qconfig.QConfigDynamic(activation=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `activation`.", "question_id": 50365}
{"snippet": "torch.quantization.qconfig.QConfigDynamic(weight=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `weight`.", "question_id": 50366}
{"snippet": "torch.quantization.qconfig.QConfigDynamic(activation=<class 'torch.nn.modules.linear.Identity'>, weight=<class 'torch.nn.modules.linear.Identity'>)", "intent": "Describes how to dynamically quantize a layer or a part of the network by providing settings ( observer classes ) for weights . With arguments `activation`, `weight`.", "question_id": 50367}
{"snippet": "torch.nn.functional.normalize(input)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as", "question_id": 50368}
{"snippet": "torch.nn.functional.normalize(input, p=2.0)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`.", "question_id": 50369}
{"snippet": "torch.nn.functional.normalize(input, dim=1)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as", "question_id": 50370}
{"snippet": "torch.nn.functional.normalize(input, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `eps`.", "question_id": 50371}
{"snippet": "torch.nn.functional.normalize(input, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `out`.", "question_id": 50372}
{"snippet": "torch.nn.functional.normalize(input, p=2.0, dim=1)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`.", "question_id": 50373}
{"snippet": "torch.nn.functional.normalize(input, p=2.0, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`, `eps`.", "question_id": 50374}
{"snippet": "torch.nn.functional.normalize(input, p=2.0, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `p`, `out`.", "question_id": 50375}
{"snippet": "torch.nn.functional.normalize(input, dim=1, eps=1e-12)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `eps`.", "question_id": 50376}
{"snippet": "torch.nn.functional.normalize(input, dim=1, out=None)", "intent": "Performs LpL_pLp\u200b normalization of inputs over specified dimension . For a tensor `input` of sizes ( n0 , ... , ndim , ... , nk ) ( n_0 , ... , n_ { `dim` } , ... , n_k ) ( n0\u200b , ... , ndim\u200b , ... , nk\u200b ) , each ndimn_ { dim } ndim\u200b -element vector vvv along dimension dim is transformed as With arguments `out`.", "question_id": 50377}
{"snippet": "Tensor.asinh_()", "intent": "In-place version of asinh ( )", "question_id": 50378}
{"snippet": "torch.kaiser_window(window_length)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` .", "question_id": 50379}
{"snippet": "torch.kaiser_window(window_length, periodic=True)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length .", "question_id": 50380}
{"snippet": "torch.kaiser_window(window_length, beta=12.0)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` .", "question_id": 50381}
{"snippet": "torch.kaiser_window(window_length, dtype=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `dtype`.", "question_id": 50382}
{"snippet": "torch.kaiser_window(window_length, layout=torch.strided)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `layout`.", "question_id": 50383}
{"snippet": "torch.kaiser_window(window_length, device=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `device`.", "question_id": 50384}
{"snippet": "torch.kaiser_window(window_length, requires_grad=False)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . With arguments `requires_grad`.", "question_id": 50385}
{"snippet": "torch.kaiser_window(window_length, periodic=True, beta=12.0)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length .", "question_id": 50386}
{"snippet": "torch.kaiser_window(window_length, periodic=True, dtype=None)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length . With arguments `dtype`.", "question_id": 50387}
{"snippet": "torch.kaiser_window(window_length, periodic=True, layout=torch.strided)", "intent": "Computes the Kaiser window with window length `window_length` and shape parameter `beta` . Let I_0 be the zeroth order modified Bessel function of the first kind ( see torch.i0 ( ) ) and N = L - 1 if `periodic` is False and L if periodic is True , where L is the window_length . With arguments `layout`.", "question_id": 50388}
{"snippet": "Tensor.view_as(other)", "intent": "View this tensor as the same size as `other` .", "question_id": 50389}
{"snippet": "torch.nn.LSTM(*args, **kwargs)", "intent": "Applies a multi-layer long short-term memory ( LSTM ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 50390}
{"snippet": "Tensor.acosh()", "intent": "See torch.acosh ( )", "question_id": 50391}
{"snippet": "Tensor.hsplit(split_size_or_sections)", "intent": "See torch.hsplit ( ) With arguments `split_size_or_sections`.", "question_id": 50392}
{"snippet": "torch.quantization.add_observer_(module)", "intent": "Add observer for the leaf child of the `module` .", "question_id": 50393}
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`.", "question_id": 50394}
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`.", "question_id": 50395}
{"snippet": "torch.quantization.add_observer_(module, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `device`.", "question_id": 50396}
{"snippet": "torch.quantization.add_observer_(module, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `custom_module_class_mapping`.", "question_id": 50397}
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `non_leaf_module_list`.", "question_id": 50398}
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `device`.", "question_id": 50399}
{"snippet": "torch.quantization.add_observer_(module, qconfig_propagation_list=None, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `qconfig_propagation_list`, `custom_module_class_mapping`.", "question_id": 50400}
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None, device=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`, `device`.", "question_id": 50401}
{"snippet": "torch.quantization.add_observer_(module, non_leaf_module_list=None, custom_module_class_mapping=None)", "intent": "Add observer for the leaf child of the `module` . With arguments `non_leaf_module_list`, `custom_module_class_mapping`.", "question_id": 50402}
{"snippet": "torch.less_equal(input, other)", "intent": "Alias for torch.le ( ) . With arguments `input`, `other`.", "question_id": 50403}
{"snippet": "torch.less_equal(input, other, out=None)", "intent": "Alias for torch.le ( ) . With arguments `input`, `other`, `out`.", "question_id": 50404}
{"snippet": "torch.jit.trace(func, example_inputs)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`.", "question_id": 50405}
{"snippet": "torch.jit.trace(func, example_inputs, optimize=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `optimize`.", "question_id": 50406}
{"snippet": "torch.jit.trace(func, example_inputs, check_trace=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_trace`.", "question_id": 50407}
{"snippet": "torch.jit.trace(func, example_inputs, check_inputs=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_inputs`.", "question_id": 50408}
{"snippet": "torch.jit.trace(func, example_inputs, check_tolerance=1e-05)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `check_tolerance`.", "question_id": 50409}
{"snippet": "torch.jit.trace(func, example_inputs, strict=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `strict`.", "question_id": 50410}
{"snippet": "torch.jit.trace(func, example_inputs, _force_outplace=False)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_force_outplace`.", "question_id": 50411}
{"snippet": "torch.jit.trace(func, example_inputs, _module_class=None)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_module_class`.", "question_id": 50412}
{"snippet": "torch.jit.trace(func, example_inputs, _compilation_unit=<torch.jit.CompilationUnit object>)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `_compilation_unit`.", "question_id": 50413}
{"snippet": "torch.jit.trace(func, example_inputs, optimize=None, check_trace=True)", "intent": "Trace a function and return an executable or ScriptFunction that will be optimized using just-in-time compilation . With arguments `func`, `example_inputs`, `optimize`, `check_trace`.", "question_id": 50414}
{"snippet": "torch.sparse.log_softmax(input, dim)", "intent": "Applies a softmax function followed by logarithm . With arguments `input`, `dim`.", "question_id": 50415}
{"snippet": "torch.sparse.log_softmax(input, dim, dtype=None)", "intent": "Applies a softmax function followed by logarithm . With arguments `input`, `dim`, `dtype`.", "question_id": 50416}
{"snippet": "Tensor.scatter_add_(dim, index, src)", "intent": "Adds all values from the tensor other into self at the indices specified in the `index` tensor in a similar fashion as scatter_ ( ) . For each value in `src` , it is added to an index in self which is specified by its index in src for dimension ! = `dim` and by the corresponding value in index for dimension = dim .", "question_id": 50417}
{"snippet": "torch.quantize_per_channel(input, scales, zero_points, axis, dtype)", "intent": "Converts a float tensor to a per-channel quantized tensor with given `scales` and zero points . With arguments `input`, `zero_points`, `axis`, `dtype`.", "question_id": 50418}
{"snippet": "torch.nn.functional.logsigmoid(input)", "intent": "Applies element-wise LogSigmoid ( xi ) =log\u2061 ( 11+exp\u2061 ( \u2212xi ) ) \\text { LogSigmoid } ( x_i ) = \\log \\left ( \\frac { 1 } { 1 + \\exp ( -x_i ) } \\right ) LogSigmoid ( xi\u200b ) =log ( 1+exp ( \u2212xi\u200b ) 1\u200b ) With arguments `input`.", "question_id": 50419}
{"snippet": "Tensor.char()", "intent": "self.char ( ) is equivalent to self.to ( torch.int8 ) .", "question_id": 50420}
{"snippet": "Tensor.char(memory_format=torch.preserve_format)", "intent": "self.char ( ) is equivalent to self.to ( torch.int8 ) . With arguments `memory_format`.", "question_id": 50421}
{"snippet": "torch.cuda.nvtx.range_push(msg)", "intent": "Pushes a range onto a stack of nested range span . With arguments `msg`.", "question_id": 50422}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 50423}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 50424}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 50425}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 50426}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 50427}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 50428}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 50429}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 50430}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 50431}
{"snippet": "torch.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm1d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 50432}
{"snippet": "Tensor.index_select(dim, index)", "intent": "See torch.index_select ( ) With arguments `dim`, `index`.", "question_id": 50433}
{"snippet": "torch.nn.intrinsic.BNReLU2d(batch_norm, relu)", "intent": "This is a sequential container which calls the BatchNorm 2d and ReLU modules . With arguments `batch_norm`, `relu`.", "question_id": 50434}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`.", "question_id": 50435}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 50436}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, device=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 50437}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 50438}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, device=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 50439}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 50440}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, device=None, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `device`, `dtype`.", "question_id": 50441}
{"snippet": "torch.nn.LSTMCell(input_size, hidden_size, bias=True, device=None, dtype=None)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `input_size`, `hidden_size`, `bias`, `device`, `dtype`.", "question_id": 50442}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 50443}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`.", "question_id": 50444}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, max_norm=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 50445}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 50446}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 50447}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, sparse=False)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 50448}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, _weight=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 50449}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, dtype=torch.quint8)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 50450}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `max_norm`.", "question_id": 50451}
{"snippet": "torch.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, norm_type=2.0)", "intent": "A quantized Embedding module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `padding_idx`, `norm_type`.", "question_id": 50452}
{"snippet": "embedding.from_float(mod)", "intent": "Create a quantized embedding module from a float module With arguments `mod`.", "question_id": 50453}
{"snippet": "Tensor.stride(dim)", "intent": "Returns the stride of self tensor . Stride is the jump necessary to go from one element to the next one in the specified dimension `dim` .", "question_id": 50454}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`.", "question_id": 50455}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`.", "question_id": 50456}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `reduction`.", "question_id": 50457}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `zero_infinity`.", "question_id": 50458}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `reduction`.", "question_id": 50459}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `zero_infinity`.", "question_id": 50460}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `reduction`, `zero_infinity`.", "question_id": 50461}
{"snippet": "torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `log_probs`, `targets`, `input_lengths`, `target_lengths`, `blank`, `reduction`, `zero_infinity`.", "question_id": 50462}
{"snippet": "Tensor.cosh()", "intent": "See torch.cosh ( )", "question_id": 50463}
{"snippet": "torch.nn.utils.prune.random_structured(module, name, amount, dim)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` selected at random .", "question_id": 50464}
{"snippet": "torch.nn.CrossEntropyLoss()", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class .", "question_id": 50465}
{"snippet": "torch.nn.CrossEntropyLoss(weight=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes .", "question_id": 50466}
{"snippet": "torch.nn.CrossEntropyLoss(size_average=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `size_average`.", "question_id": 50467}
{"snippet": "torch.nn.CrossEntropyLoss(ignore_index=- 100)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . This criterion expects a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] as the target for each value of a 1D tensor of size minibatch ; if `ignore_index` is specified , this criterion also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 50468}
{"snippet": "torch.nn.CrossEntropyLoss(reduce=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `reduce`.", "question_id": 50469}
{"snippet": "torch.nn.CrossEntropyLoss(reduction='mean')", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . With arguments `reduction`.", "question_id": 50470}
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, size_average=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `size_average`.", "question_id": 50471}
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, ignore_index=- 100)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . This criterion expects a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] as the target for each value of a 1D tensor of size minibatch ; if `ignore_index` is specified , this criterion also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 50472}
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, reduce=None)", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduce`.", "question_id": 50473}
{"snippet": "torch.nn.CrossEntropyLoss(weight=None, reduction='mean')", "intent": "This criterion combines LogSoftmax and NLLLoss in one single class . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduction`.", "question_id": 50474}
{"snippet": "torch.jit.isinstance(obj, target_type)", "intent": "This function provides for conatiner type refinement in TorchScript . With arguments `obj`, `target_type`.", "question_id": 50475}
{"snippet": "Tensor.sinc_()", "intent": "In-place version of sinc ( )", "question_id": 50476}
{"snippet": "torch.median(input)", "intent": "Returns the median of the values in `input` .", "question_id": 50477}
{"snippet": "Tensor.inner(other)", "intent": "See torch.inner ( ) . With arguments `other`.", "question_id": 50478}
{"snippet": "Tensor.prod()", "intent": "See torch.prod ( )", "question_id": 50479}
{"snippet": "Tensor.prod(dim=None)", "intent": "See torch.prod ( ) With arguments `dim`.", "question_id": 50480}
{"snippet": "Tensor.prod(keepdim=False)", "intent": "See torch.prod ( ) With arguments `keepdim`.", "question_id": 50481}
{"snippet": "Tensor.prod(dtype=None)", "intent": "See torch.prod ( ) With arguments `dtype`.", "question_id": 50482}
{"snippet": "Tensor.prod(dim=None, keepdim=False)", "intent": "See torch.prod ( ) With arguments `dim`, `keepdim`.", "question_id": 50483}
{"snippet": "Tensor.prod(dim=None, dtype=None)", "intent": "See torch.prod ( ) With arguments `dim`, `dtype`.", "question_id": 50484}
{"snippet": "Tensor.prod(keepdim=False, dtype=None)", "intent": "See torch.prod ( ) With arguments `keepdim`, `dtype`.", "question_id": 50485}
{"snippet": "Tensor.prod(dim=None, keepdim=False, dtype=None)", "intent": "See torch.prod ( ) With arguments `dim`, `keepdim`, `dtype`.", "question_id": 50486}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 50487}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `dropout`.", "question_id": 50488}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, bias=True)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `bias`.", "question_id": 50489}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, add_bias_kv=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `add_bias_kv`.", "question_id": 50490}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, add_zero_attn=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `add_zero_attn`.", "question_id": 50491}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, kdim=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 50492}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, vdim=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`.", "question_id": 50493}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `batch_first`.", "question_id": 50494}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, device=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `device`.", "question_id": 50495}
{"snippet": "torch.nn.MultiheadAttention(embed_dim, num_heads, dtype=None)", "intent": "Allows the model to jointly attend to information from different representation subspaces . Note that if `kdim` and `vdim` are None , they will be set to `embed_dim` such that query , key , and value have the same number of features . With arguments `num_heads`, `dtype`.", "question_id": 50496}
{"snippet": "torch.cuda.default_stream()", "intent": "Returns the default Stream for a given `device` .", "question_id": 50497}
{"snippet": "torch.cuda.default_stream(device=None)", "intent": "Returns the default Stream for a given `device` .", "question_id": 50498}
{"snippet": "torch.erf(input)", "intent": "Alias for torch.special.erf ( ) . With arguments `input`.", "question_id": 50499}
{"snippet": "torch.erf(input, out=None)", "intent": "Alias for torch.special.erf ( ) . With arguments `input`, `out`.", "question_id": 50500}
{"snippet": "torch.nn.functional.hardtanh_(input)", "intent": "In-place version of hardtanh ( ) . With arguments `input`.", "question_id": 50501}
{"snippet": "torch.nn.functional.hardtanh_(input, min_val=- 1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `min_val`.", "question_id": 50502}
{"snippet": "torch.nn.functional.hardtanh_(input, max_val=1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `max_val`.", "question_id": 50503}
{"snippet": "torch.nn.functional.hardtanh_(input, min_val=- 1., max_val=1.)", "intent": "In-place version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`.", "question_id": 50504}
{"snippet": "torch.nn.quantized.FXFloatFunctional", "intent": "module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly", "question_id": 50505}
{"snippet": "Optimizer.step(closure)", "intent": "Performs a single optimization step ( parameter update ) . With arguments `closure`.", "question_id": 50506}
{"snippet": "torch.broadcast_shapes(*shapes)", "intent": "Similar to broadcast_tensors ( ) but for shapes . With arguments `*shapes`.", "question_id": 50507}
{"snippet": "torch.nn.Identity(*args, **kwargs)", "intent": "A placeholder identity operator that is argument-insensitive . With arguments `*args`, `**kwargs`.", "question_id": 50508}
{"snippet": "torch.cuda.nvtx.range_pop()", "intent": "Pops a range off of a stack of nested range spans .", "question_id": 50509}
{"snippet": "torch.unique(*args, **kwargs)", "intent": "Returns the unique elements of the input tensor . With arguments `*args`, `**kwargs`.", "question_id": 50510}
{"snippet": "torch.floor_divide(input, other)", "intent": "Computes `input` divided by `other` , elementwise , and rounds each quotient towards zero .", "question_id": 50511}
{"snippet": "torch.floor_divide(input, other, out=None)", "intent": "Computes `input` divided by `other` , elementwise , and rounds each quotient towards zero . With arguments `out`.", "question_id": 50512}
{"snippet": "torch.svd(input)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` .", "question_id": 50513}
{"snippet": "torch.svd(input, some=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition .", "question_id": 50514}
{"snippet": "torch.svd(input, compute_uv=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input .", "question_id": 50515}
{"snippet": "torch.svd(input, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . With arguments `out`.", "question_id": 50516}
{"snippet": "torch.svd(input, some=True, compute_uv=True)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input .", "question_id": 50517}
{"snippet": "torch.svd(input, some=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . With arguments `out`.", "question_id": 50518}
{"snippet": "torch.svd(input, compute_uv=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input . With arguments `out`.", "question_id": 50519}
{"snippet": "torch.svd(input, some=True, compute_uv=True, out=None)", "intent": "Computes the singular value decomposition of either a matrix or batch of matrices `input` . If `some` is True ( default ) , the method returns the reduced singular value decomposition . If `compute_uv` is False , the returned U and V will be zero-filled matrices of shape ( m , m ) and ( n , n ) respectively , and the same device as input . With arguments `out`.", "question_id": 50520}
{"snippet": "torch.any(input)", "intent": "Tests if any element in `input` evaluates to True .", "question_id": 50521}
{"snippet": "torch.nn.Module", "intent": "Base class for all neural network modules.", "question_id": 50522}
{"snippet": "module.add_module(name, module)", "intent": "Adds a child `module` to the current module . The module can be accessed as an attribute using the given `name` .", "question_id": 50523}
{"snippet": "module.apply(fn)", "intent": "Applies `fn` recursively to every submodule ( as returned by .children ( ) ) as well as self .", "question_id": 50524}
{"snippet": "module.bfloat16()", "intent": "Casts all floating point parameters and buffers to bfloat16 datatype .", "question_id": 50525}
{"snippet": "module.buffers()", "intent": "Returns an iterator over module buffers .", "question_id": 50526}
{"snippet": "module.buffers(recurse=True)", "intent": "Returns an iterator over module buffers . With arguments `recurse`.", "question_id": 50527}
{"snippet": "module.children()", "intent": "Returns an iterator over immediate children modules .", "question_id": 50528}
{"snippet": "module.cpu()", "intent": "Moves all model parameters and buffers to the CPU .", "question_id": 50529}
{"snippet": "module.cuda()", "intent": "Moves all model parameters and buffers to the GPU .", "question_id": 50530}
{"snippet": "module.cuda(device=None)", "intent": "Moves all model parameters and buffers to the GPU . With arguments `device`.", "question_id": 50531}
{"snippet": "module.double()", "intent": "Casts all floating point parameters and buffers to double datatype .", "question_id": 50532}
{"snippet": "module.dump_patches", "intent": "This allows better BC support for load_state_dict().", "question_id": 50533}
{"snippet": "module.eval()", "intent": "Sets the module in evaluation mode .", "question_id": 50534}
{"snippet": "module.extra_repr()", "intent": "Set the extra representation of the module", "question_id": 50535}
{"snippet": "module.float()", "intent": "Casts all floating point parameters and buffers to float datatype .", "question_id": 50536}
{"snippet": "module.forward(*input)", "intent": "Defines the computation performed at every call . With arguments `*input`.", "question_id": 50537}
{"snippet": "module.get_buffer(target)", "intent": "Returns the buffer given by `target` if it exists , otherwise throws an error .", "question_id": 50538}
{"snippet": "module.get_parameter(target)", "intent": "Returns the parameter given by `target` if it exists , otherwise throws an error .", "question_id": 50539}
{"snippet": "module.get_submodule(target)", "intent": "Returns the submodule given by `target` if it exists , otherwise throws an error .", "question_id": 50540}
{"snippet": "module.half()", "intent": "Casts all floating point parameters and buffers to half datatype .", "question_id": 50541}
{"snippet": "module.load_state_dict(state_dict)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants .", "question_id": 50542}
{"snippet": "module.load_state_dict(state_dict, strict=True)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants . If `strict` is True , then the keys of state_dict must exactly match the keys returned by this module \u2019 s state_dict ( ) function .", "question_id": 50543}
{"snippet": "module.modules()", "intent": "Returns an iterator over all modules in the network .", "question_id": 50544}
{"snippet": "module.named_buffers()", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself .", "question_id": 50545}
{"snippet": "module.named_buffers(prefix='')", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`.", "question_id": 50546}
{"snippet": "module.named_buffers(recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `recurse`.", "question_id": 50547}
{"snippet": "module.named_buffers(prefix='', recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`, `recurse`.", "question_id": 50548}
{"snippet": "module.named_children()", "intent": "Returns an iterator over immediate children modules , yielding both the name of the module as well as the module itself .", "question_id": 50549}
{"snippet": "module.named_modules()", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself .", "question_id": 50550}
{"snippet": "module.named_modules(memo=None)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`.", "question_id": 50551}
{"snippet": "module.named_modules(prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`.", "question_id": 50552}
{"snippet": "module.named_modules(remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `remove_duplicate`.", "question_id": 50553}
{"snippet": "module.named_modules(memo=None, prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`.", "question_id": 50554}
{"snippet": "module.named_modules(memo=None, remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `remove_duplicate`.", "question_id": 50555}
{"snippet": "module.named_modules(prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`, `remove_duplicate`.", "question_id": 50556}
{"snippet": "module.named_modules(memo=None, prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`, `remove_duplicate`.", "question_id": 50557}
{"snippet": "module.named_parameters()", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself .", "question_id": 50558}
{"snippet": "module.named_parameters(prefix='')", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`.", "question_id": 50559}
{"snippet": "module.named_parameters(recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `recurse`.", "question_id": 50560}
{"snippet": "module.named_parameters(prefix='', recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`, `recurse`.", "question_id": 50561}
{"snippet": "module.parameters()", "intent": "Returns an iterator over module parameters .", "question_id": 50562}
{"snippet": "module.parameters(recurse=True)", "intent": "Returns an iterator over module parameters . With arguments `recurse`.", "question_id": 50563}
{"snippet": "module.register_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 50564}
{"snippet": "module.register_buffer(name, tensor)", "intent": "Adds a buffer to the module . With arguments `name`, `tensor`.", "question_id": 50565}
{"snippet": "module.register_buffer(name, tensor, persistent=True)", "intent": "Adds a buffer to the module . Buffers , by default , are `persistent` and will be saved alongside parameters . With arguments `name`, `tensor`.", "question_id": 50566}
{"snippet": "module.register_forward_hook(hook)", "intent": "Registers a forward `hook` on the module .", "question_id": 50567}
{"snippet": "module.register_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook on the module . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 50568}
{"snippet": "module.register_full_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 50569}
{"snippet": "module.register_parameter(name, param)", "intent": "Adds a parameter to the module . The parameter can be accessed as an attribute using given `name` . With arguments `param`.", "question_id": 50570}
{"snippet": "module.requires_grad_()", "intent": "Change if autograd should record operations on parameters in this module .", "question_id": 50571}
{"snippet": "module.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on parameters in this module . This method sets the parameters \u2019 `requires_grad` attributes in-place .", "question_id": 50572}
{"snippet": "module.share_memory()", "intent": "See torch.Tensor.share_memory_ ( )", "question_id": 50573}
{"snippet": "module.state_dict()", "intent": "Returns a dictionary containing a whole state of the module .", "question_id": 50574}
{"snippet": "module.state_dict(destination=None)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`.", "question_id": 50575}
{"snippet": "module.state_dict(prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`.", "question_id": 50576}
{"snippet": "module.state_dict(keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `keep_vars`.", "question_id": 50577}
{"snippet": "module.state_dict(destination=None, prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`.", "question_id": 50578}
{"snippet": "module.state_dict(destination=None, keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `keep_vars`.", "question_id": 50579}
{"snippet": "module.state_dict(prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`, `keep_vars`.", "question_id": 50580}
{"snippet": "module.state_dict(destination=None, prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`, `keep_vars`.", "question_id": 50581}
{"snippet": "module.to(*args, **kwargs)", "intent": "Moves and/or casts the parameters and buffers . With arguments `*args`, `**kwargs`.", "question_id": 50582}
{"snippet": "module.to_empty(device)", "intent": "Moves the parameters and buffers to the specified `device` without copying storage .", "question_id": 50583}
{"snippet": "module.train()", "intent": "Sets the module in training `mode` .", "question_id": 50584}
{"snippet": "module.train(mode=True)", "intent": "Sets the module in training `mode` .", "question_id": 50585}
{"snippet": "module.type(dst_type)", "intent": "Casts all parameters and buffers to `dst_type` .", "question_id": 50586}
{"snippet": "module.xpu()", "intent": "Moves all model parameters and buffers to the XPU .", "question_id": 50587}
{"snippet": "module.xpu(device=None)", "intent": "Moves all model parameters and buffers to the XPU . With arguments `device`.", "question_id": 50588}
{"snippet": "module.zero_grad()", "intent": "Sets gradients of all model parameters to zero .", "question_id": 50589}
{"snippet": "module.zero_grad(set_to_none=False)", "intent": "Sets gradients of all model parameters to zero . With arguments `set_to_none`.", "question_id": 50590}
{"snippet": "torch.nn.functional.huber_loss(input, target)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`.", "question_id": 50591}
{"snippet": "torch.nn.functional.huber_loss(input, target, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 50592}
{"snippet": "torch.nn.functional.huber_loss(input, target, delta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`.", "question_id": 50593}
{"snippet": "torch.nn.functional.huber_loss(input, target, reduction='mean', delta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `delta` and a delta-scaled L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 50594}
{"snippet": "Tensor.bitwise_and()", "intent": "See torch.bitwise_and ( )", "question_id": 50595}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`.", "question_id": 50596}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`.", "question_id": 50597}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `ch_axis`.", "question_id": 50598}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `dtype`.", "question_id": 50599}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `qscheme`.", "question_id": 50600}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `reduce_range`.", "question_id": 50601}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `quant_min`.", "question_id": 50602}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `quant_max`.", "question_id": 50603}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01, ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`, `ch_axis`.", "question_id": 50604}
{"snippet": "torch.quantization.observer.MovingAveragePerChannelMinMaxObserver(**kwargs, averaging_constant=0.01, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `**kwargs`, `averaging_constant`, `dtype`.", "question_id": 50605}
{"snippet": "torch.is_tensor(obj)", "intent": "Returns True if `obj` is a PyTorch tensor .", "question_id": 50606}
{"snippet": "Tensor.argmax()", "intent": "See torch.argmax ( )", "question_id": 50607}
{"snippet": "Tensor.argmax(dim=None)", "intent": "See torch.argmax ( ) With arguments `dim`.", "question_id": 50608}
{"snippet": "Tensor.argmax(keepdim=False)", "intent": "See torch.argmax ( ) With arguments `keepdim`.", "question_id": 50609}
{"snippet": "Tensor.argmax(dim=None, keepdim=False)", "intent": "See torch.argmax ( ) With arguments `dim`, `keepdim`.", "question_id": 50610}
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`.", "question_id": 50611}
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- 1)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `last_epoch`.", "question_id": 50612}
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, verbose=False)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `verbose`.", "question_id": 50613}
{"snippet": "torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=- 1, verbose=False)", "intent": "Sets the learning rate of each parameter group to the initial lr times a given function . With arguments `optimizer`, `lr_lambda`, `last_epoch`, `verbose`.", "question_id": 50614}
{"snippet": "lambda_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 50615}
{"snippet": "lambda_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 50616}
{"snippet": "lambda_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 50617}
{"snippet": "lambda_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 50618}
{"snippet": "lambda_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 50619}
{"snippet": "torch.linalg.householder_product(A, tau)", "intent": "Computes the first n columns of a product of Householder matrices . With arguments `A`, `tau`.", "question_id": 50620}
{"snippet": "torch.linalg.householder_product(A, tau, out=None)", "intent": "Computes the first n columns of a product of Householder matrices . With arguments `A`, `tau`, `out`.", "question_id": 50621}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 50622}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 50623}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 50624}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 50625}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 50626}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 50627}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 50628}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 50629}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 50630}
{"snippet": "torch.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn3d module is a module fused from Conv3d and BatchNorm3d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 50631}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 50632}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 50633}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config_dict=None)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 50634}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 50635}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, convert_custom_config_dict=None)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default", "question_id": 50636}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 50637}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config_dict=None, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 50638}
{"snippet": "torch.quantization.quantize_fx.convert_fx(graph_module, is_reference=False, convert_custom_config_dict=None, _remove_qconfig=True)", "intent": "Convert a calibrated or trained model to a quantized model : param `graph_module` : A prepared and calibrated/trained model ( GraphModule ) : param `is_reference` : flag for whether to produce a reference quantized model , : param which will be a common interface between pytorch quantization with : : param other backends like accelerators : : param `convert_custom_config_dict` : dictionary for custom configurations for convert function : : param convert_custom_config_dict = { : # addtional object ( module/operator ) mappings that will overwrite the default With arguments `_remove_qconfig`.", "question_id": 50639}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`.", "question_id": 50640}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`.", "question_id": 50641}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dropout=0.1)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dropout`.", "question_id": 50642}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, activation='relu')", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `activation`.", "question_id": 50643}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, layer_norm_eps=1e-05)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `layer_norm_eps`.", "question_id": 50644}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, batch_first=False)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `batch_first`.", "question_id": 50645}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, device=None)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `device`.", "question_id": 50646}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dtype=None)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dtype`.", "question_id": 50647}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1)", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `dropout`.", "question_id": 50648}
{"snippet": "torch.nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, activation='relu')", "intent": "TransformerDecoderLayer is made up of self-attn , multi-head-attn and feedforward network . With arguments `d_model`, `nhead`, `dim_feedforward`, `activation`.", "question_id": 50649}
{"snippet": "transformer_decoder_layer.forward(tgt, memory)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`.", "question_id": 50650}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`.", "question_id": 50651}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`.", "question_id": 50652}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_key_padding_mask`.", "question_id": 50653}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_key_padding_mask`.", "question_id": 50654}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, memory_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `memory_mask`.", "question_id": 50655}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `tgt_key_padding_mask`.", "question_id": 50656}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, tgt_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `tgt_mask`, `memory_key_padding_mask`.", "question_id": 50657}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None, tgt_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`, `tgt_key_padding_mask`.", "question_id": 50658}
{"snippet": "transformer_decoder_layer.forward(tgt, memory, memory_mask=None, memory_key_padding_mask=None)", "intent": "Pass the inputs ( and mask ) through the decoder layer . With arguments `tgt`, `memory`, `memory_mask`, `memory_key_padding_mask`.", "question_id": 50659}
{"snippet": "torch.nn.GLU()", "intent": "Applies the gated linear unit function GLU ( a , b ) =a\u2297\u03c3 ( b ) { GLU } ( a , b ) = a \\otimes \\sigma ( b ) GLU ( a , b ) =a\u2297\u03c3 ( b ) where aaa is the first half of the input matrices and bbb is the second half .", "question_id": 50660}
{"snippet": "torch.nn.GLU(dim=- 1)", "intent": "Applies the gated linear unit function GLU ( a , b ) =a\u2297\u03c3 ( b ) { GLU } ( a , b ) = a \\otimes \\sigma ( b ) GLU ( a , b ) =a\u2297\u03c3 ( b ) where aaa is the first half of the input matrices and bbb is the second half . With arguments `dim`.", "question_id": 50661}
{"snippet": "Tensor.reshape_as(other)", "intent": "Returns this tensor as the same shape as `other` .", "question_id": 50662}
{"snippet": "torch.nextafter(input, other)", "intent": "Return the next floating-point value after `input` towards `other` , elementwise .", "question_id": 50663}
{"snippet": "torch.nextafter(input, other, out=None)", "intent": "Return the next floating-point value after `input` towards `other` , elementwise . With arguments `out`.", "question_id": 50664}
{"snippet": "Tensor.addmv_(mat, vec)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`.", "question_id": 50665}
{"snippet": "Tensor.addmv_(mat, vec, beta=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `beta`.", "question_id": 50666}
{"snippet": "Tensor.addmv_(mat, vec, alpha=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `alpha`.", "question_id": 50667}
{"snippet": "Tensor.addmv_(mat, vec, beta=1, alpha=1)", "intent": "In-place version of addmv ( ) With arguments `mat`, `vec`, `beta`, `alpha`.", "question_id": 50668}
{"snippet": "torch.msort(input)", "intent": "Sorts the elements of the `input` tensor along its first dimension in ascending order by value .", "question_id": 50669}
{"snippet": "torch.msort(input, out=None)", "intent": "Sorts the elements of the `input` tensor along its first dimension in ascending order by value . With arguments `out`.", "question_id": 50670}
{"snippet": "torch.cuda.get_rng_state_all()", "intent": "Returns a list of ByteTensor representing the random number states of all devices .", "question_id": 50671}
{"snippet": "Tensor.new_tensor(data)", "intent": "Returns a new Tensor with `data` as the tensor data .", "question_id": 50672}
{"snippet": "Tensor.new_tensor(data, dtype=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`.", "question_id": 50673}
{"snippet": "Tensor.new_tensor(data, device=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `device`.", "question_id": 50674}
{"snippet": "Tensor.new_tensor(data, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `requires_grad`.", "question_id": 50675}
{"snippet": "Tensor.new_tensor(data, dtype=None, device=None)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `device`.", "question_id": 50676}
{"snippet": "Tensor.new_tensor(data, dtype=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `requires_grad`.", "question_id": 50677}
{"snippet": "Tensor.new_tensor(data, device=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `device`, `requires_grad`.", "question_id": 50678}
{"snippet": "Tensor.new_tensor(data, dtype=None, device=None, requires_grad=False)", "intent": "Returns a new Tensor with `data` as the tensor data . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 50679}
{"snippet": "Tensor.qr()", "intent": "See torch.qr ( )", "question_id": 50680}
{"snippet": "Tensor.qr(some=True)", "intent": "See torch.qr ( ) With arguments `some`.", "question_id": 50681}
{"snippet": "torch.as_tensor(data)", "intent": "Convert the `data` into a torch.Tensor .", "question_id": 50682}
{"snippet": "torch.as_tensor(data, dtype=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 50683}
{"snippet": "torch.as_tensor(data, device=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 50684}
{"snippet": "torch.as_tensor(data, dtype=None, device=None)", "intent": "Convert the `data` into a torch.Tensor . If the data is already a Tensor with the same `dtype` and `device` , no copy will be performed , otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True .", "question_id": 50685}
{"snippet": "torch.ravel(input)", "intent": "Return a contiguous flattened tensor . With arguments `input`.", "question_id": 50686}
{"snippet": "Tensor.fmod(divisor)", "intent": "See torch.fmod ( ) With arguments `divisor`.", "question_id": 50687}
{"snippet": "torch.nn.modules.lazy.LazyModuleMixin(*args, **kwargs)", "intent": "A mixin for modules that lazily initialize parameters , also known as \u201c lazy modules . \u201d With arguments `*args`, `**kwargs`.", "question_id": 50688}
{"snippet": "lazy_module_mixin.has_uninitialized_params()", "intent": "Check if a module has parameters that are not initialized", "question_id": 50689}
{"snippet": "lazy_module_mixin.initialize_parameters(*args, **kwargs)", "intent": "Initialize parameters according to the input batch properties . With arguments `*args`, `**kwargs`.", "question_id": 50690}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`.", "question_id": 50691}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`.", "question_id": 50692}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`.", "question_id": 50693}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduction`.", "question_id": 50694}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`.", "question_id": 50695}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 50696}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 50697}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, size_average=None, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `size_average`.", "question_id": 50698}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None, reduction='mean')", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 50699}
{"snippet": "torch.nn.functional.smooth_l1_loss(input, target, reduce=None, beta=1.0)", "intent": "Function that uses a squared term if the absolute element-wise error falls below `beta` and an L1 term otherwise . With arguments `input`, `target`, `reduce`.", "question_id": 50700}
{"snippet": "Tensor.hypot_(other)", "intent": "In-place version of hypot ( ) With arguments `other`.", "question_id": 50701}
{"snippet": "torch.nn.RReLU()", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper :", "question_id": 50702}
{"snippet": "torch.nn.RReLU(lower=0.125)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 50703}
{"snippet": "torch.nn.RReLU(upper=0.3333333333333333)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 50704}
{"snippet": "torch.nn.RReLU(inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : With arguments `inplace`.", "question_id": 50705}
{"snippet": "torch.nn.RReLU(lower=0.125, upper=0.3333333333333333)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) .", "question_id": 50706}
{"snippet": "torch.nn.RReLU(lower=0.125, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 50707}
{"snippet": "torch.nn.RReLU(upper=0.3333333333333333, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 50708}
{"snippet": "torch.nn.RReLU(lower=0.125, upper=0.3333333333333333, inplace=False)", "intent": "Applies the randomized leaky rectified liner unit function , element-wise , as described in the paper : where aaa is randomly sampled from uniform distribution U ( `lower` , `upper` ) \\mathcal { U } ( \\text { lower } , \\text { upper } ) U ( lower , upper ) . With arguments `inplace`.", "question_id": 50709}
{"snippet": "Tensor.as_subclass(cls)", "intent": "Makes a `cls` instance with the same data pointer as self .", "question_id": 50710}
{"snippet": "torch.nn.ELU()", "intent": "Applies the element-wise function :", "question_id": 50711}
{"snippet": "torch.nn.ELU(alpha=1.0)", "intent": "Applies the element-wise function : With arguments `alpha`.", "question_id": 50712}
{"snippet": "torch.nn.ELU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 50713}
{"snippet": "torch.nn.ELU(alpha=1.0, inplace=False)", "intent": "Applies the element-wise function : With arguments `alpha`, `inplace`.", "question_id": 50714}
{"snippet": "torch.broadcast_to(input, shape)", "intent": "Broadcasts `input` to the `shape` shape .", "question_id": 50715}
{"snippet": "Tensor.greater(other)", "intent": "See torch.greater ( ) . With arguments `other`.", "question_id": 50716}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`.", "question_id": 50717}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 50718}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, padding=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 50719}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, dilation=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 50720}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, groups=1)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 50721}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, padding_mode='zeros')", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `padding_mode`.", "question_id": 50722}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, scale=1.0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `scale`.", "question_id": 50723}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, zero_point=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `zero_point`.", "question_id": 50724}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, dtype=torch.quint8)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `dtype`.", "question_id": 50725}
{"snippet": "torch.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0)", "intent": "Applies a 2D convolution over a quantized 2D `input` composed of several input planes . With arguments `weight`, `bias`, `stride`, `padding`.", "question_id": 50726}
{"snippet": "Tensor.squeeze()", "intent": "See torch.squeeze ( )", "question_id": 50727}
{"snippet": "Tensor.squeeze(dim=None)", "intent": "See torch.squeeze ( ) With arguments `dim`.", "question_id": 50728}
{"snippet": "torch.norm(input)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`.", "question_id": 50729}
{"snippet": "torch.norm(input, p='fro')", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`.", "question_id": 50730}
{"snippet": "torch.norm(input, dim=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `dim`.", "question_id": 50731}
{"snippet": "torch.norm(input, keepdim=False)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `keepdim`.", "question_id": 50732}
{"snippet": "torch.norm(input, out=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `out`.", "question_id": 50733}
{"snippet": "torch.norm(input, dtype=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `dtype`.", "question_id": 50734}
{"snippet": "torch.norm(input, p='fro', dim=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `dim`.", "question_id": 50735}
{"snippet": "torch.norm(input, p='fro', keepdim=False)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `keepdim`.", "question_id": 50736}
{"snippet": "torch.norm(input, p='fro', out=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `out`.", "question_id": 50737}
{"snippet": "torch.norm(input, p='fro', dtype=None)", "intent": "Returns the matrix norm or vector norm of a given tensor . With arguments `input`, `p`, `dtype`.", "question_id": 50738}
{"snippet": "torch.heaviside(input, values)", "intent": "Computes the Heaviside step function for each element in `input` . With arguments `values`.", "question_id": 50739}
{"snippet": "torch.heaviside(input, values, out=None)", "intent": "Computes the Heaviside step function for each element in `input` . With arguments `values`, `out`.", "question_id": 50740}
{"snippet": "torch.nn.Dropout2d()", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) .", "question_id": 50741}
{"snippet": "torch.nn.Dropout2d(p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 50742}
{"snippet": "torch.nn.Dropout2d(inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . With arguments `inplace`.", "question_id": 50743}
{"snippet": "torch.nn.Dropout2d(p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 50744}
{"snippet": "Tensor.as_strided(size, stride)", "intent": "See torch.as_strided ( ) With arguments `size`, `stride`.", "question_id": 50745}
{"snippet": "Tensor.as_strided(size, stride, storage_offset=0)", "intent": "See torch.as_strided ( ) With arguments `size`, `stride`, `storage_offset`.", "question_id": 50746}
{"snippet": "Tensor.get_device() -> Device ordinal (Integer)", "intent": "For CUDA tensors , this function returns the device ordinal of the GPU on which the tensor resides . With arguments `) -> Device ordinal (Integer`.", "question_id": 50747}
{"snippet": "torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs)", "intent": "Globally prunes tensors corresponding to all `parameters` in parameters by applying the specified `pruning_method` . With arguments `**kwargs`.", "question_id": 50748}
{"snippet": "torch.nn.utils.prune.global_unstructured(parameters, pruning_method, **kwargs, importance_scores=None)", "intent": "Globally prunes tensors corresponding to all `parameters` in parameters by applying the specified `pruning_method` . With arguments `**kwargs`, `importance_scores`.", "question_id": 50749}
{"snippet": "Tensor.polygamma(n)", "intent": "See torch.polygamma ( ) With arguments `n`.", "question_id": 50750}
{"snippet": "Tensor.arctanh_(other)", "intent": "In-place version of arctanh ( ) With arguments `other`.", "question_id": 50751}
{"snippet": "Tensor.less_equal_(other)", "intent": "In-place version of less_equal ( ) . With arguments `other`.", "question_id": 50752}
{"snippet": "Tensor.bitwise_or_()", "intent": "In-place version of bitwise_or ( )", "question_id": 50753}
{"snippet": "torch.nn.UpsamplingNearest2d()", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels .", "question_id": 50754}
{"snippet": "torch.nn.UpsamplingNearest2d(size=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 50755}
{"snippet": "torch.nn.UpsamplingNearest2d(scale_factor=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 50756}
{"snippet": "torch.nn.UpsamplingNearest2d(size=None, scale_factor=None)", "intent": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels . To specify the scale , it takes either the `size` or the `scale_factor` as it \u2019 s constructor argument .", "question_id": 50757}
{"snippet": "Tensor.divide_(value)", "intent": "In-place version of divide ( ) With arguments `value`.", "question_id": 50758}
{"snippet": "Tensor.divide_(value, rounding_mode=None)", "intent": "In-place version of divide ( ) With arguments `value`, `rounding_mode`.", "question_id": 50759}
{"snippet": "torch.nn.BatchNorm3d(num_features)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 50760}
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 50761}
{"snippet": "torch.nn.BatchNorm3d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 50762}
{"snippet": "torch.nn.BatchNorm3d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 50763}
{"snippet": "torch.nn.BatchNorm3d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 50764}
{"snippet": "torch.nn.BatchNorm3d(num_features, device=None)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 50765}
{"snippet": "torch.nn.BatchNorm3d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 50766}
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 50767}
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 50768}
{"snippet": "torch.nn.BatchNorm3d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 50769}
{"snippet": "Tensor.xlogy_(other)", "intent": "In-place version of xlogy ( ) With arguments `other`.", "question_id": 50770}
{"snippet": "torch.triangular_solve(b, A)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices .", "question_id": 50771}
{"snippet": "torch.triangular_solve(b, A, upper=True)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`.", "question_id": 50772}
{"snippet": "torch.triangular_solve(b, A, transpose=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `transpose`.", "question_id": 50773}
{"snippet": "torch.triangular_solve(b, A, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `unitriangular`.", "question_id": 50774}
{"snippet": "torch.triangular_solve(b, A, upper=True, transpose=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `transpose`.", "question_id": 50775}
{"snippet": "torch.triangular_solve(b, A, upper=True, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `unitriangular`.", "question_id": 50776}
{"snippet": "torch.triangular_solve(b, A, transpose=False, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `transpose`, `unitriangular`.", "question_id": 50777}
{"snippet": "torch.triangular_solve(b, A, upper=True, transpose=False, unitriangular=False)", "intent": "Solves a system of equations with a triangular coefficient matrix AAA and multiple right-hand sides bbb . torch.triangular_solve ( `b` , `A` ) can take in 2D inputs b , A or inputs that are batches of 2D matrices . With arguments `upper`, `transpose`, `unitriangular`.", "question_id": 50778}
{"snippet": "torch.sin(input)", "intent": "Returns a new tensor with the sine of the elements of `input` .", "question_id": 50779}
{"snippet": "torch.sin(input, out=None)", "intent": "Returns a new tensor with the sine of the elements of `input` . With arguments `out`.", "question_id": 50780}
{"snippet": "Tensor.cummin(dim)", "intent": "See torch.cummin ( ) With arguments `dim`.", "question_id": 50781}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 50782}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 50783}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 50784}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 50785}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 50786}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 50787}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 50788}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 50789}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 50790}
{"snippet": "torch.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 50791}
{"snippet": "conv2d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 50792}
{"snippet": "torch.autograd.no_grad", "intent": "Context-manager that disabled gradient calculation.", "question_id": 50793}
{"snippet": "torch.linalg.matrix_norm(A, - 1))", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`.", "question_id": 50794}
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro')", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`.", "question_id": 50795}
{"snippet": "torch.linalg.matrix_norm(A, - 1), dim=(- 2)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) Also supports batches of matrices : the norm will be computed over the dimensions specified by the 2-tuple `dim` and the other dimensions will be treated as batch dimensions . With arguments `- 1)`.", "question_id": 50796}
{"snippet": "torch.linalg.matrix_norm(A, - 1), keepdim=False)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `keepdim`.", "question_id": 50797}
{"snippet": "torch.linalg.matrix_norm(A, - 1), dtype=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `dtype`.", "question_id": 50798}
{"snippet": "torch.linalg.matrix_norm(A, - 1), out=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) With arguments `- 1)`, `out`.", "question_id": 50799}
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', dim=(- 2)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . Also supports batches of matrices : the norm will be computed over the dimensions specified by the 2-tuple `dim` and the other dimensions will be treated as batch dimensions . With arguments `- 1)`.", "question_id": 50800}
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', keepdim=False)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `keepdim`.", "question_id": 50801}
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', dtype=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `dtype`.", "question_id": 50802}
{"snippet": "torch.linalg.matrix_norm(A, - 1), ord='fro', out=None)", "intent": "Computes a matrix norm . If `A` is complex valued , it computes the norm of A.abs ( ) `ord` defines the matrix norm that is computed . With arguments `- 1)`, `out`.", "question_id": 50803}
{"snippet": "Tensor.positive()", "intent": "See torch.positive ( )", "question_id": 50804}
{"snippet": "torch.nn.intrinsic.LinearReLU(linear, relu)", "intent": "This is a sequential container which calls the Linear and ReLU modules . With arguments `linear`, `relu`.", "question_id": 50805}
{"snippet": "torch.fliplr(input)", "intent": "Flip tensor in the left/right direction , returning a new tensor . With arguments `input`.", "question_id": 50806}
{"snippet": "torch.randn_like(input)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) .", "question_id": 50807}
{"snippet": "torch.randn_like(input, dtype=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`.", "question_id": 50808}
{"snippet": "torch.randn_like(input, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `layout`.", "question_id": 50809}
{"snippet": "torch.randn_like(input, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `device`.", "question_id": 50810}
{"snippet": "torch.randn_like(input, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `requires_grad`.", "question_id": 50811}
{"snippet": "torch.randn_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `memory_format`.", "question_id": 50812}
{"snippet": "torch.randn_like(input, dtype=None, layout=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `layout`.", "question_id": 50813}
{"snippet": "torch.randn_like(input, dtype=None, device=None)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `device`.", "question_id": 50814}
{"snippet": "torch.randn_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `requires_grad`.", "question_id": 50815}
{"snippet": "torch.randn_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor with the same size as `input` that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like ( input ) is equivalent to torch.randn ( input.size ( ) , dtype=input.dtype , layout=input.layout , device=input.device ) . With arguments `dtype`, `memory_format`.", "question_id": 50816}
{"snippet": "torch.nn.functional.max_pool2d(*args, **kwargs)", "intent": "Applies a 2D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 50817}
{"snippet": "torch.cuda.can_device_access_peer(device, peer_device)", "intent": "Checks if peer access between two devices is possible . With arguments `device`, `peer_device`.", "question_id": 50818}
{"snippet": "torch.sinh(input)", "intent": "Returns a new tensor with the hyperbolic sine of the elements of `input` .", "question_id": 50819}
{"snippet": "torch.sinh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic sine of the elements of `input` . With arguments `out`.", "question_id": 50820}
{"snippet": "torch.nn.functional.adaptive_max_pool1d(*args, **kwargs)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 50821}
{"snippet": "Tensor.log10_()", "intent": "In-place version of log10 ( )", "question_id": 50822}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level .", "question_id": 50823}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, device_ids=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `device_ids`.", "question_id": 50824}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, output_device=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `output_device`.", "question_id": 50825}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, dim=0)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `dim`.", "question_id": 50826}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, broadcast_buffers=True)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `broadcast_buffers`.", "question_id": 50827}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, process_group=None)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `process_group`.", "question_id": 50828}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, bucket_cap_mb=25)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `bucket_cap_mb`.", "question_id": 50829}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, find_unused_parameters=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `find_unused_parameters`.", "question_id": 50830}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, check_reduction=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `check_reduction`.", "question_id": 50831}
{"snippet": "torch.nn.parallel.DistributedDataParallel(module, gradient_as_bucket_view=False)", "intent": "Implements distributed data parallelism that is based on torch.distributed package at the `module` level . With arguments `gradient_as_bucket_view`.", "question_id": 50832}
{"snippet": "distributed_data_parallel.join()", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes .", "question_id": 50833}
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . With arguments `divide_by_initial_world_size`.", "question_id": 50834}
{"snippet": "distributed_data_parallel.join(enable=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop .", "question_id": 50835}
{"snippet": "distributed_data_parallel.join(throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic .", "question_id": 50836}
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, enable=True)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . With arguments `divide_by_initial_world_size`.", "question_id": 50837}
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic . With arguments `divide_by_initial_world_size`.", "question_id": 50838}
{"snippet": "distributed_data_parallel.join(enable=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic .", "question_id": 50839}
{"snippet": "distributed_data_parallel.join(divide_by_initial_world_size=True, enable=True, throw_on_early_termination=False)", "intent": "A context manager to be used in conjunction with an instance of torch.nn.parallel.DistributedDataParallel to be able to train with uneven inputs across participating processes . To use this to `enable` training with uneven inputs across processes , simply wrap this context manager around your training loop . Alternatively , if the flag `throw_on_early_termination` is specified to be True , all trainers will throw an error once one rank runs out of inputs , allowing these errors to be caught and handled according to application logic . With arguments `divide_by_initial_world_size`.", "question_id": 50840}
{"snippet": "distributed_data_parallel.no_sync()", "intent": "A context manager to disable gradient synchronizations across DDP processes .", "question_id": 50841}
{"snippet": "distributed_data_parallel.register_comm_hook(state, hook)", "intent": "Registers a communication `hook` which is an enhancement that provides a flexible hook to users where they can specify how DDP aggregates gradients across multiple workers . With arguments `state`.", "question_id": 50842}
{"snippet": "torch.set_flush_denormal(mode)", "intent": "Disables denormal floating numbers on CPU . Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal `mode` .", "question_id": 50843}
{"snippet": "torch.jit.annotate(the_type, the_value)", "intent": "This method is a pass-through function that returns `the_value` , used to hint TorchScript compiler the type of the_value . With arguments `the_type`.", "question_id": 50844}
{"snippet": "torch.sparse.sum(input)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` .", "question_id": 50845}
{"snippet": "torch.sparse.sum(input, dim=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` .", "question_id": 50846}
{"snippet": "torch.sparse.sum(input, dtype=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` . With arguments `dtype`.", "question_id": 50847}
{"snippet": "torch.sparse.sum(input, dim=None, dtype=None)", "intent": "Returns the sum of each row of the sparse tensor `input` in the given dimensions `dim` . With arguments `dtype`.", "question_id": 50848}
{"snippet": "Tensor.pow(exponent)", "intent": "See torch.pow ( ) With arguments `exponent`.", "question_id": 50849}
{"snippet": "torch.ger(input, vec2)", "intent": "Alias of torch.outer ( ) . With arguments `input`, `vec2`.", "question_id": 50850}
{"snippet": "torch.ger(input, vec2, out=None)", "intent": "Alias of torch.outer ( ) . With arguments `input`, `vec2`, `out`.", "question_id": 50851}
{"snippet": "torch.nn.LeakyReLU()", "intent": "Applies the element-wise function :", "question_id": 50852}
{"snippet": "torch.nn.LeakyReLU(negative_slope=0.01)", "intent": "Applies the element-wise function : With arguments `negative_slope`.", "question_id": 50853}
{"snippet": "torch.nn.LeakyReLU(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 50854}
{"snippet": "torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)", "intent": "Applies the element-wise function : With arguments `negative_slope`, `inplace`.", "question_id": 50855}
{"snippet": "torch.nn.quantized.ReLU6()", "intent": "Applies the element-wise function :", "question_id": 50856}
{"snippet": "torch.nn.quantized.ReLU6(inplace=False)", "intent": "Applies the element-wise function : With arguments `inplace`.", "question_id": 50857}
{"snippet": "Tensor.cos()", "intent": "See torch.cos ( )", "question_id": 50858}
{"snippet": "torch.nn.ChannelShuffle(groups)", "intent": "Divide the channels in a tensor of shape ( \u2217 , C , H , W ) ( * , C , H , W ) ( \u2217 , C , H , W ) into g `groups` and rearrange them as ( \u2217 , Cg , g , H , W ) ( * , C \\frac g , g , H , W ) ( \u2217 , C , g\u200bg , H , W ) , while keeping the original tensor shape .", "question_id": 50859}
{"snippet": "torch.nn.LazyBatchNorm3d()", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) .", "question_id": 50860}
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 50861}
{"snippet": "torch.nn.LazyBatchNorm3d(momentum=0.1)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 50862}
{"snippet": "torch.nn.LazyBatchNorm3d(affine=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 50863}
{"snippet": "torch.nn.LazyBatchNorm3d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 50864}
{"snippet": "torch.nn.LazyBatchNorm3d(device=None)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 50865}
{"snippet": "torch.nn.LazyBatchNorm3d(dtype=None)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 50866}
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 50867}
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 50868}
{"snippet": "torch.nn.LazyBatchNorm3d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm3d module with lazy initialization of the num_features argument of the BatchNorm3d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 50869}
{"snippet": "lazy_batch_norm3d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm3d", "question_id": 50870}
{"snippet": "torch.nn.quantized.functional.clamp(input, min_, max_)", "intent": "float ( `input` , `min_` , `max_` ) - > Tensor", "question_id": 50871}
{"snippet": "torch.arccos(input)", "intent": "Alias for torch.acos ( ) . With arguments `input`.", "question_id": 50872}
{"snippet": "torch.arccos(input, out=None)", "intent": "Alias for torch.acos ( ) . With arguments `input`, `out`.", "question_id": 50873}
{"snippet": "Tensor.round_()", "intent": "In-place version of round ( )", "question_id": 50874}
{"snippet": "torch.nn.Softplus()", "intent": "Applies the element-wise function :", "question_id": 50875}
{"snippet": "torch.nn.Softplus(beta=1)", "intent": "Applies the element-wise function : With arguments `beta`.", "question_id": 50876}
{"snippet": "torch.nn.Softplus(threshold=20)", "intent": "Applies the element-wise function : For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` .", "question_id": 50877}
{"snippet": "torch.nn.Softplus(beta=1, threshold=20)", "intent": "Applies the element-wise function : For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `beta`.", "question_id": 50878}
{"snippet": "Tensor.sinh()", "intent": "See torch.sinh ( )", "question_id": 50879}
{"snippet": "Optimizer.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 50880}
{"snippet": "Optimizer.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 50881}
{"snippet": "torch.fft.fft2(input, - 1))", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`.", "question_id": 50882}
{"snippet": "torch.fft.fft2(input, - 1), s=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`.", "question_id": 50883}
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`.", "question_id": 50884}
{"snippet": "torch.fft.fft2(input, - 1), norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `norm`.", "question_id": 50885}
{"snippet": "torch.fft.fft2(input, - 1), out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `out`.", "question_id": 50886}
{"snippet": "torch.fft.fft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 50887}
{"snippet": "torch.fft.fft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 50888}
{"snippet": "torch.fft.fft2(input, - 1), s=None, out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 50889}
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 50890}
{"snippet": "torch.fft.fft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2 dimensional discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 50891}
{"snippet": "softplus.apply(module, name, mask)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning `mask` . With arguments `module`, `name`.", "question_id": 50892}
{"snippet": "softplus.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 50893}
{"snippet": "softplus.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 50894}
{"snippet": "softplus.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 50895}
{"snippet": "softplus.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 50896}
{"snippet": "softplus.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 50897}
{"snippet": "softplus.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 50898}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 50899}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 50900}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 50901}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 50902}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 50903}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 50904}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 50905}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 50906}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 50907}
{"snippet": "torch.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 50908}
{"snippet": "conv3d.from_float(mod)", "intent": "Creates a quantized module from a float module or qparams_dict . With arguments `mod`.", "question_id": 50909}
{"snippet": "torch.nn.functional.conv2d(input, weight)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`.", "question_id": 50910}
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`.", "question_id": 50911}
{"snippet": "torch.nn.functional.conv2d(input, weight, stride=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `stride`.", "question_id": 50912}
{"snippet": "torch.nn.functional.conv2d(input, weight, padding=0)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `padding`.", "question_id": 50913}
{"snippet": "torch.nn.functional.conv2d(input, weight, dilation=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 50914}
{"snippet": "torch.nn.functional.conv2d(input, weight, groups=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `groups`.", "question_id": 50915}
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, stride=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 50916}
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, padding=0)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 50917}
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, dilation=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 50918}
{"snippet": "torch.nn.functional.conv2d(input, weight, bias=None, groups=1)", "intent": "Applies a 2D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 50919}
{"snippet": "profile.self_cpu_time_total", "intent": "Returns total time spent on CPU obtained as a sum of all self times across all the events.", "question_id": 50920}
{"snippet": "torch.linalg.slogdet(A)", "intent": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix . For complex `A` , it returns the angle and the natural logarithm of the modulus of the determinant , that is , a logarithmic polar decomposition of the determinant .", "question_id": 50921}
{"snippet": "torch.linalg.slogdet(A, out=None)", "intent": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix . For complex `A` , it returns the angle and the natural logarithm of the modulus of the determinant , that is , a logarithmic polar decomposition of the determinant . With arguments `out`.", "question_id": 50922}
{"snippet": "Tensor.cholesky_inverse()", "intent": "See torch.cholesky_inverse ( )", "question_id": 50923}
{"snippet": "Tensor.cholesky_inverse(upper=False)", "intent": "See torch.cholesky_inverse ( ) With arguments `upper`.", "question_id": 50924}
{"snippet": "torch.rad2deg(input)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees .", "question_id": 50925}
{"snippet": "torch.rad2deg(input, out=None)", "intent": "Returns a new tensor with each of the elements of `input` converted from angles in radians to degrees . With arguments `out`.", "question_id": 50926}
{"snippet": "torch.load(f, **pickle_load_args)", "intent": "Loads an object saved with torch.save ( ) from a file . With arguments `f`, `**pickle_load_args`.", "question_id": 50927}
{"snippet": "torch.load(f, **pickle_load_args, map_location=None)", "intent": "Loads an object saved with torch.save ( ) from a file . However , storages can be dynamically remapped to an alternative set of devices using the `map_location` argument . With arguments `f`, `**pickle_load_args`.", "question_id": 50928}
{"snippet": "torch.load(f, **pickle_load_args, pickle_module=pickle)", "intent": "Loads an object saved with torch.save ( ) from a file . With arguments `f`, `**pickle_load_args`, `pickle_module`.", "question_id": 50929}
{"snippet": "torch.load(f, **pickle_load_args, map_location=None, pickle_module=pickle)", "intent": "Loads an object saved with torch.save ( ) from a file . However , storages can be dynamically remapped to an alternative set of devices using the `map_location` argument . With arguments `f`, `**pickle_load_args`, `pickle_module`.", "question_id": 50930}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`.", "question_id": 50931}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 50932}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `nonlinearity`.", "question_id": 50933}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 50934}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `nonlinearity`.", "question_id": 50935}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 50936}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, nonlinearity='tanh', dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `nonlinearity`, `dtype`.", "question_id": 50937}
{"snippet": "torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `nonlinearity`, `dtype`.", "question_id": 50938}
{"snippet": "torch.bernoulli(input)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number .", "question_id": 50939}
{"snippet": "torch.bernoulli(input, generator=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . With arguments `generator`.", "question_id": 50940}
{"snippet": "torch.bernoulli(input, out=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . The returned `out` tensor only has values 0 or 1 and is of the same shape as input .", "question_id": 50941}
{"snippet": "torch.bernoulli(input, generator=None, out=None)", "intent": "Draws binary random numbers ( 0 or 1 ) from a Bernoulli distribution . The `input` tensor should be a tensor containing probabilities to be used for drawing the binary random number . The returned `out` tensor only has values 0 or 1 and is of the same shape as input . With arguments `generator`.", "question_id": 50942}
{"snippet": "Tensor.rad2deg()", "intent": "See torch.rad2deg ( )", "question_id": 50943}
{"snippet": "torch.jit.ignore(**kwargs)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function . With arguments `**kwargs`.", "question_id": 50944}
{"snippet": "torch.jit.ignore(**kwargs, drop=False)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function . With arguments `**kwargs`, `drop`.", "question_id": 50945}
{"snippet": "torch.cuda.set_device(device)", "intent": "Sets the current `device` .", "question_id": 50946}
{"snippet": "torch.nn.quantized.functional.celu(input, scale, zero_point)", "intent": "Applies the quantized CELU function element-wise . With arguments `input`, `scale`, `zero_point`.", "question_id": 50947}
{"snippet": "torch.nn.quantized.functional.celu(input, scale, zero_point, alpha=1.)", "intent": "Applies the quantized CELU function element-wise . With arguments `input`, `scale`, `zero_point`, `alpha`.", "question_id": 50948}
{"snippet": "Tensor.int()", "intent": "self.int ( ) is equivalent to self.to ( torch.int32 ) .", "question_id": 50949}
{"snippet": "Tensor.int(memory_format=torch.preserve_format)", "intent": "self.int ( ) is equivalent to self.to ( torch.int32 ) . With arguments `memory_format`.", "question_id": 50950}
{"snippet": "torch.linalg.qr(A)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 50951}
{"snippet": "torch.linalg.qr(A, mode='reduced')", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `mode` chooses between the full and reduced QR decomposition .", "question_id": 50952}
{"snippet": "torch.linalg.qr(A, out=None)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 50953}
{"snippet": "torch.linalg.qr(A, mode='reduced', out=None)", "intent": "Computes the QR decomposition of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `mode` chooses between the full and reduced QR decomposition . With arguments `out`.", "question_id": 50954}
{"snippet": "Tensor.less_(other)", "intent": "In-place version of less ( ) . With arguments `other`.", "question_id": 50955}
{"snippet": "Tensor.nan_to_num()", "intent": "See torch.nan_to_num ( ) .", "question_id": 50956}
{"snippet": "Tensor.nan_to_num(nan=0.0)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`.", "question_id": 50957}
{"snippet": "Tensor.nan_to_num(posinf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `posinf`.", "question_id": 50958}
{"snippet": "Tensor.nan_to_num(neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `neginf`.", "question_id": 50959}
{"snippet": "Tensor.nan_to_num(nan=0.0, posinf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `posinf`.", "question_id": 50960}
{"snippet": "Tensor.nan_to_num(nan=0.0, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `neginf`.", "question_id": 50961}
{"snippet": "Tensor.nan_to_num(posinf=None, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `posinf`, `neginf`.", "question_id": 50962}
{"snippet": "Tensor.nan_to_num(nan=0.0, posinf=None, neginf=None)", "intent": "See torch.nan_to_num ( ) . With arguments `nan`, `posinf`, `neginf`.", "question_id": 50963}
{"snippet": "torch.dstack(tensors)", "intent": "Stack `tensors` in sequence depthwise ( along third axis ) .", "question_id": 50964}
{"snippet": "torch.dstack(tensors, out=None)", "intent": "Stack `tensors` in sequence depthwise ( along third axis ) . With arguments `out`.", "question_id": 50965}
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`.", "question_id": 50966}
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, stride=None)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`.", "question_id": 50967}
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`, `ceil_mode`.", "question_id": 50968}
{"snippet": "torch.nn.LPPool2d(norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an input signal composed of several input planes . The parameters `kernel_size` , `stride` can either be : With arguments `norm_type`, `ceil_mode`.", "question_id": 50969}
{"snippet": "Tensor.negative()", "intent": "See torch.negative ( )", "question_id": 50970}
{"snippet": "Function.backward(ctx, *grad_outputs)", "intent": "Defines a formula for differentiating the operation . It must accept a context `ctx` as the first argument , followed by as many outputs as the forward ( ) returned ( None will be passed in for non tensor outputs of the forward function ) , and it should return as many tensors , as there were inputs to forward ( ) . With arguments `*grad_outputs`.", "question_id": 50971}
{"snippet": "torch.cdist(x1, x2)", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R .", "question_id": 50972}
{"snippet": "torch.cdist(x1, x2, p=2.0)", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . This function is equivalent to scipy.spatial.distance.cdist ( input , \u2019 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) .", "question_id": 50973}
{"snippet": "torch.cdist(x1, x2, compute_mode='use_mm_for_euclid_dist_if_necessary')", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . With arguments `compute_mode`.", "question_id": 50974}
{"snippet": "torch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary')", "intent": "Computes batched the p-norm distance between each pair of the two collections of row vectors . If `x1` has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M and `x2` has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R . This function is equivalent to scipy.spatial.distance.cdist ( input , \u2019 minkowski \u2019 , p=p ) if p\u2208 ( 0 , \u221e ) `p` \\in ( 0 , \\infty ) p\u2208 ( 0 , \u221e ) . With arguments `compute_mode`.", "question_id": 50975}
{"snippet": "torch.nn.utils.prune.Identity", "intent": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.", "question_id": 50976}
{"snippet": "identity.apply(module, name)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`.", "question_id": 50977}
{"snippet": "identity.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 50978}
{"snippet": "identity.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 50979}
{"snippet": "identity.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 50980}
{"snippet": "identity.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 50981}
{"snippet": "identity.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 50982}
{"snippet": "identity.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 50983}
{"snippet": "torch.nn.functional.softmin(input)", "intent": "Applies a softmin function . With arguments `input`.", "question_id": 50984}
{"snippet": "torch.nn.functional.softmin(input, dim=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`.", "question_id": 50985}
{"snippet": "torch.nn.functional.softmin(input, _stacklevel=3)", "intent": "Applies a softmin function . With arguments `input`, `_stacklevel`.", "question_id": 50986}
{"snippet": "torch.nn.functional.softmin(input, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dtype`.", "question_id": 50987}
{"snippet": "torch.nn.functional.softmin(input, dim=None, _stacklevel=3)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `_stacklevel`.", "question_id": 50988}
{"snippet": "torch.nn.functional.softmin(input, dim=None, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `dtype`.", "question_id": 50989}
{"snippet": "torch.nn.functional.softmin(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 50990}
{"snippet": "torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmin function . With arguments `input`, `dim`, `_stacklevel`, `dtype`.", "question_id": 50991}
{"snippet": "Tensor.split(split_size)", "intent": "See torch.split ( ) With arguments `split_size`.", "question_id": 50992}
{"snippet": "Tensor.split(split_size, dim=0)", "intent": "See torch.split ( ) With arguments `split_size`, `dim`.", "question_id": 50993}
{"snippet": "Tensor.logit()", "intent": "See torch.logit ( )", "question_id": 50994}
{"snippet": "Tensor.cholesky_solve(input2)", "intent": "See torch.cholesky_solve ( ) With arguments `input2`.", "question_id": 50995}
{"snippet": "Tensor.cholesky_solve(input2, upper=False)", "intent": "See torch.cholesky_solve ( ) With arguments `input2`, `upper`.", "question_id": 50996}
{"snippet": "Tensor.unique()", "intent": "Returns the unique elements of the input tensor .", "question_id": 50997}
{"snippet": "Tensor.unique(sorted=True)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`.", "question_id": 50998}
{"snippet": "Tensor.unique(return_inverse=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`.", "question_id": 50999}
{"snippet": "Tensor.unique(return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_counts`.", "question_id": 51000}
{"snippet": "Tensor.unique(dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `dim`.", "question_id": 51001}
{"snippet": "Tensor.unique(sorted=True, return_inverse=False)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `return_inverse`.", "question_id": 51002}
{"snippet": "Tensor.unique(sorted=True, return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `return_counts`.", "question_id": 51003}
{"snippet": "Tensor.unique(sorted=True, dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `sorted`, `dim`.", "question_id": 51004}
{"snippet": "Tensor.unique(return_inverse=False, return_counts=False)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`, `return_counts`.", "question_id": 51005}
{"snippet": "Tensor.unique(return_inverse=False, dim=None)", "intent": "Returns the unique elements of the input tensor . With arguments `return_inverse`, `dim`.", "question_id": 51006}
{"snippet": "Tensor.permute(*dims)", "intent": "See torch.permute ( ) With arguments `*dims`.", "question_id": 51007}
{"snippet": "torch.nn.utils.prune.ln_structured(module, name, amount, n, dim)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` with the lowest L `` `n` `` -norm .", "question_id": 51008}
{"snippet": "torch.nn.utils.prune.ln_structured(module, name, amount, n, dim, importance_scores=None)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) channels along the specified `dim` with the lowest L `` `n` `` -norm . With arguments `importance_scores`.", "question_id": 51009}
{"snippet": "torch.tanh(input)", "intent": "Returns a new tensor with the hyperbolic tangent of the elements of `input` .", "question_id": 51010}
{"snippet": "torch.tanh(input, out=None)", "intent": "Returns a new tensor with the hyperbolic tangent of the elements of `input` . With arguments `out`.", "question_id": 51011}
{"snippet": "Tensor.lcm_(other)", "intent": "In-place version of lcm ( ) With arguments `other`.", "question_id": 51012}
{"snippet": "Tensor.unbind()", "intent": "See torch.unbind ( )", "question_id": 51013}
{"snippet": "Tensor.unbind(dim=0)", "intent": "See torch.unbind ( ) With arguments `dim`.", "question_id": 51014}
{"snippet": "torch.optim.AdamW(params, 0.999))", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`.", "question_id": 51015}
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`.", "question_id": 51016}
{"snippet": "torch.optim.AdamW(params, 0.999), betas=(0.9)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `betas`.", "question_id": 51017}
{"snippet": "torch.optim.AdamW(params, 0.999), eps=1e-08)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `eps`.", "question_id": 51018}
{"snippet": "torch.optim.AdamW(params, 0.999), weight_decay=0.01)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 51019}
{"snippet": "torch.optim.AdamW(params, 0.999), amsgrad=False)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `amsgrad`.", "question_id": 51020}
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 51021}
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 51022}
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, weight_decay=0.01)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 51023}
{"snippet": "torch.optim.AdamW(params, 0.999), lr=0.001, amsgrad=False)", "intent": "Implements AdamW algorithm . With arguments `params`, `0.999)`, `lr`, `amsgrad`.", "question_id": 51024}
{"snippet": "adam_w.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51025}
{"snippet": "adam_w.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51026}
{"snippet": "adam_w.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51027}
{"snippet": "adam_w.step()", "intent": "Performs a single optimization step .", "question_id": 51028}
{"snippet": "adam_w.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51029}
{"snippet": "adam_w.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51030}
{"snippet": "adam_w.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51031}
{"snippet": "Tensor.frac()", "intent": "See torch.frac ( )", "question_id": 51032}
{"snippet": "torch.fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max)", "intent": "Returns a new tensor with the data in `input` fake quantized per channel using `scale` , `zero_point` , `quant_min` and `quant_max` , across the channel specified by axis .", "question_id": 51033}
{"snippet": "torch.nn.Hardtanh()", "intent": "Applies the HardTanh function element-wise", "question_id": 51034}
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 51035}
{"snippet": "torch.nn.Hardtanh(max_val=1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 51036}
{"snippet": "torch.nn.Hardtanh(inplace=False)", "intent": "Applies the HardTanh function element-wise With arguments `inplace`.", "question_id": 51037}
{"snippet": "torch.nn.Hardtanh(min_value=None)", "intent": "Applies the HardTanh function element-wise Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 51038}
{"snippet": "torch.nn.Hardtanh(max_value=None)", "intent": "Applies the HardTanh function element-wise Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 51039}
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, max_val=1.0)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` .", "question_id": 51040}
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, inplace=False)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . With arguments `inplace`.", "question_id": 51041}
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, min_value=None)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 51042}
{"snippet": "torch.nn.Hardtanh(min_val=- 1.0, max_value=None)", "intent": "Applies the HardTanh function element-wise The range of the linear region [ \u22121,1 ] [ -1 , 1 ] [ \u22121,1 ] can be adjusted using `min_val` and `max_val` . Keyword arguments `min_value` and `max_value` have been deprecated in favor of min_val and max_val .", "question_id": 51043}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 51044}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 51045}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 51046}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 51047}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 51048}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 51049}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 51050}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 51051}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 51052}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU2d module is a module fused from Conv2d , BatchNorm2d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 51053}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 51054}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 51055}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 51056}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 51057}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 51058}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 51059}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 51060}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 51061}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 51062}
{"snippet": "torch.nn.LazyConvTranspose1d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument of the ConvTranspose1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 51063}
{"snippet": "lazy_conv_transpose1d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose1d", "question_id": 51064}
{"snippet": "torch.quantization.qconfig.QConfig(activation, weight)", "intent": "Describes how to quantize a layer or a part of the network by providing settings ( observer classes ) for activations and weights respectively . With arguments `activation`, `weight`.", "question_id": 51065}
{"snippet": "torch.optim.Adam(params, 0.999))", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`.", "question_id": 51066}
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`.", "question_id": 51067}
{"snippet": "torch.optim.Adam(params, 0.999), betas=(0.9)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `betas`.", "question_id": 51068}
{"snippet": "torch.optim.Adam(params, 0.999), eps=1e-08)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `eps`.", "question_id": 51069}
{"snippet": "torch.optim.Adam(params, 0.999), weight_decay=0)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `weight_decay`.", "question_id": 51070}
{"snippet": "torch.optim.Adam(params, 0.999), amsgrad=False)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `amsgrad`.", "question_id": 51071}
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 51072}
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 51073}
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, weight_decay=0)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `weight_decay`.", "question_id": 51074}
{"snippet": "torch.optim.Adam(params, 0.999), lr=0.001, amsgrad=False)", "intent": "Implements Adam algorithm . With arguments `params`, `0.999)`, `lr`, `amsgrad`.", "question_id": 51075}
{"snippet": "adam.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51076}
{"snippet": "adam.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51077}
{"snippet": "adam.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51078}
{"snippet": "adam.step()", "intent": "Performs a single optimization step .", "question_id": 51079}
{"snippet": "adam.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51080}
{"snippet": "adam.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51081}
{"snippet": "adam.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51082}
{"snippet": "torch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max)", "intent": "Returns a new tensor with the data in `input` fake quantized using `scale` , `zero_point` , `quant_min` and `quant_max` .", "question_id": 51083}
{"snippet": "torch.diagflat(input)", "intent": "The argument `offset` controls which diagonal to consider : With arguments `input`.", "question_id": 51084}
{"snippet": "torch.diagflat(input, offset=0)", "intent": "The argument `offset` controls which diagonal to consider : With arguments `input`.", "question_id": 51085}
{"snippet": "Tensor.trunc()", "intent": "See torch.trunc ( )", "question_id": 51086}
{"snippet": "torch.linalg.matrix_power(A, n)", "intent": "Computes the n-th power of a square matrix for an integer `n` . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 51087}
{"snippet": "torch.linalg.matrix_power(A, n, out=None)", "intent": "Computes the n-th power of a square matrix for an integer `n` . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 51088}
{"snippet": "torch.nn.ParameterDict()", "intent": "Holds `parameters` in a dictionary .", "question_id": 51089}
{"snippet": "torch.nn.ParameterDict(parameters=None)", "intent": "Holds `parameters` in a dictionary .", "question_id": 51090}
{"snippet": "parameter_dict.clear()", "intent": "Remove all items from the ParameterDict .", "question_id": 51091}
{"snippet": "parameter_dict.items()", "intent": "Return an iterable of the ParameterDict key/value pairs .", "question_id": 51092}
{"snippet": "parameter_dict.keys()", "intent": "Return an iterable of the ParameterDict keys .", "question_id": 51093}
{"snippet": "parameter_dict.pop(key)", "intent": "Remove `key` from the ParameterDict and return its parameter .", "question_id": 51094}
{"snippet": "parameter_dict.update(parameters)", "intent": "Update the ParameterDict with the key-value pairs from a mapping or an iterable , overwriting existing keys . With arguments `parameters`.", "question_id": 51095}
{"snippet": "parameter_dict.values()", "intent": "Return an iterable of the ParameterDict values .", "question_id": 51096}
{"snippet": "Tensor.log_normal_()", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 .", "question_id": 51097}
{"snippet": "Tensor.log_normal_(mean=1)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 .", "question_id": 51098}
{"snippet": "Tensor.log_normal_(std=2)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution :", "question_id": 51099}
{"snippet": "Tensor.log_normal_(generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . With arguments `generator`.", "question_id": 51100}
{"snippet": "Tensor.log_normal_(mean=1, std=2)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution :", "question_id": 51101}
{"snippet": "Tensor.log_normal_(mean=1, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . With arguments `generator`.", "question_id": 51102}
{"snippet": "Tensor.log_normal_(std=2, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution : With arguments `generator`.", "question_id": 51103}
{"snippet": "Tensor.log_normal_(mean=1, std=2, generator=None)", "intent": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given `mean` \u03bc\\mu\u03bc and standard deviation \u03c3\\sigma\u03c3 . Note that mean and `std` are the mean and standard deviation of the underlying normal distribution , and not of the returned distribution : With arguments `generator`.", "question_id": 51104}
{"snippet": "torch.argmin(input)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`.", "question_id": 51105}
{"snippet": "torch.argmin(input, dim=None)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `dim`.", "question_id": 51106}
{"snippet": "torch.argmin(input, keepdim=False)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `keepdim`.", "question_id": 51107}
{"snippet": "torch.argmin(input, dim=None, keepdim=False)", "intent": "Returns the indices of the minimum value ( s ) of the flattened tensor or along a dimension With arguments `input`, `dim`, `keepdim`.", "question_id": 51108}
{"snippet": "torch.quantization.default_eval_fn(model, calib_data)", "intent": "Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the `model` on the dataset With arguments `calib_data`.", "question_id": 51109}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`.", "question_id": 51110}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`.", "question_id": 51111}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, bias=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `bias`.", "question_id": 51112}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `eps`.", "question_id": 51113}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `bias`.", "question_id": 51114}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `eps`.", "question_id": 51115}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, bias=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `bias`, `eps`.", "question_id": 51116}
{"snippet": "torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)", "intent": "Applies Layer Normalization for last certain number of dimensions . With arguments `input`, `normalized_shape`, `weight`, `bias`, `eps`.", "question_id": 51117}
{"snippet": "torch.nn.functional.conv1d(input, weight)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`.", "question_id": 51118}
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`.", "question_id": 51119}
{"snippet": "torch.nn.functional.conv1d(input, weight, stride=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `stride`.", "question_id": 51120}
{"snippet": "torch.nn.functional.conv1d(input, weight, padding=0)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `padding`.", "question_id": 51121}
{"snippet": "torch.nn.functional.conv1d(input, weight, dilation=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 51122}
{"snippet": "torch.nn.functional.conv1d(input, weight, groups=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `groups`.", "question_id": 51123}
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, stride=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 51124}
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, padding=0)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 51125}
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, dilation=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 51126}
{"snippet": "torch.nn.functional.conv1d(input, weight, bias=None, groups=1)", "intent": "Applies a 1D convolution over an `input` signal composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 51127}
{"snippet": "Tensor.amin()", "intent": "See torch.amin ( )", "question_id": 51128}
{"snippet": "Tensor.amin(dim=None)", "intent": "See torch.amin ( ) With arguments `dim`.", "question_id": 51129}
{"snippet": "Tensor.amin(keepdim=False)", "intent": "See torch.amin ( ) With arguments `keepdim`.", "question_id": 51130}
{"snippet": "Tensor.amin(dim=None, keepdim=False)", "intent": "See torch.amin ( ) With arguments `dim`, `keepdim`.", "question_id": 51131}
{"snippet": "torch.cummax(input, dim)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative maximum of elements of `input` in the dimension `dim` .", "question_id": 51132}
{"snippet": "torch.cummax(input, dim, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative maximum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 51133}
{"snippet": "torch.nn.SyncBatchNorm(num_features)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 51134}
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 51135}
{"snippet": "torch.nn.SyncBatchNorm(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 51136}
{"snippet": "torch.nn.SyncBatchNorm(num_features, affine=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 51137}
{"snippet": "torch.nn.SyncBatchNorm(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 51138}
{"snippet": "torch.nn.SyncBatchNorm(num_features, process_group=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `process_group`.", "question_id": 51139}
{"snippet": "torch.nn.SyncBatchNorm(num_features, device=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 51140}
{"snippet": "torch.nn.SyncBatchNorm(num_features, dtype=None)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 51141}
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 51142}
{"snippet": "torch.nn.SyncBatchNorm(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a N-Dimensional input ( a mini-batch of [ N-2 ] D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 51143}
{"snippet": "sync_batch_norm.convert_sync_batchnorm(module)", "intent": "Helper function to convert all BatchNorm * D layers in the model to torch.nn.SyncBatchNorm layers . With arguments `module`.", "question_id": 51144}
{"snippet": "sync_batch_norm.convert_sync_batchnorm(module, process_group=None)", "intent": "Helper function to convert all BatchNorm * D layers in the model to torch.nn.SyncBatchNorm layers . With arguments `module`, `process_group`.", "question_id": 51145}
{"snippet": "torch.svd_lowrank(A)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`.", "question_id": 51146}
{"snippet": "torch.svd_lowrank(A, q=6)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`.", "question_id": 51147}
{"snippet": "torch.svd_lowrank(A, niter=2)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `niter`.", "question_id": 51148}
{"snippet": "torch.svd_lowrank(A, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `M`.", "question_id": 51149}
{"snippet": "torch.svd_lowrank(A, q=6, niter=2)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `niter`.", "question_id": 51150}
{"snippet": "torch.svd_lowrank(A, q=6, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `M`.", "question_id": 51151}
{"snippet": "torch.svd_lowrank(A, niter=2, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `niter`, `M`.", "question_id": 51152}
{"snippet": "torch.svd_lowrank(A, q=6, niter=2, M=None)", "intent": "Return the singular value decomposition ( U , S , V ) of a matrix , batches of matrices , or a sparse matrix AAA such that A\u2248Udiag ( S ) VTA \\approx U diag ( S ) V^TA\u2248Udiag ( S ) VT . With arguments `A`, `q`, `niter`, `M`.", "question_id": 51153}
{"snippet": "torch.where(condition, x, y)", "intent": "Return a tensor of elements selected from either `x` or `y` , depending on `condition` .", "question_id": 51154}
{"snippet": "torch.nn.LogSoftmax()", "intent": "Applies the log\u2061 ( Softmax ( x ) ) \\log ( \\text { Softmax } ( x ) ) log ( Softmax ( x ) ) function to an n-dimensional input Tensor .", "question_id": 51155}
{"snippet": "torch.nn.LogSoftmax(dim=None)", "intent": "Applies the log\u2061 ( Softmax ( x ) ) \\log ( \\text { Softmax } ( x ) ) log ( Softmax ( x ) ) function to an n-dimensional input Tensor . With arguments `dim`.", "question_id": 51156}
{"snippet": "torch.is_grad_enabled()", "intent": "Returns True if grad mode is currently enabled .", "question_id": 51157}
{"snippet": "torch.eq(input, other)", "intent": "Computes element-wise equality With arguments `input`, `other`.", "question_id": 51158}
{"snippet": "torch.eq(input, other, out=None)", "intent": "Computes element-wise equality With arguments `input`, `other`, `out`.", "question_id": 51159}
{"snippet": "Tensor.mean()", "intent": "See torch.mean ( )", "question_id": 51160}
{"snippet": "Tensor.mean(dim=None)", "intent": "See torch.mean ( ) With arguments `dim`.", "question_id": 51161}
{"snippet": "Tensor.mean(keepdim=False)", "intent": "See torch.mean ( ) With arguments `keepdim`.", "question_id": 51162}
{"snippet": "Tensor.mean(dim=None, keepdim=False)", "intent": "See torch.mean ( ) With arguments `dim`, `keepdim`.", "question_id": 51163}
{"snippet": "torch.cuda.Stream(**kwargs)", "intent": "Wrapper around a CUDA stream . With arguments `**kwargs`.", "question_id": 51164}
{"snippet": "torch.cuda.Stream(**kwargs, device=None)", "intent": "Wrapper around a CUDA stream . A CUDA stream is a linear sequence of execution that belongs to a specific `device` , independent from other streams . With arguments `**kwargs`.", "question_id": 51165}
{"snippet": "torch.cuda.Stream(**kwargs, priority=0)", "intent": "Wrapper around a CUDA stream . With arguments `**kwargs`, `priority`.", "question_id": 51166}
{"snippet": "torch.cuda.Stream(**kwargs, device=None, priority=0)", "intent": "Wrapper around a CUDA stream . A CUDA stream is a linear sequence of execution that belongs to a specific `device` , independent from other streams . With arguments `**kwargs`, `priority`.", "question_id": 51167}
{"snippet": "stream.query()", "intent": "Checks if all the work submitted has been completed .", "question_id": 51168}
{"snippet": "stream.record_event()", "intent": "Records an `event` .", "question_id": 51169}
{"snippet": "stream.record_event(event=None)", "intent": "Records an `event` .", "question_id": 51170}
{"snippet": "stream.synchronize()", "intent": "Wait for all the kernels in this stream to complete .", "question_id": 51171}
{"snippet": "stream.wait_event(event)", "intent": "Makes all future work submitted to the stream wait for an `event` .", "question_id": 51172}
{"snippet": "stream.wait_stream(stream)", "intent": "Synchronizes with another `stream` .", "question_id": 51173}
{"snippet": "torch.permute(input, dims)", "intent": "Returns a view of the original tensor `input` with its dimensions permuted . With arguments `dims`.", "question_id": 51174}
{"snippet": "torch.nn.functional.upsample_bilinear(input)", "intent": "Upsamples the `input` , using bilinear upsampling .", "question_id": 51175}
{"snippet": "torch.nn.functional.upsample_bilinear(input, size=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`.", "question_id": 51176}
{"snippet": "torch.nn.functional.upsample_bilinear(input, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `scale_factor`.", "question_id": 51177}
{"snippet": "torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`, `scale_factor`.", "question_id": 51178}
{"snippet": "Tensor.digamma_()", "intent": "In-place version of digamma ( )", "question_id": 51179}
{"snippet": "torch.bartlett_window(window_length)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 51180}
{"snippet": "torch.bartlett_window(window_length, periodic=True)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 51181}
{"snippet": "torch.bartlett_window(window_length, dtype=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 51182}
{"snippet": "torch.bartlett_window(window_length, layout=torch.strided)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 51183}
{"snippet": "torch.bartlett_window(window_length, device=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 51184}
{"snippet": "torch.bartlett_window(window_length, requires_grad=False)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 51185}
{"snippet": "torch.bartlett_window(window_length, periodic=True, dtype=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 51186}
{"snippet": "torch.bartlett_window(window_length, periodic=True, layout=torch.strided)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 51187}
{"snippet": "torch.bartlett_window(window_length, periodic=True, device=None)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 51188}
{"snippet": "torch.bartlett_window(window_length, periodic=True, requires_grad=False)", "intent": "Bartlett window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 51189}
{"snippet": "torch.cuda.get_arch_list()", "intent": "Returns list CUDA architectures this library was compiled for .", "question_id": 51190}
{"snippet": "torch.nn.LayerNorm(normalized_shape)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` .", "question_id": 51191}
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`.", "question_id": 51192}
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True .", "question_id": 51193}
{"snippet": "torch.nn.LayerNorm(normalized_shape, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `device`.", "question_id": 51194}
{"snippet": "torch.nn.LayerNorm(normalized_shape, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `dtype`.", "question_id": 51195}
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `eps`.", "question_id": 51196}
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`, `device`.", "question_id": 51197}
{"snippet": "torch.nn.LayerNorm(normalized_shape, eps=1e-05, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . With arguments `eps`, `dtype`.", "question_id": 51198}
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True, device=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `device`.", "question_id": 51199}
{"snippet": "torch.nn.LayerNorm(normalized_shape, elementwise_affine=True, dtype=None)", "intent": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by `normalized_shape` . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable affine transform parameters of normalized_shape if `elementwise_affine` is True . With arguments `dtype`.", "question_id": 51200}
{"snippet": "torch.bincount(input)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 .", "question_id": 51201}
{"snippet": "torch.bincount(input, weights=None)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If n is the value at position i , out [ n ] += `weights` [ i ] if weights is specified else out [ n ] += 1 .", "question_id": 51202}
{"snippet": "torch.bincount(input, minlength=0)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If `minlength` is specified , the number of bins is at least minlength and if input is empty , then the result is tensor of size minlength filled with zeros .", "question_id": 51203}
{"snippet": "torch.bincount(input, weights=None, minlength=0)", "intent": "Count the frequency of each value in an array of non-negative ints . The number of bins ( size 1 ) is one larger than the largest value in `input` unless input is empty , in which case the result is a tensor of size 0 . If n is the value at position i , out [ n ] += `weights` [ i ] if weights is specified else out [ n ] += 1 . If `minlength` is specified , the number of bins is at least minlength and if input is empty , then the result is tensor of size minlength filled with zeros .", "question_id": 51204}
{"snippet": "torch.quantization.DeQuantStub", "intent": "Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.", "question_id": 51205}
{"snippet": "torch.nn.AvgPool1d(kernel_size)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as :", "question_id": 51206}
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple .", "question_id": 51207}
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 51208}
{"snippet": "torch.nn.AvgPool1d(kernel_size, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : With arguments `ceil_mode`.", "question_id": 51209}
{"snippet": "torch.nn.AvgPool1d(kernel_size, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : With arguments `count_include_pad`.", "question_id": 51210}
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, padding=0)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 51211}
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . With arguments `ceil_mode`.", "question_id": 51212}
{"snippet": "torch.nn.AvgPool1d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : The parameters kernel_size , `stride` , padding can each be an int or a one-element tuple . With arguments `count_include_pad`.", "question_id": 51213}
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0, ceil_mode=False)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points . With arguments `ceil_mode`.", "question_id": 51214}
{"snippet": "torch.nn.AvgPool1d(kernel_size, padding=0, count_include_pad=True)", "intent": "Applies a 1D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , L ) ( N , C , L ) ( N , C , L ) , output ( N , C , Lout ) ( N , C , L_ { out } ) ( N , C , Lout\u200b ) and `kernel_size` kkk can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points . With arguments `count_include_pad`.", "question_id": 51215}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 51216}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 51217}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 51218}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 51219}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 51220}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 51221}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 51222}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 51223}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 51224}
{"snippet": "torch.nn.LazyConvTranspose2d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument of the ConvTranspose2d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 51225}
{"snippet": "lazy_conv_transpose2d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose2d", "question_id": 51226}
{"snippet": "torch.nn.Tanhshrink", "intent": "Applies the element-wise function:", "question_id": 51227}
{"snippet": "torch.nn.Fold(output_size, kernel_size)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks .", "question_id": 51228}
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51229}
{"snippet": "torch.nn.Fold(output_size, kernel_size, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51230}
{"snippet": "torch.nn.Fold(output_size, kernel_size, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51231}
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51232}
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51233}
{"snippet": "torch.nn.Fold(output_size, kernel_size, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51234}
{"snippet": "torch.nn.Fold(output_size, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Combines an array of sliding local blocks into a large containing tensor . This operation combines these local blocks into the large output tensor of shape ( N , C , `output_size` [ 0 ] , output_size [ 1 ] , \u2026 ) ( N , C , \\text { output\\_size } [ 0 ] , \\text { output\\_size } [ 1 ] , \\dots ) ( N , C , output_size [ 0 ] , output_size [ 1 ] , \u2026 ) by summing the overlapping values . Consider a batched input tensor containing sliding local blocks , e.g. , patches of images , of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where NNN is batch dimension , C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the number of values within a block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of blocks . The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51235}
{"snippet": "torch.positive(input)", "intent": "Returns `input` .", "question_id": 51236}
{"snippet": "torch.lstsq(input, A)", "intent": "Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size ( m\u00d7n ) ( m \\times n ) ( m\u00d7n ) and a matrix BBB of size ( m\u00d7k ) ( m \\times k ) ( m\u00d7k ) . With arguments `input`, `A`.", "question_id": 51237}
{"snippet": "torch.lstsq(input, A, out=None)", "intent": "Computes the solution to the least squares and least norm problems for a full rank matrix AAA of size ( m\u00d7n ) ( m \\times n ) ( m\u00d7n ) and a matrix BBB of size ( m\u00d7k ) ( m \\times k ) ( m\u00d7k ) . With arguments `input`, `A`, `out`.", "question_id": 51238}
{"snippet": "Tensor.asinh()", "intent": "See torch.asinh ( )", "question_id": 51239}
{"snippet": "torch.nn.FeatureAlphaDropout()", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g .", "question_id": 51240}
{"snippet": "torch.nn.FeatureAlphaDropout(p=0.5)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . Each element will be masked independently for each sample on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 51241}
{"snippet": "torch.nn.FeatureAlphaDropout(inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . With arguments `inplace`.", "question_id": 51242}
{"snippet": "torch.nn.FeatureAlphaDropout(p=0.5, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . Each element will be masked independently for each sample on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 51243}
{"snippet": "torch.exp2(input)", "intent": "Alias for torch.special.exp2 ( ) . With arguments `input`.", "question_id": 51244}
{"snippet": "torch.exp2(input, out=None)", "intent": "Alias for torch.special.exp2 ( ) . With arguments `input`, `out`.", "question_id": 51245}
{"snippet": "Tensor.copysign_(other)", "intent": "In-place version of copysign ( ) With arguments `other`.", "question_id": 51246}
{"snippet": "torch.nn.SoftMarginLoss()", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) .", "question_id": 51247}
{"snippet": "torch.nn.SoftMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`.", "question_id": 51248}
{"snippet": "torch.nn.SoftMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduce`.", "question_id": 51249}
{"snippet": "torch.nn.SoftMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduction`.", "question_id": 51250}
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`.", "question_id": 51251}
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduction`.", "question_id": 51252}
{"snippet": "torch.nn.SoftMarginLoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `reduce`, `reduction`.", "question_id": 51253}
{"snippet": "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx and target tensor yyy ( containing 1 or -1 ) . With arguments `size_average`, `reduce`, `reduction`.", "question_id": 51254}
{"snippet": "torch.tril_indices(row, col)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates .", "question_id": 51255}
{"snippet": "torch.tril_indices(row, col, offset=0)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider .", "question_id": 51256}
{"snippet": "torch.tril_indices(row, col, dtype=torch.long)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`.", "question_id": 51257}
{"snippet": "torch.tril_indices(row, col, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `device`.", "question_id": 51258}
{"snippet": "torch.tril_indices(row, col, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `layout`.", "question_id": 51259}
{"snippet": "torch.tril_indices(row, col, offset=0, dtype=torch.long)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `dtype`.", "question_id": 51260}
{"snippet": "torch.tril_indices(row, col, offset=0, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `device`.", "question_id": 51261}
{"snippet": "torch.tril_indices(row, col, offset=0, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `layout`.", "question_id": 51262}
{"snippet": "torch.tril_indices(row, col, dtype=torch.long, device='cpu')", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `device`.", "question_id": 51263}
{"snippet": "torch.tril_indices(row, col, dtype=torch.long, layout=torch.strided)", "intent": "Returns the indices of the lower triangular part of a row-by- `col` matrix in a 2-by-N Tensor , where the first `row` contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `layout`.", "question_id": 51264}
{"snippet": "torch.nn.AdaptiveMaxPool2d(output_size)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 51265}
{"snippet": "torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 51266}
{"snippet": "torch.linalg.cholesky(A)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 51267}
{"snippet": "torch.linalg.cholesky(A, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 51268}
{"snippet": "torch.jit.ScriptModule", "intent": "A wrapper around C++ torch::jit::Module.", "question_id": 51269}
{"snippet": "script_module.add_module(name, module)", "intent": "Adds a child `module` to the current module . The module can be accessed as an attribute using the given `name` .", "question_id": 51270}
{"snippet": "script_module.apply(fn)", "intent": "Applies `fn` recursively to every submodule ( as returned by .children ( ) ) as well as self .", "question_id": 51271}
{"snippet": "script_module.bfloat16()", "intent": "Casts all floating point parameters and buffers to bfloat16 datatype .", "question_id": 51272}
{"snippet": "script_module.buffers()", "intent": "Returns an iterator over module buffers .", "question_id": 51273}
{"snippet": "script_module.buffers(recurse=True)", "intent": "Returns an iterator over module buffers . With arguments `recurse`.", "question_id": 51274}
{"snippet": "script_module.children()", "intent": "Returns an iterator over immediate children modules .", "question_id": 51275}
{"snippet": "code", "intent": "Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method.", "question_id": 51276}
{"snippet": "code_with_constants", "intent": "Returns a tuple of:", "question_id": 51277}
{"snippet": "script_module.cpu()", "intent": "Moves all model parameters and buffers to the CPU .", "question_id": 51278}
{"snippet": "script_module.cuda()", "intent": "Moves all model parameters and buffers to the GPU .", "question_id": 51279}
{"snippet": "script_module.cuda(device=None)", "intent": "Moves all model parameters and buffers to the GPU . With arguments `device`.", "question_id": 51280}
{"snippet": "script_module.double()", "intent": "Casts all floating point parameters and buffers to double datatype .", "question_id": 51281}
{"snippet": "script_module.eval()", "intent": "Sets the module in evaluation mode .", "question_id": 51282}
{"snippet": "script_module.extra_repr()", "intent": "Set the extra representation of the module", "question_id": 51283}
{"snippet": "script_module.float()", "intent": "Casts all floating point parameters and buffers to float datatype .", "question_id": 51284}
{"snippet": "script_module.get_buffer(target)", "intent": "Returns the buffer given by `target` if it exists , otherwise throws an error .", "question_id": 51285}
{"snippet": "script_module.get_parameter(target)", "intent": "Returns the parameter given by `target` if it exists , otherwise throws an error .", "question_id": 51286}
{"snippet": "script_module.get_submodule(target)", "intent": "Returns the submodule given by `target` if it exists , otherwise throws an error .", "question_id": 51287}
{"snippet": "graph", "intent": "Returns a string representation of the internal graph for the forward method.", "question_id": 51288}
{"snippet": "script_module.half()", "intent": "Casts all floating point parameters and buffers to half datatype .", "question_id": 51289}
{"snippet": "inlined_graph", "intent": "Returns a string representation of the internal graph for the forward method.", "question_id": 51290}
{"snippet": "script_module.load_state_dict(state_dict)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants .", "question_id": 51291}
{"snippet": "script_module.load_state_dict(state_dict, strict=True)", "intent": "Copies parameters and buffers from `state_dict` into this module and its descendants . If `strict` is True , then the keys of state_dict must exactly match the keys returned by this module \u2019 s state_dict ( ) function .", "question_id": 51292}
{"snippet": "script_module.modules()", "intent": "Returns an iterator over all modules in the network .", "question_id": 51293}
{"snippet": "script_module.named_buffers()", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself .", "question_id": 51294}
{"snippet": "script_module.named_buffers(prefix='')", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`.", "question_id": 51295}
{"snippet": "script_module.named_buffers(recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `recurse`.", "question_id": 51296}
{"snippet": "script_module.named_buffers(prefix='', recurse=True)", "intent": "Returns an iterator over module buffers , yielding both the name of the buffer as well as the buffer itself . With arguments `prefix`, `recurse`.", "question_id": 51297}
{"snippet": "script_module.named_children()", "intent": "Returns an iterator over immediate children modules , yielding both the name of the module as well as the module itself .", "question_id": 51298}
{"snippet": "script_module.named_modules()", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself .", "question_id": 51299}
{"snippet": "script_module.named_modules(memo=None)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`.", "question_id": 51300}
{"snippet": "script_module.named_modules(prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`.", "question_id": 51301}
{"snippet": "script_module.named_modules(remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `remove_duplicate`.", "question_id": 51302}
{"snippet": "script_module.named_modules(memo=None, prefix='')", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`.", "question_id": 51303}
{"snippet": "script_module.named_modules(memo=None, remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `remove_duplicate`.", "question_id": 51304}
{"snippet": "script_module.named_modules(prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `prefix`, `remove_duplicate`.", "question_id": 51305}
{"snippet": "script_module.named_modules(memo=None, prefix='', remove_duplicate=True)", "intent": "Returns an iterator over all modules in the network , yielding both the name of the module as well as the module itself . With arguments `memo`, `prefix`, `remove_duplicate`.", "question_id": 51306}
{"snippet": "script_module.named_parameters()", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself .", "question_id": 51307}
{"snippet": "script_module.named_parameters(prefix='')", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`.", "question_id": 51308}
{"snippet": "script_module.named_parameters(recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `recurse`.", "question_id": 51309}
{"snippet": "script_module.named_parameters(prefix='', recurse=True)", "intent": "Returns an iterator over module parameters , yielding both the name of the parameter as well as the parameter itself . With arguments `prefix`, `recurse`.", "question_id": 51310}
{"snippet": "script_module.parameters()", "intent": "Returns an iterator over module parameters .", "question_id": 51311}
{"snippet": "script_module.parameters(recurse=True)", "intent": "Returns an iterator over module parameters . With arguments `recurse`.", "question_id": 51312}
{"snippet": "script_module.register_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 51313}
{"snippet": "script_module.register_buffer(name, tensor)", "intent": "Adds a buffer to the module . With arguments `name`, `tensor`.", "question_id": 51314}
{"snippet": "script_module.register_buffer(name, tensor, persistent=True)", "intent": "Adds a buffer to the module . Buffers , by default , are `persistent` and will be saved alongside parameters . With arguments `name`, `tensor`.", "question_id": 51315}
{"snippet": "script_module.register_forward_hook(hook)", "intent": "Registers a forward `hook` on the module .", "question_id": 51316}
{"snippet": "script_module.register_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook on the module . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 51317}
{"snippet": "script_module.register_full_backward_hook(hook)", "intent": "Registers a backward `hook` on the module .", "question_id": 51318}
{"snippet": "script_module.register_parameter(name, param)", "intent": "Adds a parameter to the module . The parameter can be accessed as an attribute using given `name` . With arguments `param`.", "question_id": 51319}
{"snippet": "script_module.requires_grad_()", "intent": "Change if autograd should record operations on parameters in this module .", "question_id": 51320}
{"snippet": "script_module.requires_grad_(requires_grad=True)", "intent": "Change if autograd should record operations on parameters in this module . This method sets the parameters \u2019 `requires_grad` attributes in-place .", "question_id": 51321}
{"snippet": "script_module.save(f)", "intent": "See torch.jit.save for details . With arguments `f`.", "question_id": 51322}
{"snippet": "script_module.save(f, _extra_files={})", "intent": "See torch.jit.save for details . With arguments `f`, `_extra_files`.", "question_id": 51323}
{"snippet": "script_module.share_memory()", "intent": "See torch.Tensor.share_memory_ ( )", "question_id": 51324}
{"snippet": "script_module.state_dict()", "intent": "Returns a dictionary containing a whole state of the module .", "question_id": 51325}
{"snippet": "script_module.state_dict(destination=None)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`.", "question_id": 51326}
{"snippet": "script_module.state_dict(prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`.", "question_id": 51327}
{"snippet": "script_module.state_dict(keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `keep_vars`.", "question_id": 51328}
{"snippet": "script_module.state_dict(destination=None, prefix='')", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`.", "question_id": 51329}
{"snippet": "script_module.state_dict(destination=None, keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `keep_vars`.", "question_id": 51330}
{"snippet": "script_module.state_dict(prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `prefix`, `keep_vars`.", "question_id": 51331}
{"snippet": "script_module.state_dict(destination=None, prefix='', keep_vars=False)", "intent": "Returns a dictionary containing a whole state of the module . With arguments `destination`, `prefix`, `keep_vars`.", "question_id": 51332}
{"snippet": "script_module.to(*args, **kwargs)", "intent": "Moves and/or casts the parameters and buffers . With arguments `*args`, `**kwargs`.", "question_id": 51333}
{"snippet": "script_module.to_empty(device)", "intent": "Moves the parameters and buffers to the specified `device` without copying storage .", "question_id": 51334}
{"snippet": "script_module.train()", "intent": "Sets the module in training `mode` .", "question_id": 51335}
{"snippet": "script_module.train(mode=True)", "intent": "Sets the module in training `mode` .", "question_id": 51336}
{"snippet": "script_module.type(dst_type)", "intent": "Casts all parameters and buffers to `dst_type` .", "question_id": 51337}
{"snippet": "script_module.xpu()", "intent": "Moves all model parameters and buffers to the XPU .", "question_id": 51338}
{"snippet": "script_module.xpu(device=None)", "intent": "Moves all model parameters and buffers to the XPU . With arguments `device`.", "question_id": 51339}
{"snippet": "script_module.zero_grad()", "intent": "Sets gradients of all model parameters to zero .", "question_id": 51340}
{"snippet": "script_module.zero_grad(set_to_none=False)", "intent": "Sets gradients of all model parameters to zero . With arguments `set_to_none`.", "question_id": 51341}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`.", "question_id": 51342}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`.", "question_id": 51343}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`.", "question_id": 51344}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `dim`.", "question_id": 51345}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `module_kwargs`.", "question_id": 51346}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`.", "question_id": 51347}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `dim`.", "question_id": 51348}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, device_ids=None, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `module_kwargs`.", "question_id": 51349}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None, dim=0)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`, `dim`.", "question_id": 51350}
{"snippet": "torch.nn.parallel.data_parallel(module, inputs, output_device=None, module_kwargs=None)", "intent": "Evaluates `module` ( input ) in parallel across the GPUs given in `device_ids` . With arguments `inputs`, `output_device`, `module_kwargs`.", "question_id": 51351}
{"snippet": "Tensor.lu_solve(LU_data, LU_pivots)", "intent": "See torch.lu_solve ( ) With arguments `LU_data`, `LU_pivots`.", "question_id": 51352}
{"snippet": "torch.cuda.empty_cache()", "intent": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi .", "question_id": 51353}
{"snippet": "Tensor.sigmoid_()", "intent": "In-place version of sigmoid ( )", "question_id": 51354}
{"snippet": "torch.nn.functional.prelu(input, weight)", "intent": "Applies element-wise the function PReLU ( x ) =max\u2061 ( 0 , x ) +weight\u2217min\u2061 ( 0 , x ) \\text { PReLU } ( x ) = \\max ( 0 , x ) + \\text { `weight` } * \\min ( 0 , x ) PReLU ( x ) =max ( 0 , x ) +weight\u2217min ( 0 , x ) where weight is a learnable parameter . With arguments `input`.", "question_id": 51355}
{"snippet": "torch.logical_and(input, other)", "intent": "Computes the element-wise logical AND of the given `input` tensors . With arguments `other`.", "question_id": 51356}
{"snippet": "torch.logical_and(input, other, out=None)", "intent": "Computes the element-wise logical AND of the given `input` tensors . With arguments `other`, `out`.", "question_id": 51357}
{"snippet": "torch.nn.DataParallel(module)", "intent": "Implements data parallelism at the `module` level .", "question_id": 51358}
{"snippet": "torch.nn.DataParallel(module, device_ids=None)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module .", "question_id": 51359}
{"snippet": "torch.nn.DataParallel(module, output_device=None)", "intent": "Implements data parallelism at the `module` level . With arguments `output_device`.", "question_id": 51360}
{"snippet": "torch.nn.DataParallel(module, dim=0)", "intent": "Implements data parallelism at the `module` level . tensors will be scattered on `dim` specified ( default 0 ) .", "question_id": 51361}
{"snippet": "torch.nn.DataParallel(module, device_ids=None, output_device=None)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . With arguments `output_device`.", "question_id": 51362}
{"snippet": "torch.nn.DataParallel(module, device_ids=None, dim=0)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . tensors will be scattered on `dim` specified ( default 0 ) .", "question_id": 51363}
{"snippet": "torch.nn.DataParallel(module, output_device=None, dim=0)", "intent": "Implements data parallelism at the `module` level . tensors will be scattered on `dim` specified ( default 0 ) . With arguments `output_device`.", "question_id": 51364}
{"snippet": "torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)", "intent": "Implements data parallelism at the `module` level . The parallelized module must have its parameters and buffers on `device_ids` [ 0 ] before running this DataParallel module . tensors will be scattered on `dim` specified ( default 0 ) . With arguments `output_device`.", "question_id": 51365}
{"snippet": "Tensor.logdet()", "intent": "See torch.logdet ( )", "question_id": 51366}
{"snippet": "Tensor.float()", "intent": "self.float ( ) is equivalent to self.to ( torch.float32 ) .", "question_id": 51367}
{"snippet": "Tensor.float(memory_format=torch.preserve_format)", "intent": "self.float ( ) is equivalent to self.to ( torch.float32 ) . With arguments `memory_format`.", "question_id": 51368}
{"snippet": "Tensor.imag", "intent": "Returns a new tensor containing imaginary values of the self tensor.", "question_id": 51369}
{"snippet": "torch.broadcast_tensors(*tensors)", "intent": "Broadcasts the given tensors according to Broadcasting semantics . With arguments `*tensors`.", "question_id": 51370}
{"snippet": "torch.cuda.get_device_capability()", "intent": "Gets the cuda capability of a `device` .", "question_id": 51371}
{"snippet": "torch.cuda.get_device_capability(device=None)", "intent": "Gets the cuda capability of a `device` .", "question_id": 51372}
{"snippet": "torch.cuda.max_memory_reserved()", "intent": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 51373}
{"snippet": "torch.cuda.max_memory_reserved(device=None)", "intent": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 51374}
{"snippet": "Tensor.all()", "intent": "See torch.all ( )", "question_id": 51375}
{"snippet": "Tensor.all(dim=None)", "intent": "See torch.all ( ) With arguments `dim`.", "question_id": 51376}
{"snippet": "Tensor.all(keepdim=False)", "intent": "See torch.all ( ) With arguments `keepdim`.", "question_id": 51377}
{"snippet": "Tensor.all(dim=None, keepdim=False)", "intent": "See torch.all ( ) With arguments `dim`, `keepdim`.", "question_id": 51378}
{"snippet": "Tensor.gcd_(other)", "intent": "In-place version of gcd ( ) With arguments `other`.", "question_id": 51379}
{"snippet": "torch.nan_to_num(input)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51380}
{"snippet": "torch.nan_to_num(input, nan=0.0)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51381}
{"snippet": "torch.nan_to_num(input, posinf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51382}
{"snippet": "torch.nan_to_num(input, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51383}
{"snippet": "torch.nan_to_num(input, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 51384}
{"snippet": "torch.nan_to_num(input, nan=0.0, posinf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51385}
{"snippet": "torch.nan_to_num(input, nan=0.0, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51386}
{"snippet": "torch.nan_to_num(input, nan=0.0, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 51387}
{"snippet": "torch.nan_to_num(input, posinf=None, neginf=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively .", "question_id": 51388}
{"snippet": "torch.nan_to_num(input, posinf=None, out=None)", "intent": "Replaces NaN , positive infinity , and negative infinity values in `input` with the values specified by `nan` , `posinf` , and `neginf` , respectively . With arguments `out`.", "question_id": 51389}
{"snippet": "torch.histc(input)", "intent": "Computes the histogram of a tensor . With arguments `input`.", "question_id": 51390}
{"snippet": "torch.histc(input, bins=100)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51391}
{"snippet": "torch.histc(input, min=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51392}
{"snippet": "torch.histc(input, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51393}
{"snippet": "torch.histc(input, out=None)", "intent": "Computes the histogram of a tensor . With arguments `input`, `out`.", "question_id": 51394}
{"snippet": "torch.histc(input, bins=100, min=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51395}
{"snippet": "torch.histc(input, bins=100, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51396}
{"snippet": "torch.histc(input, bins=100, out=None)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`, `out`.", "question_id": 51397}
{"snippet": "torch.histc(input, min=0, max=0)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`.", "question_id": 51398}
{"snippet": "torch.histc(input, min=0, out=None)", "intent": "Computes the histogram of a tensor . The elements are sorted into equal width `bins` between `min` and `max` . With arguments `input`, `out`.", "question_id": 51399}
{"snippet": "torch.cuda.manual_seed(seed)", "intent": "Sets the `seed` for generating random numbers for the current GPU .", "question_id": 51400}
{"snippet": "Tensor.masked_scatter_(mask, source)", "intent": "Copies elements from `source` into self tensor at positions where the `mask` is True .", "question_id": 51401}
{"snippet": "torch.nn.TripletMarginLoss()", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 .", "question_id": 51402}
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 .", "question_id": 51403}
{"snippet": "torch.nn.TripletMarginLoss(p=2.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . A triplet is composed by a , `p` and n ( i.e. , anchor , positive examples and negative examples respectively ) .", "question_id": 51404}
{"snippet": "torch.nn.TripletMarginLoss(eps=1e-06)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `eps`.", "question_id": 51405}
{"snippet": "torch.nn.TripletMarginLoss(swap=False)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . The distance `swap` is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas , E. Riba et al .", "question_id": 51406}
{"snippet": "torch.nn.TripletMarginLoss(size_average=None)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `size_average`.", "question_id": 51407}
{"snippet": "torch.nn.TripletMarginLoss(reduce=None)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `reduce`.", "question_id": 51408}
{"snippet": "torch.nn.TripletMarginLoss(reduction='mean')", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `reduction`.", "question_id": 51409}
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0, p=2.0)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . A triplet is composed by a , `p` and n ( i.e. , anchor , positive examples and negative examples respectively ) .", "question_id": 51410}
{"snippet": "torch.nn.TripletMarginLoss(margin=1.0, eps=1e-06)", "intent": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1 , x2x2x2 , x3x3x3 and a `margin` with a value greater than 000 . With arguments `eps`.", "question_id": 51411}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`.", "question_id": 51412}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `eps`.", "question_id": 51413}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, momentum=0.1)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `momentum`.", "question_id": 51414}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05, momentum=0.1)", "intent": "A BNReLU2d module is a fused module of BatchNorm2d and ReLU With arguments `num_features`, `eps`, `momentum`.", "question_id": 51415}
{"snippet": "torch.nn.MaxUnpool1d(kernel_size)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`.", "question_id": 51416}
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `stride`.", "question_id": 51417}
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `padding`.", "question_id": 51418}
{"snippet": "torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 51419}
{"snippet": "torch.nn.ReflectionPad1d(padding)", "intent": "Pads the input tensor using the reflection of the input boundary . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 51420}
{"snippet": "torch.nn.functional.pixel_shuffle(input, upscale_factor)", "intent": "Rearranges elements in a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) to a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) , where r is the `upscale_factor` . With arguments `input`.", "question_id": 51421}
{"snippet": "torch.neg(input)", "intent": "Returns a new tensor with the negative of the elements of `input` .", "question_id": 51422}
{"snippet": "torch.neg(input, out=None)", "intent": "Returns a new tensor with the negative of the elements of `input` . With arguments `out`.", "question_id": 51423}
{"snippet": "torch.nn.Softsign", "intent": "Applies the element-wise function:", "question_id": 51424}
{"snippet": "Tensor.addr_(vec1, vec2)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`.", "question_id": 51425}
{"snippet": "Tensor.addr_(vec1, vec2, beta=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `beta`.", "question_id": 51426}
{"snippet": "Tensor.addr_(vec1, vec2, alpha=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `alpha`.", "question_id": 51427}
{"snippet": "Tensor.addr_(vec1, vec2, beta=1, alpha=1)", "intent": "In-place version of addr ( ) With arguments `vec1`, `vec2`, `beta`, `alpha`.", "question_id": 51428}
{"snippet": "torch.logsumexp(input, dim)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` .", "question_id": 51429}
{"snippet": "torch.logsumexp(input, dim, keepdim=False)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , the output tensor is of the same size as input except in the dimension ( s ) dim where it is of size 1 .", "question_id": 51430}
{"snippet": "torch.logsumexp(input, dim, out=None)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . With arguments `out`.", "question_id": 51431}
{"snippet": "torch.logsumexp(input, dim, keepdim=False, out=None)", "intent": "Returns the log of summed exponentials of each row of the `input` tensor in the given dimension `dim` . If `keepdim` is True , the output tensor is of the same size as input except in the dimension ( s ) dim where it is of size 1 . With arguments `out`.", "question_id": 51432}
{"snippet": "Tensor.erf_()", "intent": "In-place version of erf ( )", "question_id": 51433}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`.", "question_id": 51434}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`.", "question_id": 51435}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`.", "question_id": 51436}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `last_epoch`.", "question_id": 51437}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `verbose`.", "question_id": 51438}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `eta_min`.", "question_id": 51439}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `last_epoch`.", "question_id": 51440}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `T_mult`, `verbose`.", "question_id": 51441}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0, last_epoch=- 1)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`, `last_epoch`.", "question_id": 51442}
{"snippet": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, eta_min=0, verbose=False)", "intent": "Set the learning rate of each parameter group using a cosine annealing schedule , where \u03b7max\\eta_ { max } \u03b7max\u200b is set to the initial lr , TcurT_ { cur } Tcur\u200b is the number of epochs since the last restart and TiT_ { i } Ti\u200b is the number of epochs between two warm restarts in SGDR : With arguments `optimizer`, `T_0`, `eta_min`, `verbose`.", "question_id": 51443}
{"snippet": "cosine_annealing_warm_restarts.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 51444}
{"snippet": "cosine_annealing_warm_restarts.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 51445}
{"snippet": "cosine_annealing_warm_restarts.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 51446}
{"snippet": "cosine_annealing_warm_restarts.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 51447}
{"snippet": "cosine_annealing_warm_restarts.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 51448}
{"snippet": "cosine_annealing_warm_restarts.step()", "intent": "Step could be called after every batch update", "question_id": 51449}
{"snippet": "cosine_annealing_warm_restarts.step(epoch=None)", "intent": "Step could be called after every batch update With arguments `epoch`.", "question_id": 51450}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 51451}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 51452}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 51453}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 51454}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 51455}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 51456}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 51457}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 51458}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 51459}
{"snippet": "torch.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 51460}
{"snippet": "Tensor.lerp_(end, weight)", "intent": "In-place version of lerp ( ) With arguments `end`, `weight`.", "question_id": 51461}
{"snippet": "torch.nn.functional.unfold(input, kernel_size)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`.", "question_id": 51462}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`.", "question_id": 51463}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, padding=0)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `padding`.", "question_id": 51464}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `stride`.", "question_id": 51465}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `padding`.", "question_id": 51466}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `stride`.", "question_id": 51467}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `padding`, `stride`.", "question_id": 51468}
{"snippet": "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched `input` tensor . With arguments `kernel_size`, `dilation`, `padding`, `stride`.", "question_id": 51469}
{"snippet": "torch.quantization.add_quant_dequant(module)", "intent": "Wrap the leaf child `module` in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well .", "question_id": 51470}
{"snippet": "Tensor.resize_as_(tensor)", "intent": "Resizes the self `tensor` to be the same size as the specified tensor .", "question_id": 51471}
{"snippet": "Tensor.resize_as_(tensor, memory_format=torch.contiguous_format)", "intent": "Resizes the self `tensor` to be the same size as the specified tensor . With arguments `memory_format`.", "question_id": 51472}
{"snippet": "Tensor.tan_()", "intent": "In-place version of tan ( )", "question_id": 51473}
{"snippet": "torch.acosh(input)", "intent": "Returns a new tensor with the inverse hyperbolic cosine of the elements of `input` .", "question_id": 51474}
{"snippet": "torch.acosh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic cosine of the elements of `input` . With arguments `out`.", "question_id": 51475}
{"snippet": "torch.polygamma(n, input)", "intent": "Computes the nthn^ { th } nth derivative of the digamma function on `input` . With arguments `n`.", "question_id": 51476}
{"snippet": "torch.polygamma(n, input, out=None)", "intent": "Computes the nthn^ { th } nth derivative of the digamma function on `input` . With arguments `n`, `out`.", "question_id": 51477}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 51478}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 51479}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 51480}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, output_padding=0)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `output_padding`.", "question_id": 51481}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 51482}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 51483}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 51484}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 51485}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 51486}
{"snippet": "torch.nn.LazyConvTranspose3d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument of the ConvTranspose3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 51487}
{"snippet": "lazy_conv_transpose3d.cls_to_become", "intent": "alias of torch.nn.modules.conv.ConvTranspose3d", "question_id": 51488}
{"snippet": "torch.nn.quantized.functional.hardtanh(input)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`.", "question_id": 51489}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`.", "question_id": 51490}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, max_val=1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `max_val`.", "question_id": 51491}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `inplace`.", "question_id": 51492}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, max_val=1.0)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`.", "question_id": 51493}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `inplace`.", "question_id": 51494}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, max_val=1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `max_val`, `inplace`.", "question_id": 51495}
{"snippet": "torch.nn.quantized.functional.hardtanh(input, min_val=- 1.0, max_val=1.0, inplace=False)", "intent": "This is the quantized version of hardtanh ( ) . With arguments `input`, `min_val`, `max_val`, `inplace`.", "question_id": 51496}
{"snippet": "Tensor.tanh()", "intent": "See torch.tanh ( )", "question_id": 51497}
{"snippet": "Tensor.logit_()", "intent": "In-place version of logit ( )", "question_id": 51498}
{"snippet": "Tensor.matmul(tensor2)", "intent": "See torch.matmul ( ) With arguments `tensor2`.", "question_id": 51499}
{"snippet": "torch.nn.functional.max_pool1d(*args, **kwargs)", "intent": "Applies a 1D max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 51500}
{"snippet": "torch.nn.MaxUnpool3d(kernel_size)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`.", "question_id": 51501}
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `stride`.", "question_id": 51502}
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `padding`.", "question_id": 51503}
{"snippet": "torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool3d . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 51504}
{"snippet": "torch.nn.AdaptiveMaxPool1d(output_size)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 51505}
{"snippet": "torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)", "intent": "Applies a 1D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 51506}
{"snippet": "torch.nn.GaussianNLLLoss()", "intent": "Gaussian negative log likelihood loss .", "question_id": 51507}
{"snippet": "torch.nn.GaussianNLLLoss(full=False)", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True .", "question_id": 51508}
{"snippet": "torch.nn.GaussianNLLLoss(eps=1e-06)", "intent": "Gaussian negative log likelihood loss . where `eps` is used for stability .", "question_id": 51509}
{"snippet": "torch.nn.GaussianNLLLoss(reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `reduction`.", "question_id": 51510}
{"snippet": "torch.nn.GaussianNLLLoss(full=False, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . where `eps` is used for stability .", "question_id": 51511}
{"snippet": "torch.nn.GaussianNLLLoss(full=False, reduction='mean')", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . With arguments `reduction`.", "question_id": 51512}
{"snippet": "torch.nn.GaussianNLLLoss(eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . where `eps` is used for stability . With arguments `reduction`.", "question_id": 51513}
{"snippet": "torch.nn.GaussianNLLLoss(full=False, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . By default , the constant term of the loss function is omitted unless `full` is True . where `eps` is used for stability . With arguments `reduction`.", "question_id": 51514}
{"snippet": "torch.nn.InstanceNorm3d(num_features)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`.", "question_id": 51515}
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `eps`.", "question_id": 51516}
{"snippet": "torch.nn.InstanceNorm3d(num_features, momentum=0.1)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 51517}
{"snippet": "torch.nn.InstanceNorm3d(num_features, affine=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`.", "question_id": 51518}
{"snippet": "torch.nn.InstanceNorm3d(num_features, track_running_stats=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`.", "question_id": 51519}
{"snippet": "torch.nn.InstanceNorm3d(num_features, device=None)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `device`.", "question_id": 51520}
{"snippet": "torch.nn.InstanceNorm3d(num_features, dtype=None)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . With arguments `num_features`, `dtype`.", "question_id": 51521}
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 51522}
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, affine=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . \u03b3\\gamma\u03b3 and \u03b2\\beta\u03b2 are learnable parameter vectors of size C ( where C is the input size ) if `affine` is True . With arguments `num_features`, `eps`.", "question_id": 51523}
{"snippet": "torch.nn.InstanceNorm3d(num_features, eps=1e-05, track_running_stats=False)", "intent": "Applies Instance Normalization over a 5D input ( a mini-batch of 3D inputs with additional channel dimension ) as described in the paper Instance Normalization : The Missing Ingredient for Fast Stylization . If `track_running_stats` is set to True , during training this layer keeps running estimates of its computed mean and variance , which are then used for normalization during evaluation . With arguments `num_features`, `eps`.", "question_id": 51524}
{"snippet": "torch.jit.fork(func, *args, **kwargs)", "intent": "Creates an asynchronous task executing `func` and a reference to the value of the result of this execution . With arguments `*args`, `**kwargs`.", "question_id": 51525}
{"snippet": "torch.jit.optimize_for_inference(mod)", "intent": "Performs a set of optimization passes to optimize a model for the purposes of inference . With arguments `mod`.", "question_id": 51526}
{"snippet": "torch.nn.modules.module.register_module_backward_hook(hook)", "intent": "Registers a backward `hook` common to all the modules .", "question_id": 51527}
{"snippet": "torch.cuda.list_gpu_processes()", "intent": "Returns a human-readable printout of the running processes and their GPU memory use for a given `device` .", "question_id": 51528}
{"snippet": "torch.cuda.list_gpu_processes(device=None)", "intent": "Returns a human-readable printout of the running processes and their GPU memory use for a given `device` .", "question_id": 51529}
{"snippet": "torch.optim.Adadelta(params)", "intent": "Implements Adadelta algorithm . With arguments `params`.", "question_id": 51530}
{"snippet": "torch.optim.Adadelta(params, lr=1.0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`.", "question_id": 51531}
{"snippet": "torch.optim.Adadelta(params, rho=0.9)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`.", "question_id": 51532}
{"snippet": "torch.optim.Adadelta(params, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `eps`.", "question_id": 51533}
{"snippet": "torch.optim.Adadelta(params, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `weight_decay`.", "question_id": 51534}
{"snippet": "torch.optim.Adadelta(params, lr=1.0, rho=0.9)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `rho`.", "question_id": 51535}
{"snippet": "torch.optim.Adadelta(params, lr=1.0, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 51536}
{"snippet": "torch.optim.Adadelta(params, lr=1.0, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 51537}
{"snippet": "torch.optim.Adadelta(params, rho=0.9, eps=1e-06)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`, `eps`.", "question_id": 51538}
{"snippet": "torch.optim.Adadelta(params, rho=0.9, weight_decay=0)", "intent": "Implements Adadelta algorithm . With arguments `params`, `rho`, `weight_decay`.", "question_id": 51539}
{"snippet": "adadelta.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51540}
{"snippet": "adadelta.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51541}
{"snippet": "adadelta.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51542}
{"snippet": "adadelta.step()", "intent": "Performs a single optimization step .", "question_id": 51543}
{"snippet": "adadelta.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51544}
{"snippet": "adadelta.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51545}
{"snippet": "adadelta.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51546}
{"snippet": "torch.signbit(input)", "intent": "Tests if each element of `input` has its sign bit set ( is less than zero ) or not .", "question_id": 51547}
{"snippet": "torch.signbit(input, out=None)", "intent": "Tests if each element of `input` has its sign bit set ( is less than zero ) or not . With arguments `out`.", "question_id": 51548}
{"snippet": "torch.stft(input, n_fft)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True .", "question_id": 51549}
{"snippet": "torch.stft(input, n_fft, hop_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `hop_length`.", "question_id": 51550}
{"snippet": "torch.stft(input, n_fft, win_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `win_length`.", "question_id": 51551}
{"snippet": "torch.stft(input, n_fft, window=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True .", "question_id": 51552}
{"snippet": "torch.stft(input, n_fft, center=True)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `center`.", "question_id": 51553}
{"snippet": "torch.stft(input, n_fft, pad_mode='reflect')", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `pad_mode`.", "question_id": 51554}
{"snippet": "torch.stft(input, n_fft, normalized=False)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `normalized`.", "question_id": 51555}
{"snippet": "torch.stft(input, n_fft, onesided=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `onesided`.", "question_id": 51556}
{"snippet": "torch.stft(input, n_fft, return_complex=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . Returns either a complex tensor of size ( \u2217\u00d7N\u00d7T ) ( * \\times N \\times T ) ( \u2217\u00d7N\u00d7T ) if `return_complex` is true , or a real tensor of size ( \u2217\u00d7N\u00d7T\u00d72 ) ( * \\times N \\times T \\times 2 ) ( \u2217\u00d7N\u00d7T\u00d72 ) .", "question_id": 51557}
{"snippet": "torch.stft(input, n_fft, hop_length=None, win_length=None)", "intent": "Short-time Fourier transform ( STFT ) . The STFT computes the Fourier transform of short overlapping windows of the `input` . where mmm is the index of the sliding `window` , and \u03c9\\omega\u03c9 is the frequency 0\u2264\u03c9 < n_fft0 \\leq \\omega < \\text { n\\_fft } 0\u2264\u03c9 < `n_fft` for onesided=False , or 0\u2264\u03c9 < \u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text { n\\_fft } / 2 \\rfloor + 10\u2264\u03c9 < \u230an_fft/2\u230b+1 for onesided=True . With arguments `hop_length`, `win_length`.", "question_id": 51558}
{"snippet": "torch.cuda.stream(stream)", "intent": "Wrapper around the Context-manager StreamContext that selects a given `stream` .", "question_id": 51559}
{"snippet": "torch.nn.Unfold(kernel_size)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks :", "question_id": 51560}
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51561}
{"snippet": "torch.nn.Unfold(kernel_size, padding=0)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51562}
{"snippet": "torch.nn.Unfold(kernel_size, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51563}
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, padding=0)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51564}
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51565}
{"snippet": "torch.nn.Unfold(kernel_size, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51566}
{"snippet": "torch.nn.Unfold(kernel_size, dilation=1, padding=0, stride=1)", "intent": "Extracts sliding local blocks from a batched input tensor . This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column ( i.e. , last dimension ) of a 3-D output tensor of shape ( N , C\u00d7\u220f ( `kernel_size` ) , L ) ( N , C \\times \\prod ( \\text { kernel\\_size } ) , L ) ( N , C\u00d7\u220f ( kernel_size ) , L ) , where C\u00d7\u220f ( kernel_size ) C \\times \\prod ( \\text { kernel\\_size } ) C\u00d7\u220f ( kernel_size ) is the total number of values within each block ( a block has \u220f ( kernel_size ) \\prod ( \\text { kernel\\_size } ) \u220f ( kernel_size ) spatial locations each containing a CCC-channeled vector ) , and LLL is the total number of such blocks : The `padding` , `stride` and `dilation` arguments specify how the sliding blocks are retrieved .", "question_id": 51567}
{"snippet": "Tensor.isneginf()", "intent": "See torch.isneginf ( )", "question_id": 51568}
{"snippet": "torch.nn.quantized.functional.adaptive_avg_pool3d(input, output_size)", "intent": "Applies a 3D adaptive average pooling over a quantized `input` signal composed of several quantized input planes . With arguments `output_size`.", "question_id": 51569}
{"snippet": "Tensor.dequantize()", "intent": "Given a quantized Tensor , dequantize it and return the dequantized float Tensor .", "question_id": 51570}
{"snippet": "torch.nn.AvgPool2d(kernel_size)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as :", "question_id": 51571}
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be :", "question_id": 51572}
{"snippet": "torch.nn.AvgPool2d(kernel_size, padding=0)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 51573}
{"snippet": "torch.nn.AvgPool2d(kernel_size, ceil_mode=False)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 51574}
{"snippet": "torch.nn.AvgPool2d(kernel_size, count_include_pad=True)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `count_include_pad`.", "question_id": 51575}
{"snippet": "torch.nn.AvgPool2d(kernel_size, divisor_override=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : With arguments `divisor_override`.", "question_id": 51576}
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, padding=0)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 51577}
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `ceil_mode`.", "question_id": 51578}
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `count_include_pad`.", "question_id": 51579}
{"snippet": "torch.nn.AvgPool2d(kernel_size, stride=None, divisor_override=None)", "intent": "Applies a 2D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , H , W ) ( N , C , H , W ) ( N , C , H , W ) , output ( N , C , Hout , Wout ) ( N , C , H_ { out } , W_ { out } ) ( N , C , Hout\u200b , Wout\u200b ) and `kernel_size` ( kH , kW ) ( kH , kW ) ( kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding can either be : With arguments `divisor_override`.", "question_id": 51580}
{"snippet": "torch.nn.ReLU()", "intent": "Applies the rectified linear unit function element-wise :", "question_id": 51581}
{"snippet": "torch.nn.ReLU(inplace=False)", "intent": "Applies the rectified linear unit function element-wise : With arguments `inplace`.", "question_id": 51582}
{"snippet": "torch.logit(input)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`.", "question_id": 51583}
{"snippet": "torch.logit(input, eps=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `eps`.", "question_id": 51584}
{"snippet": "torch.logit(input, out=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `out`.", "question_id": 51585}
{"snippet": "torch.logit(input, eps=None, out=None)", "intent": "Alias for torch.special.logit ( ) . With arguments `input`, `eps`, `out`.", "question_id": 51586}
{"snippet": "torch._assert(condition, message)", "intent": "A wrapper around Python \u2019 s assert which is symbolically traceable . With arguments `condition`, `message`.", "question_id": 51587}
{"snippet": "torch.logaddexp(input, other)", "intent": "Logarithm of the sum of exponentiations of the inputs . With arguments `input`, `other`.", "question_id": 51588}
{"snippet": "torch.logaddexp(input, other, out=None)", "intent": "Logarithm of the sum of exponentiations of the inputs . With arguments `input`, `other`, `out`.", "question_id": 51589}
{"snippet": "torch.cuda.initial_seed()", "intent": "Returns the current random seed of the current GPU .", "question_id": 51590}
{"snippet": "torch.nn.functional.fractional_max_pool2d(*args, **kwargs)", "intent": "Applies 2D fractional max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 51591}
{"snippet": "torch.ones_like(input)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` .", "question_id": 51592}
{"snippet": "torch.ones_like(input, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`.", "question_id": 51593}
{"snippet": "torch.ones_like(input, layout=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `layout`.", "question_id": 51594}
{"snippet": "torch.ones_like(input, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `device`.", "question_id": 51595}
{"snippet": "torch.ones_like(input, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `requires_grad`.", "question_id": 51596}
{"snippet": "torch.ones_like(input, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `memory_format`.", "question_id": 51597}
{"snippet": "torch.ones_like(input, dtype=None, layout=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 51598}
{"snippet": "torch.ones_like(input, dtype=None, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 51599}
{"snippet": "torch.ones_like(input, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 51600}
{"snippet": "torch.ones_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns a tensor filled with the scalar value 1 , with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 51601}
{"snippet": "Tensor.bitwise_xor()", "intent": "See torch.bitwise_xor ( )", "question_id": 51602}
{"snippet": "torch.empty_strided(size, stride)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively .", "question_id": 51603}
{"snippet": "torch.empty_strided(size, stride, dtype=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`.", "question_id": 51604}
{"snippet": "torch.empty_strided(size, stride, layout=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `layout`.", "question_id": 51605}
{"snippet": "torch.empty_strided(size, stride, device=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `device`.", "question_id": 51606}
{"snippet": "torch.empty_strided(size, stride, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `requires_grad`.", "question_id": 51607}
{"snippet": "torch.empty_strided(size, stride, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `pin_memory`.", "question_id": 51608}
{"snippet": "torch.empty_strided(size, stride, dtype=None, layout=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `layout`.", "question_id": 51609}
{"snippet": "torch.empty_strided(size, stride, dtype=None, device=None)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `device`.", "question_id": 51610}
{"snippet": "torch.empty_strided(size, stride, dtype=None, requires_grad=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `requires_grad`.", "question_id": 51611}
{"snippet": "torch.empty_strided(size, stride, dtype=None, pin_memory=False)", "intent": "Returns a tensor filled with uninitialized data . The shape and strides of the tensor is defined by the variable argument `size` and `stride` respectively . With arguments `dtype`, `pin_memory`.", "question_id": 51612}
{"snippet": "torch.optim.Rprop(params, 1.2), 50))", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`.", "question_id": 51613}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`.", "question_id": 51614}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), etas=(0.5)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `etas`.", "question_id": 51615}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `step_sizes`.", "question_id": 51616}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, etas=(0.5)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `etas`.", "question_id": 51617}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `step_sizes`.", "question_id": 51618}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), etas=(0.5, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `etas`, `step_sizes`.", "question_id": 51619}
{"snippet": "torch.optim.Rprop(params, 1.2), 50), lr=0.01, etas=(0.5, step_sizes=(1e-06)", "intent": "Implements the resilient backpropagation algorithm . With arguments `params`, `1.2)`, `50)`, `lr`, `etas`, `step_sizes`.", "question_id": 51620}
{"snippet": "rprop.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51621}
{"snippet": "rprop.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51622}
{"snippet": "rprop.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51623}
{"snippet": "rprop.step()", "intent": "Performs a single optimization step .", "question_id": 51624}
{"snippet": "rprop.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51625}
{"snippet": "rprop.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51626}
{"snippet": "rprop.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51627}
{"snippet": "Tensor.heaviside(values)", "intent": "See torch.heaviside ( ) With arguments `values`.", "question_id": 51628}
{"snippet": "torch.nn.utils.remove_weight_norm(module)", "intent": "Removes the weight normalization reparameterization from a `module` .", "question_id": 51629}
{"snippet": "torch.nn.utils.remove_weight_norm(module, name='weight')", "intent": "Removes the weight normalization reparameterization from a `module` . With arguments `name`.", "question_id": 51630}
{"snippet": "Tensor.multiply(value)", "intent": "See torch.multiply ( ) . With arguments `value`.", "question_id": 51631}
{"snippet": "Tensor.is_shared()", "intent": "Checks if tensor is in shared memory .", "question_id": 51632}
{"snippet": "torch.gcd(input, other)", "intent": "Computes the element-wise greatest common divisor ( GCD ) of `input` and `other` .", "question_id": 51633}
{"snippet": "torch.gcd(input, other, out=None)", "intent": "Computes the element-wise greatest common divisor ( GCD ) of `input` and `other` . With arguments `out`.", "question_id": 51634}
{"snippet": "Tensor.frac_()", "intent": "In-place version of frac ( )", "question_id": 51635}
{"snippet": "torch.compiled_with_cxx11_abi()", "intent": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1", "question_id": 51636}
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`.", "question_id": 51637}
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `last_epoch`.", "question_id": 51638}
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `verbose`.", "question_id": 51639}
{"snippet": "torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every epoch . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 51640}
{"snippet": "exponential_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 51641}
{"snippet": "exponential_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 51642}
{"snippet": "exponential_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 51643}
{"snippet": "exponential_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 51644}
{"snippet": "exponential_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 51645}
{"snippet": "torch.nn.functional.embedding_bag(input, weight)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`.", "question_id": 51646}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, offsets=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `offsets`.", "question_id": 51647}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, max_norm=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `max_norm`.", "question_id": 51648}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, norm_type=2)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `norm_type`.", "question_id": 51649}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, scale_grad_by_freq=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `scale_grad_by_freq`.", "question_id": 51650}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, mode='mean')", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `mode`.", "question_id": 51651}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, sparse=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `sparse`.", "question_id": 51652}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, per_sample_weights=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `per_sample_weights`.", "question_id": 51653}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, include_last_offset=False)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `include_last_offset`.", "question_id": 51654}
{"snippet": "torch.nn.functional.embedding_bag(input, weight, padding_idx=None)", "intent": "Computes sums , means or maxes of bags of embeddings , without instantiating the intermediate embeddings . With arguments `input`, `weight`, `padding_idx`.", "question_id": 51655}
{"snippet": "torch.quantization.convert(module)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class .", "question_id": 51656}
{"snippet": "torch.quantization.convert(module, mapping=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class .", "question_id": 51657}
{"snippet": "torch.quantization.convert(module, inplace=False)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`.", "question_id": 51658}
{"snippet": "torch.quantization.convert(module, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True .", "question_id": 51659}
{"snippet": "torch.quantization.convert(module, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `convert_custom_config_dict`.", "question_id": 51660}
{"snippet": "torch.quantization.convert(module, mapping=None, inplace=False)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`.", "question_id": 51661}
{"snippet": "torch.quantization.convert(module, mapping=None, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True .", "question_id": 51662}
{"snippet": "torch.quantization.convert(module, mapping=None, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `convert_custom_config_dict`.", "question_id": 51663}
{"snippet": "torch.quantization.convert(module, inplace=False, remove_qconfig=True)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . And remove qconfig at the end if `remove_qconfig` is set to True . With arguments `inplace`.", "question_id": 51664}
{"snippet": "torch.quantization.convert(module, inplace=False, convert_custom_config_dict=None)", "intent": "Converts submodules in input `module` to a different module according to `mapping` by calling from_float method on the target module class . With arguments `inplace`, `convert_custom_config_dict`.", "question_id": 51665}
{"snippet": "torch.optim.Adagrad(params)", "intent": "Implements Adagrad algorithm . With arguments `params`.", "question_id": 51666}
{"snippet": "torch.optim.Adagrad(params, lr=0.01)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`.", "question_id": 51667}
{"snippet": "torch.optim.Adagrad(params, lr_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr_decay`.", "question_id": 51668}
{"snippet": "torch.optim.Adagrad(params, weight_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `weight_decay`.", "question_id": 51669}
{"snippet": "torch.optim.Adagrad(params, initial_accumulator_value=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `initial_accumulator_value`.", "question_id": 51670}
{"snippet": "torch.optim.Adagrad(params, eps=1e-10)", "intent": "Implements Adagrad algorithm . With arguments `params`, `eps`.", "question_id": 51671}
{"snippet": "torch.optim.Adagrad(params, lr=0.01, lr_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `lr_decay`.", "question_id": 51672}
{"snippet": "torch.optim.Adagrad(params, lr=0.01, weight_decay=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 51673}
{"snippet": "torch.optim.Adagrad(params, lr=0.01, initial_accumulator_value=0)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `initial_accumulator_value`.", "question_id": 51674}
{"snippet": "torch.optim.Adagrad(params, lr=0.01, eps=1e-10)", "intent": "Implements Adagrad algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 51675}
{"snippet": "adagrad.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51676}
{"snippet": "adagrad.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51677}
{"snippet": "adagrad.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51678}
{"snippet": "adagrad.step()", "intent": "Performs a single optimization step .", "question_id": 51679}
{"snippet": "adagrad.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51680}
{"snippet": "adagrad.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51681}
{"snippet": "adagrad.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51682}
{"snippet": "torch.set_rng_state(new_state)", "intent": "Sets the random number generator state . With arguments `new_state`.", "question_id": 51683}
{"snippet": "Tensor.argsort()", "intent": "See torch.argsort ( )", "question_id": 51684}
{"snippet": "Tensor.argsort(dim=- 1)", "intent": "See torch.argsort ( ) With arguments `dim`.", "question_id": 51685}
{"snippet": "Tensor.argsort(descending=False)", "intent": "See torch.argsort ( ) With arguments `descending`.", "question_id": 51686}
{"snippet": "Tensor.argsort(dim=- 1, descending=False)", "intent": "See torch.argsort ( ) With arguments `dim`, `descending`.", "question_id": 51687}
{"snippet": "Tensor.indices()", "intent": "Return the indices tensor of a sparse COO tensor .", "question_id": 51688}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`.", "question_id": 51689}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`.", "question_id": 51690}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, stride=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `stride`.", "question_id": 51691}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `padding`.", "question_id": 51692}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `output_padding`.", "question_id": 51693}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, groups=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `groups`.", "question_id": 51694}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `dilation`.", "question_id": 51695}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `stride`.", "question_id": 51696}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `padding`.", "question_id": 51697}
{"snippet": "torch.nn.functional.conv_transpose1d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an `input` signal composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `output_padding`.", "question_id": 51698}
{"snippet": "torch.nn.functional.instance_norm(input)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`.", "question_id": 51699}
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`.", "question_id": 51700}
{"snippet": "torch.nn.functional.instance_norm(input, running_var=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_var`.", "question_id": 51701}
{"snippet": "torch.nn.functional.instance_norm(input, weight=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `weight`.", "question_id": 51702}
{"snippet": "torch.nn.functional.instance_norm(input, bias=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `bias`.", "question_id": 51703}
{"snippet": "torch.nn.functional.instance_norm(input, use_input_stats=True)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `use_input_stats`.", "question_id": 51704}
{"snippet": "torch.nn.functional.instance_norm(input, momentum=0.1)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `momentum`.", "question_id": 51705}
{"snippet": "torch.nn.functional.instance_norm(input, eps=1e-05)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `eps`.", "question_id": 51706}
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None, running_var=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`, `running_var`.", "question_id": 51707}
{"snippet": "torch.nn.functional.instance_norm(input, running_mean=None, weight=None)", "intent": "Applies Instance Normalization for each channel in each data sample in a batch . With arguments `input`, `running_mean`, `weight`.", "question_id": 51708}
{"snippet": "torch.optim.RMSprop(params)", "intent": "Implements RMSprop algorithm . With arguments `params`.", "question_id": 51709}
{"snippet": "torch.optim.RMSprop(params, lr=0.01)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`.", "question_id": 51710}
{"snippet": "torch.optim.RMSprop(params, alpha=0.99)", "intent": "Implements RMSprop algorithm . With arguments `params`, `alpha`.", "question_id": 51711}
{"snippet": "torch.optim.RMSprop(params, eps=1e-08)", "intent": "Implements RMSprop algorithm . With arguments `params`, `eps`.", "question_id": 51712}
{"snippet": "torch.optim.RMSprop(params, weight_decay=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `weight_decay`.", "question_id": 51713}
{"snippet": "torch.optim.RMSprop(params, momentum=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `momentum`.", "question_id": 51714}
{"snippet": "torch.optim.RMSprop(params, centered=False)", "intent": "Implements RMSprop algorithm . The `centered` version first appears in Generating Sequences With Recurrent Neural Networks . With arguments `params`.", "question_id": 51715}
{"snippet": "torch.optim.RMSprop(params, lr=0.01, alpha=0.99)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `alpha`.", "question_id": 51716}
{"snippet": "torch.optim.RMSprop(params, lr=0.01, eps=1e-08)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `eps`.", "question_id": 51717}
{"snippet": "torch.optim.RMSprop(params, lr=0.01, weight_decay=0)", "intent": "Implements RMSprop algorithm . With arguments `params`, `lr`, `weight_decay`.", "question_id": 51718}
{"snippet": "rm_sprop.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51719}
{"snippet": "rm_sprop.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51720}
{"snippet": "rm_sprop.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51721}
{"snippet": "rm_sprop.step()", "intent": "Performs a single optimization step .", "question_id": 51722}
{"snippet": "rm_sprop.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51723}
{"snippet": "rm_sprop.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51724}
{"snippet": "rm_sprop.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51725}
{"snippet": "torch.nn.AlphaDropout()", "intent": "Applies Alpha Dropout over the input .", "question_id": 51726}
{"snippet": "torch.nn.AlphaDropout(p=0.5)", "intent": "Applies Alpha Dropout over the input . During training , it randomly masks some of the elements of the input tensor with probability `p` using samples from a bernoulli distribution .", "question_id": 51727}
{"snippet": "torch.nn.AlphaDropout(inplace=False)", "intent": "Applies Alpha Dropout over the input . With arguments `inplace`.", "question_id": 51728}
{"snippet": "torch.nn.AlphaDropout(p=0.5, inplace=False)", "intent": "Applies Alpha Dropout over the input . During training , it randomly masks some of the elements of the input tensor with probability `p` using samples from a bernoulli distribution . With arguments `inplace`.", "question_id": 51729}
{"snippet": "torch.are_deterministic_algorithms_enabled()", "intent": "Returns True if the global deterministic flag is turned on .", "question_id": 51730}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 51731}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 51732}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, training=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 51733}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 51734}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 51735}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 51736}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, training=False, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 51737}
{"snippet": "torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)", "intent": "Randomly masks out entire channels ( a channel is a feature map , e.g . the jjj-th channel of the iii-th sample in the batch `input` is a tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each element will be masked independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 51738}
{"snippet": "torch.matrix_rank(input)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues .", "question_id": 51739}
{"snippet": "torch.matrix_rank(input, tol=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 .", "question_id": 51740}
{"snippet": "torch.matrix_rank(input, symmetric=False)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues .", "question_id": 51741}
{"snippet": "torch.matrix_rank(input, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . With arguments `out`.", "question_id": 51742}
{"snippet": "torch.matrix_rank(input, tol=None, symmetric=False)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 .", "question_id": 51743}
{"snippet": "torch.matrix_rank(input, tol=None, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 . With arguments `out`.", "question_id": 51744}
{"snippet": "torch.matrix_rank(input, symmetric=False, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . With arguments `out`.", "question_id": 51745}
{"snippet": "torch.matrix_rank(input, tol=None, symmetric=False, out=None)", "intent": "Returns the numerical rank of a 2-D tensor . If `symmetric` is True , then `input` is assumed to be symmetric , and the computation of the rank is done by obtaining the eigenvalues . `tol` is the threshold below which the singular values ( or the eigenvalues when symmetric is True ) are considered to be 0 . With arguments `out`.", "question_id": 51746}
{"snippet": "Tensor.to_mkldnn()", "intent": "Returns a copy of the tensor in torch.mkldnn layout .", "question_id": 51747}
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input)", "intent": "Upsamples the `input` , using bilinear upsampling .", "question_id": 51748}
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, size=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`.", "question_id": 51749}
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `scale_factor`.", "question_id": 51750}
{"snippet": "torch.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None)", "intent": "Upsamples the `input` , using bilinear upsampling . With arguments `size`, `scale_factor`.", "question_id": 51751}
{"snippet": "Tensor.clamp()", "intent": "See torch.clamp ( )", "question_id": 51752}
{"snippet": "Tensor.clamp(min=None)", "intent": "See torch.clamp ( ) With arguments `min`.", "question_id": 51753}
{"snippet": "Tensor.clamp(max=None)", "intent": "See torch.clamp ( ) With arguments `max`.", "question_id": 51754}
{"snippet": "Tensor.clamp(min=None, max=None)", "intent": "See torch.clamp ( ) With arguments `min`, `max`.", "question_id": 51755}
{"snippet": "Tensor.eq(other)", "intent": "See torch.eq ( ) With arguments `other`.", "question_id": 51756}
{"snippet": "Tensor.storage_type()", "intent": "Returns the type of the underlying storage .", "question_id": 51757}
{"snippet": "Tensor.unfold(dimension, size, step)", "intent": "Returns a view of the original tensor which contains all slices of `size` size from self tensor in the `dimension` dimension . Step between two slices is given by `step` .", "question_id": 51758}
{"snippet": "torch.nn.PairwiseDistance()", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm :", "question_id": 51759}
{"snippet": "torch.nn.PairwiseDistance(p=2.0)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`.", "question_id": 51760}
{"snippet": "torch.nn.PairwiseDistance(eps=1e-06)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `eps`.", "question_id": 51761}
{"snippet": "torch.nn.PairwiseDistance(keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `keepdim`.", "question_id": 51762}
{"snippet": "torch.nn.PairwiseDistance(p=2.0, eps=1e-06)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `eps`.", "question_id": 51763}
{"snippet": "torch.nn.PairwiseDistance(p=2.0, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `keepdim`.", "question_id": 51764}
{"snippet": "torch.nn.PairwiseDistance(eps=1e-06, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `eps`, `keepdim`.", "question_id": 51765}
{"snippet": "torch.nn.PairwiseDistance(p=2.0, eps=1e-06, keepdim=False)", "intent": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b , v2v_2v2\u200b using the p-norm : With arguments `p`, `eps`, `keepdim`.", "question_id": 51766}
{"snippet": "torch.arange(end)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 51767}
{"snippet": "torch.arange(end, start=0)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 51768}
{"snippet": "torch.arange(end, step=1)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 51769}
{"snippet": "torch.arange(end, out=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `out`.", "question_id": 51770}
{"snippet": "torch.arange(end, dtype=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `dtype`.", "question_id": 51771}
{"snippet": "torch.arange(end, layout=torch.strided)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `layout`.", "question_id": 51772}
{"snippet": "torch.arange(end, device=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `device`.", "question_id": 51773}
{"snippet": "torch.arange(end, requires_grad=False)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `requires_grad`.", "question_id": 51774}
{"snippet": "torch.arange(end, start=0, step=1)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start .", "question_id": 51775}
{"snippet": "torch.arange(end, start=0, out=None)", "intent": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rceil\u2308stepend\u2212start\u200b\u2309 with values from the interval [ start , end ) taken with common difference step beginning from start . With arguments `out`.", "question_id": 51776}
{"snippet": "torch.poisson(input)", "intent": "Returns a tensor of the same size as `input` with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e. ,", "question_id": 51777}
{"snippet": "torch.poisson(input, generator=None)", "intent": "Returns a tensor of the same size as `input` with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e. , With arguments `generator`.", "question_id": 51778}
{"snippet": "torch.optim.SGD(params)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`.", "question_id": 51779}
{"snippet": "torch.optim.SGD(params, lr=<required parameter>)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`.", "question_id": 51780}
{"snippet": "torch.optim.SGD(params, momentum=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`.", "question_id": 51781}
{"snippet": "torch.optim.SGD(params, dampening=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `dampening`.", "question_id": 51782}
{"snippet": "torch.optim.SGD(params, weight_decay=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `weight_decay`.", "question_id": 51783}
{"snippet": "torch.optim.SGD(params, nesterov=False)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `nesterov`.", "question_id": 51784}
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, momentum=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`.", "question_id": 51785}
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, dampening=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `dampening`.", "question_id": 51786}
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, weight_decay=0)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `weight_decay`.", "question_id": 51787}
{"snippet": "torch.optim.SGD(params, lr=<required parameter>, nesterov=False)", "intent": "Implements stochastic gradient descent ( optionally with `momentum` ) . With arguments `params`, `lr`, `nesterov`.", "question_id": 51788}
{"snippet": "sgd.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 51789}
{"snippet": "sgd.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 51790}
{"snippet": "sgd.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 51791}
{"snippet": "sgd.step()", "intent": "Performs a single optimization step .", "question_id": 51792}
{"snippet": "sgd.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 51793}
{"snippet": "sgd.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 51794}
{"snippet": "sgd.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 51795}
{"snippet": "torch.nn.utils.prune.identity(module, name)", "intent": "Applies pruning reparametrization to the tensor corresponding to the parameter called `name` in `module` without actually pruning any units .", "question_id": 51796}
{"snippet": "torch.dequantize(tensor)", "intent": "Returns an fp32 Tensor by dequantizing a quantized Tensor With arguments `tensor`.", "question_id": 51797}
{"snippet": "Tensor.true_divide(value)", "intent": "See torch.true_divide ( ) With arguments `value`.", "question_id": 51798}
{"snippet": "torch.digamma(input)", "intent": "Computes the logarithmic derivative of the gamma function on `input` .", "question_id": 51799}
{"snippet": "torch.digamma(input, out=None)", "intent": "Computes the logarithmic derivative of the gamma function on `input` . With arguments `out`.", "question_id": 51800}
{"snippet": "torch.linalg.lstsq(A, B)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as", "question_id": 51801}
{"snippet": "torch.linalg.lstsq(A, B, rcond=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as With arguments `rcond`.", "question_id": 51802}
{"snippet": "torch.linalg.lstsq(A, B, driver=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as `driver` chooses the LAPACK/MAGMA function that will be used .", "question_id": 51803}
{"snippet": "torch.linalg.lstsq(A, B, rcond=None, driver=None)", "intent": "Computes a solution to the least squares problem of a system of linear equations . For CUDA input , the only valid driver is \u2018 gels \u2019 , which assumes that `A` is full-rank . Letting K\\mathbb { K } K be R\\mathbb { R } R or C\\mathbb { C } C , the least squares problem for a linear system AX=BAX = BAX=B with A\u2208Km\u00d7n , B\u2208Km\u00d7kA \\in \\mathbb { K } ^ { m \\times n } , `B` \\in \\mathbb { K } ^ { m \\times k } A\u2208Km\u00d7n , B\u2208Km\u00d7k is defined as `driver` chooses the LAPACK/MAGMA function that will be used . With arguments `rcond`.", "question_id": 51804}
{"snippet": "Tensor.equal(other)", "intent": "See torch.equal ( ) With arguments `other`.", "question_id": 51805}
{"snippet": "Tensor.storage_offset()", "intent": "Returns self tensor \u2019 s offset in the underlying storage in terms of number of storage elements ( not bytes ) .", "question_id": 51806}
{"snippet": "torch.logical_xor(input, other)", "intent": "Computes the element-wise logical XOR of the given `input` tensors . With arguments `other`.", "question_id": 51807}
{"snippet": "torch.logical_xor(input, other, out=None)", "intent": "Computes the element-wise logical XOR of the given `input` tensors . With arguments `other`, `out`.", "question_id": 51808}
{"snippet": "torch.mean(input)", "intent": "Returns the mean value of all elements in the `input` tensor .", "question_id": 51809}
{"snippet": "Tensor.storage()", "intent": "Returns the underlying storage .", "question_id": 51810}
{"snippet": "Tensor.var(dim)", "intent": "See torch.var ( ) With arguments `dim`.", "question_id": 51811}
{"snippet": "Tensor.var(dim, unbiased=True)", "intent": "See torch.var ( ) With arguments `dim`, `unbiased`.", "question_id": 51812}
{"snippet": "Tensor.var(dim, keepdim=False)", "intent": "See torch.var ( ) With arguments `dim`, `keepdim`.", "question_id": 51813}
{"snippet": "Tensor.var(dim, unbiased=True, keepdim=False)", "intent": "See torch.var ( ) With arguments `dim`, `unbiased`, `keepdim`.", "question_id": 51814}
{"snippet": "torch.greater(input, other)", "intent": "Alias for torch.gt ( ) . With arguments `input`, `other`.", "question_id": 51815}
{"snippet": "torch.greater(input, other, out=None)", "intent": "Alias for torch.gt ( ) . With arguments `input`, `other`, `out`.", "question_id": 51816}
{"snippet": "Tensor.rsqrt_()", "intent": "In-place version of rsqrt ( )", "question_id": 51817}
{"snippet": "torch.quantization.fake_quantize.FakeQuantizeBase", "intent": "Base fake quantize module Any fake quantize implementation should derive from this class.", "question_id": 51818}
{"snippet": "fake_quantize_base.with_args(**kwargs)", "intent": "Wrapper that allows creation of class factories . With arguments `**kwargs`.", "question_id": 51819}
{"snippet": "Tensor.multinomial(num_samples)", "intent": "See torch.multinomial ( ) With arguments `num_samples`.", "question_id": 51820}
{"snippet": "Tensor.multinomial(num_samples, replacement=False)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `replacement`.", "question_id": 51821}
{"snippet": "Tensor.multinomial(num_samples, generator=None)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `generator`.", "question_id": 51822}
{"snippet": "Tensor.multinomial(num_samples, replacement=False, generator=None)", "intent": "See torch.multinomial ( ) With arguments `num_samples`, `replacement`, `generator`.", "question_id": 51823}
{"snippet": "torch.nn.ConstantPad1d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 51824}
{"snippet": "torch.set_num_interop_threads(int)", "intent": "Sets the number of threads used for interop parallelism ( e.g . With arguments `int`.", "question_id": 51825}
{"snippet": "Tensor.diagonal()", "intent": "See torch.diagonal ( )", "question_id": 51826}
{"snippet": "Tensor.diagonal(offset=0)", "intent": "See torch.diagonal ( ) With arguments `offset`.", "question_id": 51827}
{"snippet": "Tensor.diagonal(dim1=0)", "intent": "See torch.diagonal ( ) With arguments `dim1`.", "question_id": 51828}
{"snippet": "Tensor.diagonal(dim2=1)", "intent": "See torch.diagonal ( ) With arguments `dim2`.", "question_id": 51829}
{"snippet": "Tensor.diagonal(offset=0, dim1=0)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim1`.", "question_id": 51830}
{"snippet": "Tensor.diagonal(offset=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim2`.", "question_id": 51831}
{"snippet": "Tensor.diagonal(dim1=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `dim1`, `dim2`.", "question_id": 51832}
{"snippet": "Tensor.diagonal(offset=0, dim1=0, dim2=1)", "intent": "See torch.diagonal ( ) With arguments `offset`, `dim1`, `dim2`.", "question_id": 51833}
{"snippet": "Tensor.is_contiguous()", "intent": "Returns True if self tensor is contiguous in memory in the order specified by memory format .", "question_id": 51834}
{"snippet": "Tensor.is_contiguous(memory_format=torch.contiguous_format)", "intent": "Returns True if self tensor is contiguous in memory in the order specified by memory format . With arguments `memory_format`.", "question_id": 51835}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`.", "question_id": 51836}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 51837}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 51838}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, steps_per_epoch=None)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . You must either provide a value for `total_steps` or provide a value for both `epochs` and `steps_per_epoch` . With arguments `optimizer`, `max_lr`.", "question_id": 51839}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, pct_start=0.3)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `pct_start`.", "question_id": 51840}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, anneal_strategy='cos')", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `anneal_strategy`.", "question_id": 51841}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, cycle_momentum=True)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `cycle_momentum`.", "question_id": 51842}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, base_momentum=0.85)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `base_momentum`.", "question_id": 51843}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, max_momentum=0.95)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `max_momentum`.", "question_id": 51844}
{"snippet": "torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, div_factor=25.0)", "intent": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy . With arguments `optimizer`, `max_lr`, `div_factor`.", "question_id": 51845}
{"snippet": "one_cycle_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 51846}
{"snippet": "one_cycle_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 51847}
{"snippet": "one_cycle_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 51848}
{"snippet": "one_cycle_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 51849}
{"snippet": "one_cycle_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 51850}
{"snippet": "Tensor.dot(other)", "intent": "See torch.dot ( ) With arguments `other`.", "question_id": 51851}
{"snippet": "torch.nn.LazyBatchNorm1d()", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) .", "question_id": 51852}
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`.", "question_id": 51853}
{"snippet": "torch.nn.LazyBatchNorm1d(momentum=0.1)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `momentum`.", "question_id": 51854}
{"snippet": "torch.nn.LazyBatchNorm1d(affine=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `affine`.", "question_id": 51855}
{"snippet": "torch.nn.LazyBatchNorm1d(track_running_stats=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `track_running_stats`.", "question_id": 51856}
{"snippet": "torch.nn.LazyBatchNorm1d(device=None)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `device`.", "question_id": 51857}
{"snippet": "torch.nn.LazyBatchNorm1d(dtype=None)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `dtype`.", "question_id": 51858}
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, momentum=0.1)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `momentum`.", "question_id": 51859}
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, affine=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `affine`.", "question_id": 51860}
{"snippet": "torch.nn.LazyBatchNorm1d(eps=1e-05, track_running_stats=True)", "intent": "A torch.nn.BatchNorm1d module with lazy initialization of the num_features argument of the BatchNorm1d that is inferred from the input.size ( 1 ) . With arguments `eps`, `track_running_stats`.", "question_id": 51861}
{"snippet": "lazy_batch_norm1d.cls_to_become", "intent": "alias of torch.nn.modules.batchnorm.BatchNorm1d", "question_id": 51862}
{"snippet": "torch.nn.functional.softplus(input)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . With arguments `input`.", "question_id": 51863}
{"snippet": "torch.nn.functional.softplus(input, beta=1)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . With arguments `input`, `beta`.", "question_id": 51864}
{"snippet": "torch.nn.functional.softplus(input, threshold=20)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `input`.", "question_id": 51865}
{"snippet": "torch.nn.functional.softplus(input, beta=1, threshold=20)", "intent": "Applies element-wise , the function Softplus ( x ) =1\u03b2\u2217log\u2061 ( 1+exp\u2061 ( \u03b2\u2217x ) ) \\text { Softplus } ( x ) = \\frac { 1 } { \\beta } * \\log ( 1 + \\exp ( \\beta * x ) ) Softplus ( x ) =\u03b21\u200b\u2217log ( 1+exp ( \u03b2\u2217x ) ) . For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2 > thresholdinput \\times \\beta > thresholdinput\u00d7\u03b2 > `threshold` . With arguments `input`, `beta`.", "question_id": 51866}
{"snippet": "Tensor.ge_(other)", "intent": "In-place version of ge ( ) . With arguments `other`.", "question_id": 51867}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`.", "question_id": 51868}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 51869}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`.", "question_id": 51870}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 51871}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 51872}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 51873}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 51874}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 51875}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh', device=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `device`.", "question_id": 51876}
{"snippet": "torch.nn.RNNCell(input_size, hidden_size, nonlinearity='tanh', dtype=None)", "intent": "An Elman RNN cell with tanh or ReLU non-linearity . If `nonlinearity` is \u2018 relu \u2019 , then ReLU is used in place of tanh . With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 51877}
{"snippet": "torch.quantization.propagate_qconfig_(module)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module", "question_id": 51878}
{"snippet": "torch.quantization.propagate_qconfig_(module, qconfig_dict=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `qconfig_dict`.", "question_id": 51879}
{"snippet": "torch.quantization.propagate_qconfig_(module, allow_list=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `allow_list`.", "question_id": 51880}
{"snippet": "torch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None)", "intent": "Propagate qconfig through the `module` hierarchy and assign qconfig attribute on each leaf module With arguments `qconfig_dict`, `allow_list`.", "question_id": 51881}
{"snippet": "torch.nn.functional.silu(input)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `input`.", "question_id": 51882}
{"snippet": "torch.nn.functional.silu(input, inplace=False)", "intent": "Applies the Sigmoid Linear Unit ( SiLU ) function , element-wise . With arguments `input`, `inplace`.", "question_id": 51883}
{"snippet": "torch.cuda.max_memory_allocated()", "intent": "Returns the maximum GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 51884}
{"snippet": "torch.cuda.max_memory_allocated(device=None)", "intent": "Returns the maximum GPU memory occupied by tensors in bytes for a given `device` .", "question_id": 51885}
{"snippet": "torch.nn.functional.pad(input, pad)", "intent": "Pads tensor . With arguments `input`, `pad`.", "question_id": 51886}
{"snippet": "torch.nn.functional.pad(input, pad, mode='constant')", "intent": "Pads tensor . With arguments `input`, `pad`, `mode`.", "question_id": 51887}
{"snippet": "torch.nn.functional.pad(input, pad, value=0)", "intent": "Pads tensor . With arguments `input`, `pad`, `value`.", "question_id": 51888}
{"snippet": "torch.nn.functional.pad(input, pad, mode='constant', value=0)", "intent": "Pads tensor . With arguments `input`, `pad`, `mode`, `value`.", "question_id": 51889}
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) .", "question_id": 51890}
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . If `batch_first` is True , B x T x * input is expected .", "question_id": 51891}
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, enforce_sorted=True)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 51892}
{"snippet": "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)", "intent": "Packs a Tensor containing padded sequences of variable length . `input` can be of size T x B x * where T is the length of the longest sequence ( equal to `lengths` [ 0 ] ) , B is the batch size , and * is any number of dimensions ( including 0 ) . If `batch_first` is True , B x T x * input is expected . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 51893}
{"snippet": "torch.lt(input, other)", "intent": "Computes `input` < other\\text { input } < \\text { `other` } input < other element-wise .", "question_id": 51894}
{"snippet": "torch.lt(input, other, out=None)", "intent": "Computes `input` < other\\text { input } < \\text { `other` } input < other element-wise . With arguments `out`.", "question_id": 51895}
{"snippet": "torch.cuda.memory_reserved()", "intent": "Returns the current GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 51896}
{"snippet": "torch.cuda.memory_reserved(device=None)", "intent": "Returns the current GPU memory managed by the caching allocator in bytes for a given `device` .", "question_id": 51897}
{"snippet": "Tensor.fmod_(divisor)", "intent": "In-place version of fmod ( ) With arguments `divisor`.", "question_id": 51898}
{"snippet": "torch.frac(input)", "intent": "Computes the fractional portion of each element in `input` .", "question_id": 51899}
{"snippet": "torch.frac(input, out=None)", "intent": "Computes the fractional portion of each element in `input` . With arguments `out`.", "question_id": 51900}
{"snippet": "torch.mm(input, mat2)", "intent": "Performs a matrix multiplication of the matrices `input` and `mat2` .", "question_id": 51901}
{"snippet": "torch.mm(input, mat2, out=None)", "intent": "Performs a matrix multiplication of the matrices `input` and `mat2` . If input is a ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) tensor , mat2 is a ( m\u00d7p ) ( m \\times p ) ( m\u00d7p ) tensor , `out` will be a ( n\u00d7p ) ( n \\times p ) ( n\u00d7p ) tensor .", "question_id": 51902}
{"snippet": "Tensor.erf()", "intent": "See torch.erf ( )", "question_id": 51903}
{"snippet": "Tensor.sin_()", "intent": "In-place version of sin ( )", "question_id": 51904}
{"snippet": "torch.nn.Unflatten(dim, unflattened_size)", "intent": "Unflattens a tensor `dim` expanding it to a desired shape . With arguments `unflattened_size`.", "question_id": 51905}
{"snippet": "torch.nn.qat.Linear(in_features, out_features)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`.", "question_id": 51906}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`.", "question_id": 51907}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`.", "question_id": 51908}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `device`.", "question_id": 51909}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 51910}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `qconfig`.", "question_id": 51911}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `device`.", "question_id": 51912}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, bias=True, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 51913}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None, device=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`, `device`.", "question_id": 51914}
{"snippet": "torch.nn.qat.Linear(in_features, out_features, qconfig=None, dtype=None)", "intent": "A linear module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_features`, `out_features`, `qconfig`, `dtype`.", "question_id": 51915}
{"snippet": "linear.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 51916}
{"snippet": "torch.jit.ScriptFunction", "intent": "Functionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters.", "question_id": 51917}
{"snippet": "torch.lu(*args, **kwargs)", "intent": "Computes the LU factorization of a matrix or batches of matrices A . With arguments `*args`, `**kwargs`.", "question_id": 51918}
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`.", "question_id": 51919}
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `bias`.", "question_id": 51920}
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, qconfig=None)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `qconfig`.", "question_id": 51921}
{"snippet": "torch.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None)", "intent": "A LinearReLU module fused from Linear and ReLU modules , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_features`, `out_features`, `bias`, `qconfig`.", "question_id": 51922}
{"snippet": "torch.trapz(y, x)", "intent": "Estimate \u222by dx\\int y\\ , dx\u222bydx along `dim` , using the trapezoid rule . With arguments `y`, `x`.", "question_id": 51923}
{"snippet": "torch.trapz(y, x, dim=- 1)", "intent": "Estimate \u222by dx\\int y\\ , dx\u222bydx along `dim` , using the trapezoid rule . With arguments `y`, `x`.", "question_id": 51924}
{"snippet": "torch.nn.intrinsic.ConvBnReLU1d(conv, bn, relu)", "intent": "This is a sequential container which calls the Conv 1d , Batch Norm 1d , and ReLU modules . With arguments `conv`, `bn`, `relu`.", "question_id": 51925}
{"snippet": "torch.fft.fftn(input)", "intent": "Computes the N dimensional discrete Fourier transform of `input` .", "question_id": 51926}
{"snippet": "torch.fft.fftn(input, s=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`.", "question_id": 51927}
{"snippet": "torch.fft.fftn(input, dim=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 51928}
{"snippet": "torch.fft.fftn(input, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 51929}
{"snippet": "torch.fft.fftn(input, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `out`.", "question_id": 51930}
{"snippet": "torch.fft.fftn(input, s=None, dim=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `dim`.", "question_id": 51931}
{"snippet": "torch.fft.fftn(input, s=None, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `norm`.", "question_id": 51932}
{"snippet": "torch.fft.fftn(input, s=None, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `s`, `out`.", "question_id": 51933}
{"snippet": "torch.fft.fftn(input, dim=None, norm=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 51934}
{"snippet": "torch.fft.fftn(input, dim=None, out=None)", "intent": "Computes the N dimensional discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 51935}
{"snippet": "torch.is_complex(input)", "intent": "Returns True if the data type of `input` is a complex data type i.e. , one of torch.complex64 , and torch.complex128 .", "question_id": 51936}
{"snippet": "torch.blackman_window(window_length)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 51937}
{"snippet": "torch.blackman_window(window_length, periodic=True)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 51938}
{"snippet": "torch.blackman_window(window_length, dtype=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 51939}
{"snippet": "torch.blackman_window(window_length, layout=torch.strided)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 51940}
{"snippet": "torch.blackman_window(window_length, device=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 51941}
{"snippet": "torch.blackman_window(window_length, requires_grad=False)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 51942}
{"snippet": "torch.blackman_window(window_length, periodic=True, dtype=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `dtype`.", "question_id": 51943}
{"snippet": "torch.blackman_window(window_length, periodic=True, layout=torch.strided)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `layout`.", "question_id": 51944}
{"snippet": "torch.blackman_window(window_length, periodic=True, device=None)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `device`.", "question_id": 51945}
{"snippet": "torch.blackman_window(window_length, periodic=True, requires_grad=False)", "intent": "Blackman window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `requires_grad`.", "question_id": 51946}
{"snippet": "Tensor.data_ptr()", "intent": "Returns the address of the first element of self tensor .", "question_id": 51947}
{"snippet": "Tensor.isclose(other)", "intent": "See torch.isclose ( ) With arguments `other`.", "question_id": 51948}
{"snippet": "Tensor.isclose(other, rtol=1e-05)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`.", "question_id": 51949}
{"snippet": "Tensor.isclose(other, atol=1e-08)", "intent": "See torch.isclose ( ) With arguments `other`, `atol`.", "question_id": 51950}
{"snippet": "Tensor.isclose(other, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `equal_nan`.", "question_id": 51951}
{"snippet": "Tensor.isclose(other, rtol=1e-05, atol=1e-08)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `atol`.", "question_id": 51952}
{"snippet": "Tensor.isclose(other, rtol=1e-05, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `equal_nan`.", "question_id": 51953}
{"snippet": "Tensor.isclose(other, atol=1e-08, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `atol`, `equal_nan`.", "question_id": 51954}
{"snippet": "Tensor.isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False)", "intent": "See torch.isclose ( ) With arguments `other`, `rtol`, `atol`, `equal_nan`.", "question_id": 51955}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`.", "question_id": 51956}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`.", "question_id": 51957}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, margin=1.0)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `margin`.", "question_id": 51958}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, weight=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `weight`.", "question_id": 51959}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, size_average=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 51960}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, reduce=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 51961}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, reduction='mean')", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 51962}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `margin`.", "question_id": 51963}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, weight=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `weight`.", "question_id": 51964}
{"snippet": "torch.nn.functional.multi_margin_loss(input, target, p=1, size_average=None)", "intent": "See MultiMarginLoss for details . With arguments `input`, `target`, `p`, `size_average`.", "question_id": 51965}
{"snippet": "Tensor.gt(other)", "intent": "See torch.gt ( ) . With arguments `other`.", "question_id": 51966}
{"snippet": "torch.randperm(n)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 .", "question_id": 51967}
{"snippet": "torch.randperm(n, generator=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`.", "question_id": 51968}
{"snippet": "torch.randperm(n, out=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `out`.", "question_id": 51969}
{"snippet": "torch.randperm(n, dtype=torch.int64)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `dtype`.", "question_id": 51970}
{"snippet": "torch.randperm(n, layout=torch.strided)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `layout`.", "question_id": 51971}
{"snippet": "torch.randperm(n, device=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `device`.", "question_id": 51972}
{"snippet": "torch.randperm(n, requires_grad=False)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `requires_grad`.", "question_id": 51973}
{"snippet": "torch.randperm(n, pin_memory=False)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `pin_memory`.", "question_id": 51974}
{"snippet": "torch.randperm(n, generator=None, out=None)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`, `out`.", "question_id": 51975}
{"snippet": "torch.randperm(n, generator=None, dtype=torch.int64)", "intent": "Returns a random permutation of integers from 0 to `n` - 1 . With arguments `generator`, `dtype`.", "question_id": 51976}
{"snippet": "torch.amax(input, dim)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` .", "question_id": 51977}
{"snippet": "torch.amax(input, dim, keepdim=False)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is `` True ` , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 .", "question_id": 51978}
{"snippet": "torch.amax(input, dim, out=None)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . With arguments `out`.", "question_id": 51979}
{"snippet": "torch.amax(input, dim, keepdim=False, out=None)", "intent": "Returns the maximum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is `` True ` , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 . With arguments `out`.", "question_id": 51980}
{"snippet": "torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs)", "intent": "A long short-term memory ( LSTM ) cell . With arguments `*args`, `**kwargs`.", "question_id": 51981}
{"snippet": "torch.moveaxis(input, source, destination)", "intent": "Alias for torch.movedim ( ) . With arguments `input`, `source`, `destination`.", "question_id": 51982}
{"snippet": "torch.conj(input)", "intent": "Computes the element-wise conjugate of the given `input` tensor .", "question_id": 51983}
{"snippet": "torch.conj(input, out=None)", "intent": "Computes the element-wise conjugate of the given `input` tensor . With arguments `out`.", "question_id": 51984}
{"snippet": "torch.quantization.prepare(model)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training .", "question_id": 51985}
{"snippet": "torch.quantization.prepare(model, inplace=False)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`.", "question_id": 51986}
{"snippet": "torch.quantization.prepare(model, allow_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`.", "question_id": 51987}
{"snippet": "torch.quantization.prepare(model, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `observer_non_leaf_module_list`.", "question_id": 51988}
{"snippet": "torch.quantization.prepare(model, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `prepare_custom_config_dict`.", "question_id": 51989}
{"snippet": "torch.quantization.prepare(model, inplace=False, allow_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `allow_list`.", "question_id": 51990}
{"snippet": "torch.quantization.prepare(model, inplace=False, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `observer_non_leaf_module_list`.", "question_id": 51991}
{"snippet": "torch.quantization.prepare(model, inplace=False, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `inplace`, `prepare_custom_config_dict`.", "question_id": 51992}
{"snippet": "torch.quantization.prepare(model, allow_list=None, observer_non_leaf_module_list=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`, `observer_non_leaf_module_list`.", "question_id": 51993}
{"snippet": "torch.quantization.prepare(model, allow_list=None, prepare_custom_config_dict=None)", "intent": "Prepares a copy of the `model` for quantization calibration or quantization-aware training . With arguments `allow_list`, `prepare_custom_config_dict`.", "question_id": 51994}
{"snippet": "torch.cuda.current_device()", "intent": "Returns the index of a currently selected device .", "question_id": 51995}
{"snippet": "Tensor.tril_()", "intent": "In-place version of tril ( )", "question_id": 51996}
{"snippet": "Tensor.tril_(k=0)", "intent": "In-place version of tril ( ) With arguments `k`.", "question_id": 51997}
{"snippet": "torch.no_grad", "intent": "Context-manager that disabled gradient calculation.", "question_id": 51998}
{"snippet": "Tensor.logical_not_()", "intent": "In-place version of logical_not ( )", "question_id": 51999}
{"snippet": "torch.arcsinh(input)", "intent": "Alias for torch.asinh ( ) . With arguments `input`.", "question_id": 52000}
{"snippet": "torch.arcsinh(input, out=None)", "intent": "Alias for torch.asinh ( ) . With arguments `input`, `out`.", "question_id": 52001}
{"snippet": "torch.atleast_3d(*tensors)", "intent": "Returns a 3-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 52002}
{"snippet": "profile.key_averages()", "intent": "Averages all function events over their keys .", "question_id": 52003}
{"snippet": "profile.key_averages(group_by_input_shape=False)", "intent": "Averages all function events over their keys . With arguments `group_by_input_shape`.", "question_id": 52004}
{"snippet": "profile.key_averages(group_by_stack_n=0)", "intent": "Averages all function events over their keys . With arguments `group_by_stack_n`.", "question_id": 52005}
{"snippet": "profile.key_averages(group_by_input_shape=False, group_by_stack_n=0)", "intent": "Averages all function events over their keys . With arguments `group_by_input_shape`, `group_by_stack_n`.", "question_id": 52006}
{"snippet": "torch.nn.quantized.FloatFunctional", "intent": "State collector class for float operations.", "question_id": 52007}
{"snippet": "torch.linalg.cond(A)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm .", "question_id": 52008}
{"snippet": "torch.linalg.cond(A, p=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . `p` defines the matrix norm that is computed .", "question_id": 52009}
{"snippet": "torch.linalg.cond(A, out=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . With arguments `out`.", "question_id": 52010}
{"snippet": "torch.linalg.cond(A, p=None, out=None)", "intent": "Computes the condition number of a matrix with respect to a matrix norm . The condition number of `A` measures the numerical stability of the linear system AX = B with respect to a matrix norm . `p` defines the matrix norm that is computed . With arguments `out`.", "question_id": 52011}
{"snippet": "torch.min(input)", "intent": "Returns the minimum value of all elements in the `input` tensor .", "question_id": 52012}
{"snippet": "Tensor.sign()", "intent": "See torch.sign ( )", "question_id": 52013}
{"snippet": "torch.cuda.get_rng_state()", "intent": "Returns the random number generator state of the specified GPU as a ByteTensor .", "question_id": 52014}
{"snippet": "torch.cuda.get_rng_state(device='cuda')", "intent": "Returns the random number generator state of the specified GPU as a ByteTensor . With arguments `device`.", "question_id": 52015}
{"snippet": "Tensor.to_dense()", "intent": "Creates a strided copy of self .", "question_id": 52016}
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`.", "question_id": 52017}
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `bias`.", "question_id": 52018}
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, dtype=torch.qint8)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `dtype`.", "question_id": 52019}
{"snippet": "torch.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8)", "intent": "A LinearReLU module fused from Linear and ReLU modules With arguments `in_features`, `out_features`, `bias`, `dtype`.", "question_id": 52020}
{"snippet": "torch.matrix_exp(input)", "intent": "Computes the matrix exponential of a square matrix or of each square matrix in a batch . For a matrix `input` , the matrix exponential is defined as", "question_id": 52021}
{"snippet": "Tensor.fmax(other)", "intent": "See torch.fmax ( ) With arguments `other`.", "question_id": 52022}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52023}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52024}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52025}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52026}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52027}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52028}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52029}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, qconfig=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `qconfig`.", "question_id": 52030}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, device=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 52031}
{"snippet": "torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "A Conv2d module attached with FakeQuantize modules for weight , used for quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 52032}
{"snippet": "conv2d.from_float(mod)", "intent": "Create a qat module from a float module or qparams_dict Args : `mod` a float module , either produced by torch.quantization utilities or directly from user", "question_id": 52033}
{"snippet": "torch.full(size, fill_value)", "intent": "Creates a tensor of `size` size filled with `fill_value` .", "question_id": 52034}
{"snippet": "torch.full(size, fill_value, out=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`.", "question_id": 52035}
{"snippet": "torch.full(size, fill_value, dtype=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . The tensor \u2019 s `dtype` is inferred from fill_value .", "question_id": 52036}
{"snippet": "torch.full(size, fill_value, layout=torch.strided)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `layout`.", "question_id": 52037}
{"snippet": "torch.full(size, fill_value, device=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `device`.", "question_id": 52038}
{"snippet": "torch.full(size, fill_value, requires_grad=False)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `requires_grad`.", "question_id": 52039}
{"snippet": "torch.full(size, fill_value, out=None, dtype=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . The tensor \u2019 s `dtype` is inferred from fill_value . With arguments `out`.", "question_id": 52040}
{"snippet": "torch.full(size, fill_value, out=None, layout=torch.strided)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `layout`.", "question_id": 52041}
{"snippet": "torch.full(size, fill_value, out=None, device=None)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `device`.", "question_id": 52042}
{"snippet": "torch.full(size, fill_value, out=None, requires_grad=False)", "intent": "Creates a tensor of `size` size filled with `fill_value` . With arguments `out`, `requires_grad`.", "question_id": 52043}
{"snippet": "Tensor.ne(other)", "intent": "See torch.ne ( ) . With arguments `other`.", "question_id": 52044}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52045}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52046}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52047}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 52048}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52049}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52050}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52051}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52052}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 52053}
{"snippet": "torch.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 52054}
{"snippet": "torch.nn.functional.glu(input)", "intent": "The gated linear unit . where `input` is split in half along `dim` to form a and b , \u03c3\\sigma\u03c3 is the sigmoid function and \u2297\\otimes\u2297 is the element-wise product between matrices .", "question_id": 52055}
{"snippet": "torch.nn.functional.glu(input, dim=- 1)", "intent": "The gated linear unit . where `input` is split in half along `dim` to form a and b , \u03c3\\sigma\u03c3 is the sigmoid function and \u2297\\otimes\u2297 is the element-wise product between matrices .", "question_id": 52056}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`.", "question_id": 52057}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 52058}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduce=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduce`.", "question_id": 52059}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduction`.", "question_id": 52060}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None)", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 52061}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 52062}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, reduce=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 52063}
{"snippet": "torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "See MultiLabelMarginLoss for details . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 52064}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`.", "question_id": 52065}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`.", "question_id": 52066}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `padding`.", "question_id": 52067}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `output_size`.", "question_id": 52068}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`.", "question_id": 52069}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `output_size`.", "question_id": 52070}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `padding`, `output_size`.", "question_id": 52071}
{"snippet": "torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)", "intent": "Computes a partial inverse of MaxPool1d . With arguments `input`, `indices`, `kernel_size`, `stride`, `padding`, `output_size`.", "question_id": 52072}
{"snippet": "torch.cummin(input, dim)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative minimum of elements of `input` in the dimension `dim` .", "question_id": 52073}
{"snippet": "torch.cummin(input, dim, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the cumulative minimum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 52074}
{"snippet": "torch.nn.functional.softmax(input)", "intent": "Applies a softmax function . With arguments `input`.", "question_id": 52075}
{"snippet": "torch.nn.functional.softmax(input, dim=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`.", "question_id": 52076}
{"snippet": "torch.nn.functional.softmax(input, _stacklevel=3)", "intent": "Applies a softmax function . With arguments `input`, `_stacklevel`.", "question_id": 52077}
{"snippet": "torch.nn.functional.softmax(input, dtype=None)", "intent": "Applies a softmax function . With arguments `input`, `dtype`.", "question_id": 52078}
{"snippet": "torch.nn.functional.softmax(input, dim=None, _stacklevel=3)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `_stacklevel`.", "question_id": 52079}
{"snippet": "torch.nn.functional.softmax(input, dim=None, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `dtype`.", "question_id": 52080}
{"snippet": "torch.nn.functional.softmax(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmax function . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 52081}
{"snippet": "torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmax function . It is applied to all slices along `dim` , and will re-scale them so that the elements lie in the range [ 0 , 1 ] and sum to 1 . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 52082}
{"snippet": "Tensor.cummax(dim)", "intent": "See torch.cummax ( ) With arguments `dim`.", "question_id": 52083}
{"snippet": "torch.set_grad_enabled(mode)", "intent": "Context-manager that sets gradient calculation to on or off . set_grad_enabled will enable or disable grads based on its argument `mode` .", "question_id": 52084}
{"snippet": "torch.get_rng_state()", "intent": "Returns the random number generator state as a torch.ByteTensor .", "question_id": 52085}
{"snippet": "Tensor.nextafter(other)", "intent": "See torch.nextafter ( ) With arguments `other`.", "question_id": 52086}
{"snippet": "Tensor.sigmoid()", "intent": "See torch.sigmoid ( )", "question_id": 52087}
{"snippet": "torch.quantization.quantize_qat(model, run_fn, run_args)", "intent": "Do quantization aware training and output a quantized `model` With arguments `run_fn`, `run_args`.", "question_id": 52088}
{"snippet": "torch.quantization.quantize_qat(model, run_fn, run_args, inplace=False)", "intent": "Do quantization aware training and output a quantized `model` With arguments `run_fn`, `run_args`, `inplace`.", "question_id": 52089}
{"snippet": "torch.quantization.observer.default_placeholder_observer", "intent": "alias of torch.quantization.observer.PlaceholderObserver", "question_id": 52090}
{"snippet": "torch.sqrt(input)", "intent": "Returns a new tensor with the square-root of the elements of `input` .", "question_id": 52091}
{"snippet": "torch.sqrt(input, out=None)", "intent": "Returns a new tensor with the square-root of the elements of `input` . With arguments `out`.", "question_id": 52092}
{"snippet": "Tensor.scatter(dim, index, src)", "intent": "Out-of-place version of torch.Tensor.scatter_ ( ) With arguments `dim`, `index`, `src`.", "question_id": 52093}
{"snippet": "torch.quantization.observer.PlaceholderObserver()", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) .", "question_id": 52094}
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`.", "question_id": 52095}
{"snippet": "torch.quantization.observer.PlaceholderObserver(custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`.", "question_id": 52096}
{"snippet": "torch.quantization.observer.PlaceholderObserver(compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `compute_dtype`.", "question_id": 52097}
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`.", "question_id": 52098}
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `compute_dtype`.", "question_id": 52099}
{"snippet": "torch.quantization.observer.PlaceholderObserver(custom_op_name='', compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`, `compute_dtype`.", "question_id": 52100}
{"snippet": "torch.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='', compute_dtype=None)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`, `compute_dtype`.", "question_id": 52101}
{"snippet": "Tensor.tanh_()", "intent": "In-place version of tanh ( )", "question_id": 52102}
{"snippet": "torch.triu_indices(row, col)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates .", "question_id": 52103}
{"snippet": "torch.triu_indices(row, col, offset=0)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider .", "question_id": 52104}
{"snippet": "torch.triu_indices(row, col, dtype=torch.long)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`.", "question_id": 52105}
{"snippet": "torch.triu_indices(row, col, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `device`.", "question_id": 52106}
{"snippet": "torch.triu_indices(row, col, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `layout`.", "question_id": 52107}
{"snippet": "torch.triu_indices(row, col, offset=0, dtype=torch.long)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `dtype`.", "question_id": 52108}
{"snippet": "torch.triu_indices(row, col, offset=0, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `device`.", "question_id": 52109}
{"snippet": "torch.triu_indices(row, col, offset=0, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . The argument `offset` controls which diagonal to consider . With arguments `layout`.", "question_id": 52110}
{"snippet": "torch.triu_indices(row, col, dtype=torch.long, device='cpu')", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `device`.", "question_id": 52111}
{"snippet": "torch.triu_indices(row, col, dtype=torch.long, layout=torch.strided)", "intent": "Returns the indices of the upper triangular part of a `row` by `col` matrix in a 2-by-N Tensor , where the first row contains row coordinates of all indices and the second row contains column coordinates . With arguments `dtype`, `layout`.", "question_id": 52112}
{"snippet": "Tensor.arccos_()", "intent": "In-place version of arccos ( )", "question_id": 52113}
{"snippet": "torch.lcm(input, other)", "intent": "Computes the element-wise least common multiple ( LCM ) of `input` and `other` .", "question_id": 52114}
{"snippet": "torch.lcm(input, other, out=None)", "intent": "Computes the element-wise least common multiple ( LCM ) of `input` and `other` . With arguments `out`.", "question_id": 52115}
{"snippet": "torch.fft.irfft(input)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) .", "question_id": 52116}
{"snippet": "torch.fft.irfft(input, n=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` :", "question_id": 52117}
{"snippet": "torch.fft.irfft(input, dim=- 1)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`.", "question_id": 52118}
{"snippet": "torch.fft.irfft(input, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `norm`.", "question_id": 52119}
{"snippet": "torch.fft.irfft(input, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `out`.", "question_id": 52120}
{"snippet": "torch.fft.irfft(input, n=None, dim=- 1)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `dim`.", "question_id": 52121}
{"snippet": "torch.fft.irfft(input, n=None, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `norm`.", "question_id": 52122}
{"snippet": "torch.fft.irfft(input, n=None, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . So , it is recommended to always pass the signal length `n` : With arguments `out`.", "question_id": 52123}
{"snippet": "torch.fft.irfft(input, dim=- 1, norm=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`, `norm`.", "question_id": 52124}
{"snippet": "torch.fft.irfft(input, dim=- 1, out=None)", "intent": "Computes the inverse of rfft ( ) . `input` is interpreted as a one-sided Hermitian signal in the Fourier domain , as produced by rfft ( ) . With arguments `dim`, `out`.", "question_id": 52125}
{"snippet": "Tensor.symeig()", "intent": "See torch.symeig ( )", "question_id": 52126}
{"snippet": "Tensor.symeig(eigenvectors=False)", "intent": "See torch.symeig ( ) With arguments `eigenvectors`.", "question_id": 52127}
{"snippet": "Tensor.symeig(upper=True)", "intent": "See torch.symeig ( ) With arguments `upper`.", "question_id": 52128}
{"snippet": "Tensor.symeig(eigenvectors=False, upper=True)", "intent": "See torch.symeig ( ) With arguments `eigenvectors`, `upper`.", "question_id": 52129}
{"snippet": "torch.igamma(input, other)", "intent": "Computes the regularized lower incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive .", "question_id": 52130}
{"snippet": "torch.igamma(input, other, out=None)", "intent": "Computes the regularized lower incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive . If both are zero or either is negative then outi=nan\\text { `out` } _i=\\text { nan } outi\u200b=nan .", "question_id": 52131}
{"snippet": "torch.lerp(input, end, weight)", "intent": "Does a linear interpolation of two tensors start ( given by `input` ) and `end` based on a scalar or tensor `weight` and returns the resulting `out` tensor .", "question_id": 52132}
{"snippet": "torch.lerp(input, end, weight, out=None)", "intent": "Does a linear interpolation of two tensors start ( given by `input` ) and `end` based on a scalar or tensor `weight` and returns the resulting `out` tensor .", "question_id": 52133}
{"snippet": "Tensor.new_full(size, fill_value)", "intent": "Returns a Tensor of `size` size filled with `fill_value` .", "question_id": 52134}
{"snippet": "Tensor.new_full(size, fill_value, dtype=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`.", "question_id": 52135}
{"snippet": "Tensor.new_full(size, fill_value, device=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `device`.", "question_id": 52136}
{"snippet": "Tensor.new_full(size, fill_value, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `requires_grad`.", "question_id": 52137}
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `device`.", "question_id": 52138}
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `requires_grad`.", "question_id": 52139}
{"snippet": "Tensor.new_full(size, fill_value, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `device`, `requires_grad`.", "question_id": 52140}
{"snippet": "Tensor.new_full(size, fill_value, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with `fill_value` . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 52141}
{"snippet": "torch.nn.AdaptiveAvgPool1d(output_size)", "intent": "Applies a 1D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 52142}
{"snippet": "Tensor.index_put_(indices, values)", "intent": "Puts `values` from the tensor values into the tensor self using the `indices` specified in indices ( which is a tuple of Tensors ) .", "question_id": 52143}
{"snippet": "Tensor.index_put_(indices, values, accumulate=False)", "intent": "Puts `values` from the tensor values into the tensor self using the `indices` specified in indices ( which is a tuple of Tensors ) . If `accumulate` is True , the elements in values are added to self .", "question_id": 52144}
{"snippet": "Tensor.transpose_(dim0, dim1)", "intent": "In-place version of transpose ( ) With arguments `dim0`, `dim1`.", "question_id": 52145}
{"snippet": "Tensor.is_meta", "intent": "Is True if the Tensor is a meta tensor, False otherwise.", "question_id": 52146}
{"snippet": "torch.movedim(input, source, destination)", "intent": "Moves the dimension ( s ) of `input` at the position ( s ) in `source` to the position ( s ) in `destination` .", "question_id": 52147}
{"snippet": "torch.hsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with one or more dimensions , into multiple tensors horizontally according to `indices_or_sections` .", "question_id": 52148}
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`.", "question_id": 52149}
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `weight`.", "question_id": 52150}
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, size_average=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `size_average`.", "question_id": 52151}
{"snippet": "torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None)", "intent": "See MultiLabelSoftMarginLoss for details . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 52152}
{"snippet": "torch.nn.functional.interpolate(input)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52153}
{"snippet": "torch.nn.functional.interpolate(input, size=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52154}
{"snippet": "torch.nn.functional.interpolate(input, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52155}
{"snippet": "torch.nn.functional.interpolate(input, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` The algorithm used for interpolation is determined by `mode` .", "question_id": 52156}
{"snippet": "torch.nn.functional.interpolate(input, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 52157}
{"snippet": "torch.nn.functional.interpolate(input, recompute_scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `recompute_scale_factor`.", "question_id": 52158}
{"snippet": "torch.nn.functional.interpolate(input, size=None, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52159}
{"snippet": "torch.nn.functional.interpolate(input, size=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` The algorithm used for interpolation is determined by `mode` .", "question_id": 52160}
{"snippet": "torch.nn.functional.interpolate(input, size=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 52161}
{"snippet": "torch.nn.functional.interpolate(input, size=None, recompute_scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `recompute_scale_factor`.", "question_id": 52162}
{"snippet": "Tensor.contiguous()", "intent": "Returns a contiguous in memory tensor containing the same data as self tensor .", "question_id": 52163}
{"snippet": "Tensor.contiguous(memory_format=torch.contiguous_format)", "intent": "Returns a contiguous in memory tensor containing the same data as self tensor . With arguments `memory_format`.", "question_id": 52164}
{"snippet": "torch.complex(real, imag)", "intent": "Constructs a complex tensor with its `real` part equal to real and its imaginary part equal to `imag` .", "question_id": 52165}
{"snippet": "torch.complex(real, imag, out=None)", "intent": "Constructs a complex tensor with its `real` part equal to real and its imaginary part equal to `imag` . With arguments `out`.", "question_id": 52166}
{"snippet": "torch.nn.functional.mse_loss(input, target)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`.", "question_id": 52167}
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`.", "question_id": 52168}
{"snippet": "torch.nn.functional.mse_loss(input, target, reduce=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduce`.", "question_id": 52169}
{"snippet": "torch.nn.functional.mse_loss(input, target, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduction`.", "question_id": 52170}
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None)", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduce`.", "question_id": 52171}
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduction`.", "question_id": 52172}
{"snippet": "torch.nn.functional.mse_loss(input, target, reduce=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `reduce`, `reduction`.", "question_id": 52173}
{"snippet": "torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean')", "intent": "Measures the element-wise mean squared error . With arguments `input`, `target`, `size_average`, `reduce`, `reduction`.", "question_id": 52174}
{"snippet": "torch.nn.Softmax2d", "intent": "Applies SoftMax over features to each spatial location.", "question_id": 52175}
{"snippet": "Tensor.mvlgamma_(p)", "intent": "In-place version of mvlgamma ( ) With arguments `p`.", "question_id": 52176}
{"snippet": "Tensor.ormqr(input2, input3)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`.", "question_id": 52177}
{"snippet": "Tensor.ormqr(input2, input3, left=True)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `left`.", "question_id": 52178}
{"snippet": "Tensor.ormqr(input2, input3, transpose=False)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `transpose`.", "question_id": 52179}
{"snippet": "Tensor.ormqr(input2, input3, left=True, transpose=False)", "intent": "See torch.ormqr ( ) With arguments `input2`, `input3`, `left`, `transpose`.", "question_id": 52180}
{"snippet": "torch.isreal(input)", "intent": "Returns a new tensor with boolean elements representing if each element of `input` is real-valued or not .", "question_id": 52181}
{"snippet": "torch.nn.ConstantPad3d(padding, value)", "intent": "Pads the input tensor boundaries with a constant `value` . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 52182}
{"snippet": "Tensor.nextafter_(other)", "intent": "In-place version of nextafter ( ) With arguments `other`.", "question_id": 52183}
{"snippet": "Tensor.any()", "intent": "See torch.any ( )", "question_id": 52184}
{"snippet": "Tensor.any(dim=None)", "intent": "See torch.any ( ) With arguments `dim`.", "question_id": 52185}
{"snippet": "Tensor.any(keepdim=False)", "intent": "See torch.any ( ) With arguments `keepdim`.", "question_id": 52186}
{"snippet": "Tensor.any(dim=None, keepdim=False)", "intent": "See torch.any ( ) With arguments `dim`, `keepdim`.", "question_id": 52187}
{"snippet": "Tensor.sin()", "intent": "See torch.sin ( )", "question_id": 52188}
{"snippet": "torch.nn.quantized.functional.hardswish(input, scale, zero_point)", "intent": "This is the quantized version of hardswish ( ) . With arguments `input`, `scale`, `zero_point`.", "question_id": 52189}
{"snippet": "torch.fft.rfft(input)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` .", "question_id": 52190}
{"snippet": "torch.fft.rfft(input, n=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`.", "question_id": 52191}
{"snippet": "torch.fft.rfft(input, dim=- 1)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`.", "question_id": 52192}
{"snippet": "torch.fft.rfft(input, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `norm`.", "question_id": 52193}
{"snippet": "torch.fft.rfft(input, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `out`.", "question_id": 52194}
{"snippet": "torch.fft.rfft(input, n=None, dim=- 1)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `dim`.", "question_id": 52195}
{"snippet": "torch.fft.rfft(input, n=None, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `norm`.", "question_id": 52196}
{"snippet": "torch.fft.rfft(input, n=None, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `n`, `out`.", "question_id": 52197}
{"snippet": "torch.fft.rfft(input, dim=- 1, norm=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`, `norm`.", "question_id": 52198}
{"snippet": "torch.fft.rfft(input, dim=- 1, out=None)", "intent": "Computes the one dimensional Fourier transform of real-valued `input` . With arguments `dim`, `out`.", "question_id": 52199}
{"snippet": "Tensor.ceil_()", "intent": "In-place version of ceil ( )", "question_id": 52200}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`.", "question_id": 52201}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`.", "question_id": 52202}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, stride=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `stride`.", "question_id": 52203}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `padding`.", "question_id": 52204}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `output_padding`.", "question_id": 52205}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, groups=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `groups`.", "question_id": 52206}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `dilation`.", "question_id": 52207}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `stride`.", "question_id": 52208}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `padding`.", "question_id": 52209}
{"snippet": "torch.nn.functional.conv_transpose2d(input, weight, bias=None, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an `input` image composed of several input planes , sometimes also called \u201c deconvolution \u201d . With arguments `weight`, `bias`, `output_padding`.", "question_id": 52210}
{"snippet": "Tensor.arccosh_()", "intent": "acosh_ ( ) - > Tensor", "question_id": 52211}
{"snippet": "torch.ge(input, other)", "intent": "Computes input\u2265other\\text { `input` } \\geq \\text { `other` } input\u2265other element-wise .", "question_id": 52212}
{"snippet": "torch.ge(input, other, out=None)", "intent": "Computes input\u2265other\\text { `input` } \\geq \\text { `other` } input\u2265other element-wise . With arguments `out`.", "question_id": 52213}
{"snippet": "torch.autograd.functional.jvp(func, inputs)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`.", "question_id": 52214}
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`.", "question_id": 52215}
{"snippet": "torch.autograd.functional.jvp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`.", "question_id": 52216}
{"snippet": "torch.autograd.functional.jvp(func, inputs, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `strict`.", "question_id": 52217}
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`.", "question_id": 52218}
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `strict`.", "question_id": 52219}
{"snippet": "torch.autograd.functional.jvp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`, `strict`.", "question_id": 52220}
{"snippet": "torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between the Jacobian of the given function at the point given by the `inputs` and a vector `v` . With arguments `func`, `create_graph`, `strict`.", "question_id": 52221}
{"snippet": "torch.nn.modules.module.register_module_forward_hook(hook)", "intent": "Registers a global forward `hook` for all the modules", "question_id": 52222}
{"snippet": "torch.is_nonzero(input)", "intent": "Returns True if the `input` is a single element tensor which is not equal to zero after type conversions .", "question_id": 52223}
{"snippet": "torch.hspmm(mat1, mat2)", "intent": "Performs a matrix multiplication of a sparse COO matrix `mat1` and a strided matrix `mat2` .", "question_id": 52224}
{"snippet": "torch.hspmm(mat1, mat2, out=None)", "intent": "Performs a matrix multiplication of a sparse COO matrix `mat1` and a strided matrix `mat2` . With arguments `out`.", "question_id": 52225}
{"snippet": "Tensor.histc()", "intent": "See torch.histc ( )", "question_id": 52226}
{"snippet": "Tensor.histc(bins=100)", "intent": "See torch.histc ( ) With arguments `bins`.", "question_id": 52227}
{"snippet": "Tensor.histc(min=0)", "intent": "See torch.histc ( ) With arguments `min`.", "question_id": 52228}
{"snippet": "Tensor.histc(max=0)", "intent": "See torch.histc ( ) With arguments `max`.", "question_id": 52229}
{"snippet": "Tensor.histc(bins=100, min=0)", "intent": "See torch.histc ( ) With arguments `bins`, `min`.", "question_id": 52230}
{"snippet": "Tensor.histc(bins=100, max=0)", "intent": "See torch.histc ( ) With arguments `bins`, `max`.", "question_id": 52231}
{"snippet": "Tensor.histc(min=0, max=0)", "intent": "See torch.histc ( ) With arguments `min`, `max`.", "question_id": 52232}
{"snippet": "Tensor.histc(bins=100, min=0, max=0)", "intent": "See torch.histc ( ) With arguments `bins`, `min`, `max`.", "question_id": 52233}
{"snippet": "torch.nn.functional.linear(input, weight)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`.", "question_id": 52234}
{"snippet": "torch.nn.functional.linear(input, weight, bias=None)", "intent": "Applies a linear transformation to the incoming data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`.", "question_id": 52235}
{"snippet": "Tensor.ldexp(other)", "intent": "See torch.ldexp ( ) With arguments `other`.", "question_id": 52236}
{"snippet": "torch.var(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`.", "question_id": 52237}
{"snippet": "torch.var(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 52238}
{"snippet": "torch.var(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 52239}
{"snippet": "torch.var(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the variance of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 52240}
{"snippet": "torch.nn.utils.prune.L1Unstructured(amount)", "intent": "Prune ( currently unpruned ) units in a tensor by zeroing out the ones with the lowest L1-norm . With arguments `amount`.", "question_id": 52241}
{"snippet": "l1_unstructured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 52242}
{"snippet": "l1_unstructured.apply(module, name, amount, importance_scores=None)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`, `importance_scores`.", "question_id": 52243}
{"snippet": "l1_unstructured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 52244}
{"snippet": "l1_unstructured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 52245}
{"snippet": "l1_unstructured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 52246}
{"snippet": "l1_unstructured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 52247}
{"snippet": "l1_unstructured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 52248}
{"snippet": "l1_unstructured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 52249}
{"snippet": "Tensor.diagflat()", "intent": "See torch.diagflat ( )", "question_id": 52250}
{"snippet": "Tensor.diagflat(offset=0)", "intent": "See torch.diagflat ( ) With arguments `offset`.", "question_id": 52251}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 52252}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 52253}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 52254}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 52255}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 52256}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 52257}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52258}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 52259}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 52260}
{"snippet": "torch.nn.LazyConv3d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv3d module with lazy initialization of the in_channels argument of the Conv3d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 52261}
{"snippet": "lazy_conv3d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv3d", "question_id": 52262}
{"snippet": "Tensor.q_per_channel_scales()", "intent": "Given a Tensor quantized by linear ( affine ) per-channel quantization , returns a Tensor of scales of the underlying quantizer .", "question_id": 52263}
{"snippet": "torch.nn.MultiLabelMarginLoss()", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) .", "question_id": 52264}
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`.", "question_id": 52265}
{"snippet": "torch.nn.MultiLabelMarginLoss(reduce=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduce`.", "question_id": 52266}
{"snippet": "torch.nn.MultiLabelMarginLoss(reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduction`.", "question_id": 52267}
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None)", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduce`.", "question_id": 52268}
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduction`.", "question_id": 52269}
{"snippet": "torch.nn.MultiLabelMarginLoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `reduce`, `reduction`.", "question_id": 52270}
{"snippet": "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that optimizes a multi-class multi-classification hinge loss ( margin-based loss ) between input xxx ( a 2D mini-batch Tensor ) and output yyy ( which is a 2D Tensor of target class indices ) . With arguments `size_average`, `reduce`, `reduction`.", "question_id": 52271}
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 52272}
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 52273}
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 52274}
{"snippet": "torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 2D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 52275}
{"snippet": "torch.nn.functional.fractional_max_pool3d(*args, **kwargs)", "intent": "Applies 3D fractional max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 52276}
{"snippet": "torch.initial_seed()", "intent": "Returns the initial seed for generating random numbers as a Python long .", "question_id": 52277}
{"snippet": "torch.nn.utils.spectral_norm(module)", "intent": "Applies spectral normalization to a parameter in the given `module` .", "question_id": 52278}
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight')", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`.", "question_id": 52279}
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`.", "question_id": 52280}
{"snippet": "torch.nn.utils.spectral_norm(module, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `eps`.", "question_id": 52281}
{"snippet": "torch.nn.utils.spectral_norm(module, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 52282}
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `n_power_iterations`.", "question_id": 52283}
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `eps`.", "question_id": 52284}
{"snippet": "torch.nn.utils.spectral_norm(module, name='weight', dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `dim`.", "question_id": 52285}
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `eps`.", "question_id": 52286}
{"snippet": "torch.nn.utils.spectral_norm(module, n_power_iterations=1, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `dim`.", "question_id": 52287}
{"snippet": "Tensor.le(other)", "intent": "See torch.le ( ) . With arguments `other`.", "question_id": 52288}
{"snippet": "Tensor.dist(other)", "intent": "See torch.dist ( ) With arguments `other`.", "question_id": 52289}
{"snippet": "Tensor.dist(other, p=2)", "intent": "See torch.dist ( ) With arguments `other`, `p`.", "question_id": 52290}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence .", "question_id": 52291}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence .", "question_id": 52292}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, sorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`.", "question_id": 52293}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `unsorted_indices`.", "question_id": 52294}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`.", "question_id": 52295}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `unsorted_indices`.", "question_id": 52296}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, sorted_indices=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`, `unsorted_indices`.", "question_id": 52297}
{"snippet": "torch.nn.utils.rnn.PackedSequence(data, batch_sizes=None, sorted_indices=None, unsorted_indices=None)", "intent": "Holds the `data` and list of `batch_sizes` of a packed sequence . With arguments `sorted_indices`, `unsorted_indices`.", "question_id": 52298}
{"snippet": "packed_sequence.batch_sizes", "intent": "Alias for field number 1", "question_id": 52299}
{"snippet": "packed_sequence.count(value, /)", "intent": "Return number of occurrences of `value` . With arguments `/`.", "question_id": 52300}
{"snippet": "packed_sequence.data", "intent": "Alias for field number 0", "question_id": 52301}
{"snippet": "packed_sequence.index(value, /)", "intent": "Return first index of `value` . With arguments `/`.", "question_id": 52302}
{"snippet": "packed_sequence.index(value, /, start=0)", "intent": "Return first index of `value` . With arguments `/`, `start`.", "question_id": 52303}
{"snippet": "packed_sequence.index(value, /, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `stop`.", "question_id": 52304}
{"snippet": "packed_sequence.index(value, /, start=0, stop=9223372036854775807)", "intent": "Return first index of `value` . With arguments `/`, `start`, `stop`.", "question_id": 52305}
{"snippet": "is_cuda", "intent": "Returns true if self.data stored on a gpu", "question_id": 52306}
{"snippet": "packed_sequence.is_pinned()", "intent": "Returns true if self.data stored on in pinned memory", "question_id": 52307}
{"snippet": "packed_sequence.sorted_indices", "intent": "Alias for field number 2", "question_id": 52308}
{"snippet": "packed_sequence.to(*args, **kwargs)", "intent": "Performs dtype and/or device conversion on self.data . With arguments `*args`, `**kwargs`.", "question_id": 52309}
{"snippet": "packed_sequence.unsorted_indices", "intent": "Alias for field number 3", "question_id": 52310}
{"snippet": "Tensor.rot90(k, dims)", "intent": "See torch.rot90 ( ) With arguments `k`, `dims`.", "question_id": 52311}
{"snippet": "Tensor.sspaddmm(mat1, mat2)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`.", "question_id": 52312}
{"snippet": "Tensor.sspaddmm(mat1, mat2, beta=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 52313}
{"snippet": "Tensor.sspaddmm(mat1, mat2, alpha=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 52314}
{"snippet": "Tensor.sspaddmm(mat1, mat2, beta=1, alpha=1)", "intent": "See torch.sspaddmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 52315}
{"snippet": "Tensor.trunc_()", "intent": "In-place version of trunc ( )", "question_id": 52316}
{"snippet": "torch.quantile(input, q)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input .", "question_id": 52317}
{"snippet": "torch.quantile(input, q, dim=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input .", "question_id": 52318}
{"snippet": "torch.quantile(input, q, keepdim=False)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`.", "question_id": 52319}
{"snippet": "torch.quantile(input, q, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `out`.", "question_id": 52320}
{"snippet": "torch.quantile(input, q, dim=None, keepdim=False)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`.", "question_id": 52321}
{"snippet": "torch.quantile(input, q, dim=None, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `out`.", "question_id": 52322}
{"snippet": "torch.quantile(input, q, keepdim=False, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`, `out`.", "question_id": 52323}
{"snippet": "torch.quantile(input, q, dim=None, keepdim=False, out=None)", "intent": "Computes the q-th quantiles of each row of the `input` tensor along the dimension `dim` . To compute the quantile , we map `q` in [ 0 , 1 ] to the range of indices [ 0 , n ] to find the location of the quantile in the sorted input . With arguments `keepdim`, `out`.", "question_id": 52324}
{"snippet": "torch.autograd.functional.hessian(func, inputs)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`.", "question_id": 52325}
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`.", "question_id": 52326}
{"snippet": "torch.autograd.functional.hessian(func, inputs, strict=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `strict`.", "question_id": 52327}
{"snippet": "torch.autograd.functional.hessian(func, inputs, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `vectorize`.", "question_id": 52328}
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `strict`.", "question_id": 52329}
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `vectorize`.", "question_id": 52330}
{"snippet": "torch.autograd.functional.hessian(func, inputs, strict=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `strict`, `vectorize`.", "question_id": 52331}
{"snippet": "torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False)", "intent": "Function that computes the Hessian of a given scalar function . With arguments `func`, `inputs`, `create_graph`, `strict`, `vectorize`.", "question_id": 52332}
{"snippet": "Tensor.bitwise_not_()", "intent": "In-place version of bitwise_not ( )", "question_id": 52333}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver()", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values .", "question_id": 52334}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`.", "question_id": 52335}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `dtype`.", "question_id": 52336}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `qscheme`.", "question_id": 52337}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(reduce_range=False)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `reduce_range`.", "question_id": 52338}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(quant_min=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `quant_min`.", "question_id": 52339}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(quant_max=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `quant_max`.", "question_id": 52340}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(factory_kwargs=None)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `factory_kwargs`.", "question_id": 52341}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`, `dtype`.", "question_id": 52342}
{"snippet": "torch.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, qscheme=torch.per_channel_affine)", "intent": "Observer module for computing the quantization parameters based on the running per channel min and max values . With arguments `ch_axis`, `qscheme`.", "question_id": 52343}
{"snippet": "torch.std(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`.", "question_id": 52344}
{"snippet": "torch.std(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 52345}
{"snippet": "torch.std(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 52346}
{"snippet": "torch.std(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used . Calculates the standard deviation of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 52347}
{"snippet": "torch.nn.utils.prune.RandomUnstructured(amount)", "intent": "Prune ( currently unpruned ) units in a tensor at random . With arguments `amount`.", "question_id": 52348}
{"snippet": "random_unstructured.apply(module, name, amount)", "intent": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask . With arguments `module`, `name`, `amount`.", "question_id": 52349}
{"snippet": "random_unstructured.apply_mask(module)", "intent": "Simply handles the multiplication between the parameter being pruned and the generated mask . Fetches the mask and the original tensor from the `module` and returns the pruned version of the tensor .", "question_id": 52350}
{"snippet": "random_unstructured.prune(t)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) .", "question_id": 52351}
{"snippet": "random_unstructured.prune(t, default_mask=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`.", "question_id": 52352}
{"snippet": "random_unstructured.prune(t, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `importance_scores`.", "question_id": 52353}
{"snippet": "random_unstructured.prune(t, default_mask=None, importance_scores=None)", "intent": "Computes and returns a pruned version of input tensor `t` according to the pruning rule specified in compute_mask ( ) . With arguments `default_mask`, `importance_scores`.", "question_id": 52354}
{"snippet": "random_unstructured.remove(module)", "intent": "Removes the pruning reparameterization from a `module` .", "question_id": 52355}
{"snippet": "Tensor.repeat_interleave(repeats)", "intent": "See torch.repeat_interleave ( ) . With arguments `repeats`.", "question_id": 52356}
{"snippet": "Tensor.repeat_interleave(repeats, dim=None)", "intent": "See torch.repeat_interleave ( ) . With arguments `repeats`, `dim`.", "question_id": 52357}
{"snippet": "torch.bitwise_not(input)", "intent": "Computes the bitwise NOT of the given `input` tensor .", "question_id": 52358}
{"snippet": "torch.bitwise_not(input, out=None)", "intent": "Computes the bitwise NOT of the given `input` tensor . With arguments `out`.", "question_id": 52359}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`.", "question_id": 52360}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`.", "question_id": 52361}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`.", "question_id": 52362}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `verbose`.", "question_id": 52363}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`.", "question_id": 52364}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `verbose`.", "question_id": 52365}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 52366}
{"snippet": "torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=- 1, verbose=False)", "intent": "Decays the learning rate of each parameter group by `gamma` every `step_size` epochs . With arguments `optimizer`, `last_epoch`, `verbose`.", "question_id": 52367}
{"snippet": "step_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 52368}
{"snippet": "step_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 52369}
{"snippet": "step_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 52370}
{"snippet": "step_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 52371}
{"snippet": "step_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 52372}
{"snippet": "torch.nn.intrinsic.ConvBn1d(conv, bn)", "intent": "This is a sequential container which calls the Conv 1d and Batch Norm 1d modules . With arguments `conv`, `bn`.", "question_id": 52373}
{"snippet": "torch.nn.Upsample()", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data .", "question_id": 52374}
{"snippet": "torch.nn.Upsample(size=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 52375}
{"snippet": "torch.nn.Upsample(scale_factor=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 52376}
{"snippet": "torch.nn.Upsample(mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . With arguments `mode`.", "question_id": 52377}
{"snippet": "torch.nn.Upsample(align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . With arguments `align_corners`.", "question_id": 52378}
{"snippet": "torch.nn.Upsample(size=None, scale_factor=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size .", "question_id": 52379}
{"snippet": "torch.nn.Upsample(size=None, mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `mode`.", "question_id": 52380}
{"snippet": "torch.nn.Upsample(size=None, align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `align_corners`.", "question_id": 52381}
{"snippet": "torch.nn.Upsample(scale_factor=None, mode='nearest')", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `mode`.", "question_id": 52382}
{"snippet": "torch.nn.Upsample(scale_factor=None, align_corners=None)", "intent": "Upsamples a given multi-channel 1D ( temporal ) , 2D ( spatial ) or 3D ( volumetric ) data . One can either give a `scale_factor` or the target output `size` to calculate the output size . With arguments `align_corners`.", "question_id": 52383}
{"snippet": "torch.use_deterministic_algorithms(mode)", "intent": "Sets whether PyTorch operations must use \u201c deterministic \u201d algorithms . With arguments `mode`.", "question_id": 52384}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52385}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52386}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52387}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52388}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52389}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52390}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52391}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 52392}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 52393}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU1d module is a fused module of Conv1d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 52394}
{"snippet": "torch.cuda.synchronize()", "intent": "Waits for all kernels in all streams on a CUDA `device` to complete .", "question_id": 52395}
{"snippet": "torch.cuda.synchronize(device=None)", "intent": "Waits for all kernels in all streams on a CUDA `device` to complete .", "question_id": 52396}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`.", "question_id": 52397}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`.", "question_id": 52398}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, device=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `device`.", "question_id": 52399}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `dtype`.", "question_id": 52400}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `device`.", "question_id": 52401}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `dtype`.", "question_id": 52402}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, device=None, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `device`, `dtype`.", "question_id": 52403}
{"snippet": "torch.nn.GRUCell(input_size, hidden_size, bias=True, device=None, dtype=None)", "intent": "A gated recurrent unit ( GRU ) cell With arguments `input_size`, `hidden_size`, `bias`, `device`, `dtype`.", "question_id": 52404}
{"snippet": "torch.frexp(input, Tensor exponent)", "intent": "Decomposes `input` into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text { input } = \\text { mantissa } \\times 2^ { \\text { exponent } } input=mantissa\u00d72exponent . With arguments `Tensor exponent`.", "question_id": 52405}
{"snippet": "torch.frexp(input, Tensor exponent, out=None) -> (Tensor mantissa)", "intent": "Decomposes `input` into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text { input } = \\text { mantissa } \\times 2^ { \\text { exponent } } input=mantissa\u00d72exponent . With arguments `Tensor exponent`, `out`.", "question_id": 52406}
{"snippet": "torch.numel(input)", "intent": "Returns the total number of elements in the `input` tensor .", "question_id": 52407}
{"snippet": "Tensor.log2()", "intent": "See torch.log2 ( )", "question_id": 52408}
{"snippet": "Tensor.share_memory_()", "intent": "Moves the underlying storage to shared memory .", "question_id": 52409}
{"snippet": "torch.chain_matmul(*matrices)", "intent": "Returns the matrix product of the NNN 2-D tensors . With arguments `*matrices`.", "question_id": 52410}
{"snippet": "torch.chain_matmul(*matrices, out=None)", "intent": "Returns the matrix product of the NNN 2-D tensors . With arguments `*matrices`, `out`.", "question_id": 52411}
{"snippet": "Optimizer.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 52412}
{"snippet": "torch.nn.functional.rrelu_(input)", "intent": "In-place version of rrelu ( ) . With arguments `input`.", "question_id": 52413}
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`.", "question_id": 52414}
{"snippet": "torch.nn.functional.rrelu_(input, upper=1. / 3)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `upper`.", "question_id": 52415}
{"snippet": "torch.nn.functional.rrelu_(input, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `training`.", "question_id": 52416}
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, upper=1. / 3)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `upper`.", "question_id": 52417}
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `training`.", "question_id": 52418}
{"snippet": "torch.nn.functional.rrelu_(input, upper=1. / 3, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `upper`, `training`.", "question_id": 52419}
{"snippet": "torch.nn.functional.rrelu_(input, lower=1. / 8, upper=1. / 3, training=False)", "intent": "In-place version of rrelu ( ) . With arguments `input`, `lower`, `upper`, `training`.", "question_id": 52420}
{"snippet": "Tensor.cholesky()", "intent": "See torch.cholesky ( )", "question_id": 52421}
{"snippet": "Tensor.cholesky(upper=False)", "intent": "See torch.cholesky ( ) With arguments `upper`.", "question_id": 52422}
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`.", "question_id": 52423}
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, negative_slope=0.01)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `negative_slope`.", "question_id": 52424}
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `inplace`.", "question_id": 52425}
{"snippet": "torch.nn.quantized.functional.leaky_relu(input, scale, zero_point, negative_slope=0.01, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `scale`, `zero_point`, `negative_slope`, `inplace`.", "question_id": 52426}
{"snippet": "Tensor.acos()", "intent": "See torch.acos ( )", "question_id": 52427}
{"snippet": "torch.optim.ASGD(params)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`.", "question_id": 52428}
{"snippet": "torch.optim.ASGD(params, lr=0.01)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`.", "question_id": 52429}
{"snippet": "torch.optim.ASGD(params, lambd=0.0001)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lambd`.", "question_id": 52430}
{"snippet": "torch.optim.ASGD(params, alpha=0.75)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `alpha`.", "question_id": 52431}
{"snippet": "torch.optim.ASGD(params, t0=1000000.0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `t0`.", "question_id": 52432}
{"snippet": "torch.optim.ASGD(params, weight_decay=0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `weight_decay`.", "question_id": 52433}
{"snippet": "torch.optim.ASGD(params, lr=0.01, lambd=0.0001)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `lambd`.", "question_id": 52434}
{"snippet": "torch.optim.ASGD(params, lr=0.01, alpha=0.75)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `alpha`.", "question_id": 52435}
{"snippet": "torch.optim.ASGD(params, lr=0.01, t0=1000000.0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `t0`.", "question_id": 52436}
{"snippet": "torch.optim.ASGD(params, lr=0.01, weight_decay=0)", "intent": "Implements Averaged Stochastic Gradient Descent . With arguments `params`, `lr`, `weight_decay`.", "question_id": 52437}
{"snippet": "asgd.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 52438}
{"snippet": "asgd.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 52439}
{"snippet": "asgd.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 52440}
{"snippet": "asgd.step()", "intent": "Performs a single optimization step .", "question_id": 52441}
{"snippet": "asgd.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 52442}
{"snippet": "asgd.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 52443}
{"snippet": "asgd.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 52444}
{"snippet": "Tensor.arcsinh()", "intent": "See torch.arcsinh ( )", "question_id": 52445}
{"snippet": "torch.cholesky_inverse(input)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . With arguments `input`.", "question_id": 52446}
{"snippet": "torch.cholesky_inverse(input, upper=False)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . If `upper` is False , uuu is lower triangular such that the returned tensor is With arguments `input`.", "question_id": 52447}
{"snippet": "torch.cholesky_inverse(input, out=None)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . With arguments `input`, `out`.", "question_id": 52448}
{"snippet": "torch.cholesky_inverse(input, upper=False, out=None)", "intent": "Computes the inverse of a symmetric positive-definite matrix AAA using its Cholesky factor uuu : returns matrix inv . If `upper` is False , uuu is lower triangular such that the returned tensor is With arguments `input`, `out`.", "question_id": 52449}
{"snippet": "torch.diff(input)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] .", "question_id": 52450}
{"snippet": "torch.diff(input, n=1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`.", "question_id": 52451}
{"snippet": "torch.diff(input, dim=- 1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`.", "question_id": 52452}
{"snippet": "torch.diff(input, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `prepend`.", "question_id": 52453}
{"snippet": "torch.diff(input, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `append`.", "question_id": 52454}
{"snippet": "torch.diff(input, n=1, dim=- 1)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `dim`.", "question_id": 52455}
{"snippet": "torch.diff(input, n=1, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `prepend`.", "question_id": 52456}
{"snippet": "torch.diff(input, n=1, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `n`, `append`.", "question_id": 52457}
{"snippet": "torch.diff(input, dim=- 1, prepend=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`, `prepend`.", "question_id": 52458}
{"snippet": "torch.diff(input, dim=- 1, append=None)", "intent": "Computes the n-th forward difference along the given dimension . The first-order differences are given by out [ i ] = `input` [ i + 1 ] - input [ i ] . With arguments `dim`, `append`.", "question_id": 52459}
{"snippet": "torch.range(end)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 52460}
{"snippet": "torch.range(end, start=0)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 52461}
{"snippet": "torch.range(end, step=1)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 52462}
{"snippet": "torch.range(end, out=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `out`.", "question_id": 52463}
{"snippet": "torch.range(end, dtype=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `dtype`.", "question_id": 52464}
{"snippet": "torch.range(end, layout=torch.strided)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `layout`.", "question_id": 52465}
{"snippet": "torch.range(end, device=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `device`.", "question_id": 52466}
{"snippet": "torch.range(end, requires_grad=False)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `requires_grad`.", "question_id": 52467}
{"snippet": "torch.range(end, start=0, step=1)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step .", "question_id": 52468}
{"snippet": "torch.range(end, start=0, out=None)", "intent": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac { \\text { `end` } - \\text { `start` } } { \\text { `step` } } \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1 with values from start to end with step step . With arguments `out`.", "question_id": 52469}
{"snippet": "torch.nn.functional.dropout2d(input)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) .", "question_id": 52470}
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 52471}
{"snippet": "torch.nn.functional.dropout2d(input, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`.", "question_id": 52472}
{"snippet": "torch.nn.functional.dropout2d(input, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `inplace`.", "question_id": 52473}
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, training=True)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`.", "question_id": 52474}
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 52475}
{"snippet": "torch.nn.functional.dropout2d(input, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . With arguments `training`, `inplace`.", "question_id": 52476}
{"snippet": "torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 2D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched `input` is a 2D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) of the input tensor ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `training`, `inplace`.", "question_id": 52477}
{"snippet": "torch.quantization.observer.NoopObserver()", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) .", "question_id": 52478}
{"snippet": "torch.quantization.observer.NoopObserver(dtype=torch.float16)", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`.", "question_id": 52479}
{"snippet": "torch.quantization.observer.NoopObserver(custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `custom_op_name`.", "question_id": 52480}
{"snippet": "torch.quantization.observer.NoopObserver(dtype=torch.float16, custom_op_name='')", "intent": "Observer that doesn \u2019 t do anything and just passes its configuration to the quantized module \u2019 s .from_float ( ) . With arguments `dtype`, `custom_op_name`.", "question_id": 52481}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module)", "intent": "Applies spectral normalization to a parameter in the given `module` .", "question_id": 52482}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight')", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`.", "question_id": 52483}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`.", "question_id": 52484}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `eps`.", "question_id": 52485}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `dim`.", "question_id": 52486}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', n_power_iterations=1)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `n_power_iterations`.", "question_id": 52487}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `eps`.", "question_id": 52488}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, name='weight', dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `name`, `dim`.", "question_id": 52489}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1, eps=1e-12)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `eps`.", "question_id": 52490}
{"snippet": "torch.nn.utils.parametrizations.spectral_norm(module, n_power_iterations=1, dim=None)", "intent": "Applies spectral normalization to a parameter in the given `module` . With arguments `n_power_iterations`, `dim`.", "question_id": 52491}
{"snippet": "torch.bucketize(input, boundaries)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries .", "question_id": 52492}
{"snippet": "torch.bucketize(input, boundaries, out_int32=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out_int32`.", "question_id": 52493}
{"snippet": "torch.bucketize(input, boundaries, right=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed .", "question_id": 52494}
{"snippet": "torch.bucketize(input, boundaries, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out`.", "question_id": 52495}
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, right=False)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out_int32`.", "question_id": 52496}
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . With arguments `out_int32`, `out`.", "question_id": 52497}
{"snippet": "torch.bucketize(input, boundaries, right=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out`.", "question_id": 52498}
{"snippet": "torch.bucketize(input, boundaries, out_int32=False, right=False, out=None)", "intent": "Returns the indices of the buckets to which each value in the `input` belongs , where the `boundaries` of the buckets are set by boundaries . If `right` is False ( default ) , then the left boundary is closed . With arguments `out_int32`, `out`.", "question_id": 52499}
{"snippet": "torch.quantization.quantize_dynamic(model)", "intent": "Converts a float `model` to dynamic ( i.e .", "question_id": 52500}
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `qconfig_spec`.", "question_id": 52501}
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 .", "question_id": 52502}
{"snippet": "torch.quantization.quantize_dynamic(model, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) .", "question_id": 52503}
{"snippet": "torch.quantization.quantize_dynamic(model, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `inplace`.", "question_id": 52504}
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . With arguments `qconfig_spec`.", "question_id": 52505}
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) . With arguments `qconfig_spec`.", "question_id": 52506}
{"snippet": "torch.quantization.quantize_dynamic(model, qconfig_spec=None, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . With arguments `qconfig_spec`, `inplace`.", "question_id": 52507}
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8, mapping=None)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . Fine grained control is possible with qconfig and `mapping` that act similarly to quantize ( ) .", "question_id": 52508}
{"snippet": "torch.quantization.quantize_dynamic(model, dtype=torch.qint8, inplace=False)", "intent": "Converts a float `model` to dynamic ( i.e . For simplest usage provide `dtype` argument that can be float16 or qint8 . With arguments `inplace`.", "question_id": 52509}
{"snippet": "torch.quantization.swap_module(mod, mapping, custom_module_class_mapping)", "intent": "Swaps the module if it has a quantized counterpart and it has an observer attached . With arguments `mod`, `mapping`, `custom_module_class_mapping`.", "question_id": 52510}
{"snippet": "torch.sspaddmm(input, mat1, mat2)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result .", "question_id": 52511}
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`.", "question_id": 52512}
{"snippet": "torch.sspaddmm(input, mat1, mat2, alpha=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `alpha`.", "question_id": 52513}
{"snippet": "torch.sspaddmm(input, mat1, mat2, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `out`.", "question_id": 52514}
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, alpha=1)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `alpha`.", "question_id": 52515}
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `out`.", "question_id": 52516}
{"snippet": "torch.sspaddmm(input, mat1, mat2, alpha=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `alpha`, `out`.", "question_id": 52517}
{"snippet": "torch.sspaddmm(input, mat1, mat2, beta=1, alpha=1, out=None)", "intent": "Matrix multiplies a sparse tensor `mat1` with a dense tensor `mat2` , then adds the sparse tensor `input` to the result . With arguments `beta`, `alpha`, `out`.", "question_id": 52518}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`.", "question_id": 52519}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`.", "question_id": 52520}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`.", "question_id": 52521}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `reduce`.", "question_id": 52522}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `reduction`.", "question_id": 52523}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `size_average`.", "question_id": 52524}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduce`.", "question_id": 52525}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `margin`, `reduction`.", "question_id": 52526}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None, reduce=None)", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduce`.", "question_id": 52527}
{"snippet": "torch.nn.functional.margin_ranking_loss(input1, input2, target, size_average=None, reduction='mean')", "intent": "See MarginRankingLoss for details . With arguments `input1`, `input2`, `target`, `size_average`, `reduction`.", "question_id": 52528}
{"snippet": "Tensor.addbmm_(batch1, batch2)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 52529}
{"snippet": "Tensor.addbmm_(batch1, batch2, beta=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 52530}
{"snippet": "Tensor.addbmm_(batch1, batch2, alpha=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 52531}
{"snippet": "Tensor.addbmm_(batch1, batch2, beta=1, alpha=1)", "intent": "In-place version of addbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 52532}
{"snippet": "Tensor.byte()", "intent": "self.byte ( ) is equivalent to self.to ( torch.uint8 ) .", "question_id": 52533}
{"snippet": "Tensor.byte(memory_format=torch.preserve_format)", "intent": "self.byte ( ) is equivalent to self.to ( torch.uint8 ) . With arguments `memory_format`.", "question_id": 52534}
{"snippet": "torch.cuda.device_of(obj)", "intent": "Context-manager that changes the current device to that of given object . With arguments `obj`.", "question_id": 52535}
{"snippet": "Tensor.atanh_(other)", "intent": "In-place version of atanh ( ) With arguments `other`.", "question_id": 52536}
{"snippet": "Tensor.clamp_()", "intent": "In-place version of clamp ( )", "question_id": 52537}
{"snippet": "Tensor.clamp_(min=None)", "intent": "In-place version of clamp ( ) With arguments `min`.", "question_id": 52538}
{"snippet": "Tensor.clamp_(max=None)", "intent": "In-place version of clamp ( ) With arguments `max`.", "question_id": 52539}
{"snippet": "Tensor.clamp_(min=None, max=None)", "intent": "In-place version of clamp ( ) With arguments `min`, `max`.", "question_id": 52540}
{"snippet": "torch.asinh(input)", "intent": "Returns a new tensor with the inverse hyperbolic sine of the elements of `input` .", "question_id": 52541}
{"snippet": "torch.asinh(input, out=None)", "intent": "Returns a new tensor with the inverse hyperbolic sine of the elements of `input` . With arguments `out`.", "question_id": 52542}
{"snippet": "torch.nn.functional.local_response_norm(input, size)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`.", "question_id": 52543}
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`.", "question_id": 52544}
{"snippet": "torch.nn.functional.local_response_norm(input, size, beta=0.75)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`.", "question_id": 52545}
{"snippet": "torch.nn.functional.local_response_norm(input, size, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `k`.", "question_id": 52546}
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`.", "question_id": 52547}
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `k`.", "question_id": 52548}
{"snippet": "torch.nn.functional.local_response_norm(input, size, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`, `k`.", "question_id": 52549}
{"snippet": "torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an `input` signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`, `k`.", "question_id": 52550}
{"snippet": "torch.nn.GRU(*args, **kwargs)", "intent": "Applies a multi-layer gated recurrent unit ( GRU ) RNN to an input sequence . With arguments `*args`, `**kwargs`.", "question_id": 52551}
{"snippet": "Tensor.long()", "intent": "self.long ( ) is equivalent to self.to ( torch.int64 ) .", "question_id": 52552}
{"snippet": "Tensor.long(memory_format=torch.preserve_format)", "intent": "self.long ( ) is equivalent to self.to ( torch.int64 ) . With arguments `memory_format`.", "question_id": 52553}
{"snippet": "Tensor.renorm_(p, dim, maxnorm)", "intent": "In-place version of renorm ( ) With arguments `p`, `dim`, `maxnorm`.", "question_id": 52554}
{"snippet": "Tensor.floor_divide(value)", "intent": "See torch.floor_divide ( ) With arguments `value`.", "question_id": 52555}
{"snippet": "Tensor.bool()", "intent": "self.bool ( ) is equivalent to self.to ( torch.bool ) .", "question_id": 52556}
{"snippet": "Tensor.bool(memory_format=torch.preserve_format)", "intent": "self.bool ( ) is equivalent to self.to ( torch.bool ) . With arguments `memory_format`.", "question_id": 52557}
{"snippet": "Tensor.lcm(other)", "intent": "See torch.lcm ( ) With arguments `other`.", "question_id": 52558}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`.", "question_id": 52559}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`.", "question_id": 52560}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuser_func=<function fuse_known_modules>)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuser_func`.", "question_id": 52561}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuse_custom_config_dict`.", "question_id": 52562}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuser_func`.", "question_id": 52563}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuse_custom_config_dict`.", "question_id": 52564}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `fuser_func`, `fuse_custom_config_dict`.", "question_id": 52565}
{"snippet": "torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None)", "intent": "Fuses a list of modules into a single module With arguments `model`, `modules_to_fuse`, `inplace`, `fuser_func`, `fuse_custom_config_dict`.", "question_id": 52566}
{"snippet": "torch.sub(input, other)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` .", "question_id": 52567}
{"snippet": "torch.sub(input, other, alpha=1)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` .", "question_id": 52568}
{"snippet": "torch.sub(input, other, out=None)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` . With arguments `out`.", "question_id": 52569}
{"snippet": "torch.sub(input, other, alpha=1, out=None)", "intent": "Subtracts `other` , scaled by `alpha` , from `input` . With arguments `out`.", "question_id": 52570}
{"snippet": "torch.nn.PixelUnshuffle(downscale_factor)", "intent": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) to a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) , where r is a downscale factor . With arguments `downscale_factor`.", "question_id": 52571}
{"snippet": "torch.chunk(input, chunks)", "intent": "Splits a tensor into a specific number of `chunks` . Each chunk is a view of the `input` tensor .", "question_id": 52572}
{"snippet": "torch.chunk(input, chunks, dim=0)", "intent": "Splits a tensor into a specific number of `chunks` . Each chunk is a view of the `input` tensor . Last chunk will be smaller if the tensor size along the given dimension `dim` is not divisible by chunks .", "question_id": 52573}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`.", "question_id": 52574}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `step_size_up`.", "question_id": 52575}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_down=None)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `step_size_down`.", "question_id": 52576}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, mode='triangular')", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `mode`.", "question_id": 52577}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, gamma=1.0)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `gamma`.", "question_id": 52578}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, scale_fn=None)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `scale_fn`.", "question_id": 52579}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, scale_mode='cycle')", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `scale_mode`.", "question_id": 52580}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, cycle_momentum=True)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `cycle_momentum`.", "question_id": 52581}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, base_momentum=0.8)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `base_momentum`.", "question_id": 52582}
{"snippet": "torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, max_momentum=0.9)", "intent": "Sets the learning rate of each parameter group according to cyclical learning rate policy ( CLR ) . With arguments `optimizer`, `base_lr`, `max_lr`, `max_momentum`.", "question_id": 52583}
{"snippet": "cyclic_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 52584}
{"snippet": "cyclic_lr.get_lr()", "intent": "Calculates the learning rate at batch index .", "question_id": 52585}
{"snippet": "cyclic_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 52586}
{"snippet": "cyclic_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 52587}
{"snippet": "cyclic_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 52588}
{"snippet": "cyclic_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 52589}
{"snippet": "torch.nn.functional.affine_grid(theta, size)", "intent": "Generates a 2D or 3D flow field ( sampling grid ) , given a batch of affine matrices `theta` . With arguments `size`.", "question_id": 52590}
{"snippet": "torch.nn.functional.affine_grid(theta, size, align_corners=None)", "intent": "Generates a 2D or 3D flow field ( sampling grid ) , given a batch of affine matrices `theta` . With arguments `size`, `align_corners`.", "question_id": 52591}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 52592}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 52593}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 52594}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`.", "question_id": 52595}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `groups`.", "question_id": 52596}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `bias`.", "question_id": 52597}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dilation`.", "question_id": 52598}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `padding_mode`.", "question_id": 52599}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `device`.", "question_id": 52600}
{"snippet": "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 2D transposed convolution operator over an input image composed of several input planes . The parameters `kernel_size` , `stride` , `padding` , `output_padding` can either be : With arguments `in_channels`, `out_channels`, `dtype`.", "question_id": 52601}
{"snippet": "torch.linalg.inv(A)", "intent": "Computes the inverse of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 52602}
{"snippet": "torch.linalg.inv(A, out=None)", "intent": "Computes the inverse of a square matrix if it exists . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 52603}
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`.", "question_id": 52604}
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`.", "question_id": 52605}
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, dtype=torch.qint8)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `dtype`.", "question_id": 52606}
{"snippet": "torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)", "intent": "A dynamic quantized linear module with floating point tensor as inputs and outputs . With arguments `in_features`, `out_features`, `bias_`, `dtype`.", "question_id": 52607}
{"snippet": "linear.from_float(mod)", "intent": "Create a dynamic quantized module from a float module or qparams_dict With arguments `mod`.", "question_id": 52608}
{"snippet": "Tensor.igamma(other)", "intent": "See torch.igamma ( ) With arguments `other`.", "question_id": 52609}
{"snippet": "torch.nn.LogSigmoid", "intent": "Applies the element-wise function:", "question_id": 52610}
{"snippet": "Tensor.lt(other)", "intent": "See torch.lt ( ) . With arguments `other`.", "question_id": 52611}
{"snippet": "Tensor.arccos()", "intent": "See torch.arccos ( )", "question_id": 52612}
{"snippet": "Tensor.cos_()", "intent": "In-place version of cos ( )", "question_id": 52613}
{"snippet": "torch.optim.SparseAdam(params, 0.999))", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`.", "question_id": 52614}
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`.", "question_id": 52615}
{"snippet": "torch.optim.SparseAdam(params, 0.999), betas=(0.9)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `betas`.", "question_id": 52616}
{"snippet": "torch.optim.SparseAdam(params, 0.999), eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `eps`.", "question_id": 52617}
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, betas=(0.9)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `betas`.", "question_id": 52618}
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `eps`.", "question_id": 52619}
{"snippet": "torch.optim.SparseAdam(params, 0.999), betas=(0.9, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `betas`, `eps`.", "question_id": 52620}
{"snippet": "torch.optim.SparseAdam(params, 0.999), lr=0.001, betas=(0.9, eps=1e-08)", "intent": "Implements lazy version of Adam algorithm suitable for sparse tensors . With arguments `params`, `0.999)`, `lr`, `betas`, `eps`.", "question_id": 52621}
{"snippet": "sparse_adam.add_param_group(param_group)", "intent": "Add a param group to the Optimizer s param_groups . With arguments `param_group`.", "question_id": 52622}
{"snippet": "sparse_adam.load_state_dict(state_dict)", "intent": "Loads the optimizer state . With arguments `state_dict`.", "question_id": 52623}
{"snippet": "sparse_adam.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 52624}
{"snippet": "sparse_adam.step()", "intent": "Performs a single optimization step .", "question_id": 52625}
{"snippet": "sparse_adam.step(closure=None)", "intent": "Performs a single optimization step . With arguments `closure`.", "question_id": 52626}
{"snippet": "sparse_adam.zero_grad()", "intent": "Sets the gradients of all optimized torch.Tensor s to zero .", "question_id": 52627}
{"snippet": "sparse_adam.zero_grad(set_to_none=False)", "intent": "Sets the gradients of all optimized torch.Tensor s to zero . With arguments `set_to_none`.", "question_id": 52628}
{"snippet": "Tensor.sgn()", "intent": "See torch.sgn ( )", "question_id": 52629}
{"snippet": "Tensor.atan2(other)", "intent": "See torch.atan2 ( ) With arguments `other`.", "question_id": 52630}
{"snippet": "torch.igammac(input, other)", "intent": "Computes the regularized upper incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive .", "question_id": 52631}
{"snippet": "torch.igammac(input, other, out=None)", "intent": "Computes the regularized upper incomplete gamma function : where both inputi\\text { `input` } _iinputi\u200b and otheri\\text { `other` } _iotheri\u200b are weakly positive and at least one is strictly positive . If both are zero or either is negative then outi=nan\\text { `out` } _i=\\text { nan } outi\u200b=nan .", "question_id": 52632}
{"snippet": "Tensor.dim()", "intent": "Returns the number of dimensions of self tensor .", "question_id": 52633}
{"snippet": "sparse_adam.flatten_parameters()", "intent": "Resets parameter data pointer so that they can use faster code paths .", "question_id": 52634}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52635}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52636}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52637}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52638}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52639}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, bias=True)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52640}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52641}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 52642}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, dilation=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `dilation`.", "question_id": 52643}
{"snippet": "torch.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, groups=1)", "intent": "A ConvReLU2d module is a fused module of Conv2d and ReLU With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`, `groups`.", "question_id": 52644}
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`.", "question_id": 52645}
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=- 1)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `last_epoch`.", "question_id": 52646}
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, verbose=False)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `verbose`.", "question_id": 52647}
{"snippet": "torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda, last_epoch=- 1, verbose=False)", "intent": "Multiply the learning rate of each parameter group by the factor given in the specified function . With arguments `optimizer`, `lr_lambda`, `last_epoch`, `verbose`.", "question_id": 52648}
{"snippet": "multiplicative_lr.get_last_lr()", "intent": "Return last computed learning rate by current scheduler .", "question_id": 52649}
{"snippet": "multiplicative_lr.load_state_dict(state_dict)", "intent": "Loads the schedulers state . With arguments `state_dict`.", "question_id": 52650}
{"snippet": "multiplicative_lr.print_lr(is_verbose, group, lr)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`.", "question_id": 52651}
{"snippet": "multiplicative_lr.print_lr(is_verbose, group, lr, epoch=None)", "intent": "Display the current learning rate . With arguments `is_verbose`, `group`, `lr`, `epoch`.", "question_id": 52652}
{"snippet": "multiplicative_lr.state_dict()", "intent": "Returns the state of the scheduler as a dict .", "question_id": 52653}
{"snippet": "torch.quantization.observer.default_debug_observer", "intent": "alias of torch.quantization.observer.RecordingObserver", "question_id": 52654}
{"snippet": "torch.div(input, other)", "intent": "Divides each element of the `input` input by the corresponding element of `other` .", "question_id": 52655}
{"snippet": "torch.div(input, other, rounding_mode=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `rounding_mode`.", "question_id": 52656}
{"snippet": "torch.div(input, other, out=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `out`.", "question_id": 52657}
{"snippet": "torch.div(input, other, rounding_mode=None, out=None)", "intent": "Divides each element of the `input` input by the corresponding element of `other` . With arguments `rounding_mode`, `out`.", "question_id": 52658}
{"snippet": "Tensor.less_equal(other)", "intent": "See torch.less_equal ( ) . With arguments `other`.", "question_id": 52659}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 52660}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 52661}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 52662}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 52663}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, mode='sum')", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `mode`.", "question_id": 52664}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, sparse=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 52665}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, _weight=None)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 52666}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, include_last_offset=False)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `include_last_offset`.", "question_id": 52667}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, dtype=torch.quint8)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `dtype`.", "question_id": 52668}
{"snippet": "torch.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0)", "intent": "A quantized EmbeddingBag module with quantized packed weights as inputs . With arguments `num_embeddings`, `embedding_dim`, `max_norm`, `norm_type`.", "question_id": 52669}
{"snippet": "embedding_bag.from_float(mod)", "intent": "Create a quantized embedding_bag module from a float module With arguments `mod`.", "question_id": 52670}
{"snippet": "torch.zeros(*size)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`.", "question_id": 52671}
{"snippet": "torch.zeros(*size, out=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`.", "question_id": 52672}
{"snippet": "torch.zeros(*size, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `dtype`.", "question_id": 52673}
{"snippet": "torch.zeros(*size, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `layout`.", "question_id": 52674}
{"snippet": "torch.zeros(*size, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `device`.", "question_id": 52675}
{"snippet": "torch.zeros(*size, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `requires_grad`.", "question_id": 52676}
{"snippet": "torch.zeros(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `dtype`.", "question_id": 52677}
{"snippet": "torch.zeros(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `layout`.", "question_id": 52678}
{"snippet": "torch.zeros(*size, out=None, device=None)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `device`.", "question_id": 52679}
{"snippet": "torch.zeros(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 0 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `requires_grad`.", "question_id": 52680}
{"snippet": "torch.i0(input)", "intent": "Computes the zeroth order modified Bessel function of the first kind for each element of `input` .", "question_id": 52681}
{"snippet": "torch.i0(input, out=None)", "intent": "Computes the zeroth order modified Bessel function of the first kind for each element of `input` . With arguments `out`.", "question_id": 52682}
{"snippet": "torch.jit.unused(fn)", "intent": "This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception . With arguments `fn`.", "question_id": 52683}
{"snippet": "torch.jit.wait(future)", "intent": "Forces completion of a torch.jit.Future [ T ] asynchronous task , returning the result of the task . With arguments `future`.", "question_id": 52684}
{"snippet": "torch.nn.PoissonNLLLoss()", "intent": "Negative log likelihood loss with Poisson distribution of target .", "question_id": 52685}
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`.", "question_id": 52686}
{"snippet": "torch.nn.PoissonNLLLoss(full=False)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `full`.", "question_id": 52687}
{"snippet": "torch.nn.PoissonNLLLoss(size_average=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `size_average`.", "question_id": 52688}
{"snippet": "torch.nn.PoissonNLLLoss(eps=1e-08)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `eps`.", "question_id": 52689}
{"snippet": "torch.nn.PoissonNLLLoss(reduce=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `reduce`.", "question_id": 52690}
{"snippet": "torch.nn.PoissonNLLLoss(reduction='mean')", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `reduction`.", "question_id": 52691}
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, full=False)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `full`.", "question_id": 52692}
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, size_average=None)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `size_average`.", "question_id": 52693}
{"snippet": "torch.nn.PoissonNLLLoss(log_input=True, eps=1e-08)", "intent": "Negative log likelihood loss with Poisson distribution of target . With arguments `log_input`, `eps`.", "question_id": 52694}
{"snippet": "torch.cuda.Event()", "intent": "Wrapper around a CUDA event .", "question_id": 52695}
{"snippet": "torch.cuda.Event(enable_timing=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`.", "question_id": 52696}
{"snippet": "torch.cuda.Event(blocking=False)", "intent": "Wrapper around a CUDA event . With arguments `blocking`.", "question_id": 52697}
{"snippet": "torch.cuda.Event(interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `interprocess`.", "question_id": 52698}
{"snippet": "torch.cuda.Event(enable_timing=False, blocking=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `blocking`.", "question_id": 52699}
{"snippet": "torch.cuda.Event(enable_timing=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `interprocess`.", "question_id": 52700}
{"snippet": "torch.cuda.Event(blocking=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `blocking`, `interprocess`.", "question_id": 52701}
{"snippet": "torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False)", "intent": "Wrapper around a CUDA event . With arguments `enable_timing`, `blocking`, `interprocess`.", "question_id": 52702}
{"snippet": "event.elapsed_time(end_event)", "intent": "Returns the time elapsed in milliseconds after the event was recorded and before the `end_event` was recorded .", "question_id": 52703}
{"snippet": "event.from_ipc_handle(device, handle)", "intent": "Reconstruct an event from an IPC `handle` on the given `device` .", "question_id": 52704}
{"snippet": "event.ipc_handle()", "intent": "Returns an IPC handle of this event .", "question_id": 52705}
{"snippet": "event.query()", "intent": "Checks if all work currently captured by event has completed .", "question_id": 52706}
{"snippet": "event.record()", "intent": "Records the event in a given `stream` .", "question_id": 52707}
{"snippet": "event.record(stream=None)", "intent": "Records the event in a given `stream` .", "question_id": 52708}
{"snippet": "event.synchronize()", "intent": "Waits for the event to complete .", "question_id": 52709}
{"snippet": "event.wait()", "intent": "Makes all future work submitted to the given `stream` wait for this event .", "question_id": 52710}
{"snippet": "event.wait(stream=None)", "intent": "Makes all future work submitted to the given `stream` wait for this event .", "question_id": 52711}
{"snippet": "Tensor.narrow_copy(dimension, start, length)", "intent": "Same as Tensor.narrow ( ) except returning a copy rather than shared storage . Calling narrow_copy with dimemsion > self.sparse_dim ( ) will return a copy with the relevant dense `dimension` narrowed , and self.shape updated accordingly . With arguments `start`, `length`.", "question_id": 52712}
{"snippet": "Tensor.triu()", "intent": "See torch.triu ( )", "question_id": 52713}
{"snippet": "Tensor.triu(k=0)", "intent": "See torch.triu ( ) With arguments `k`.", "question_id": 52714}
{"snippet": "Tensor.is_complex()", "intent": "Returns True if the data type of self is a complex data type .", "question_id": 52715}
{"snippet": "torch.nn.quantized.functional.interpolate(input)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52716}
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52717}
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52718}
{"snippet": "torch.nn.quantized.functional.interpolate(input, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 52719}
{"snippet": "torch.nn.quantized.functional.interpolate(input, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 52720}
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, scale_factor=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor`", "question_id": 52721}
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 52722}
{"snippet": "torch.nn.quantized.functional.interpolate(input, size=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 52723}
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None, mode='nearest')", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `mode`.", "question_id": 52724}
{"snippet": "torch.nn.quantized.functional.interpolate(input, scale_factor=None, align_corners=None)", "intent": "Down/up samples the `input` to either the given `size` or the given `scale_factor` With arguments `align_corners`.", "question_id": 52725}
{"snippet": "torch.nn.Softshrink()", "intent": "Applies the soft shrinkage function elementwise :", "question_id": 52726}
{"snippet": "torch.nn.Softshrink(lambd=0.5)", "intent": "Applies the soft shrinkage function elementwise : With arguments `lambd`.", "question_id": 52727}
{"snippet": "Tensor.fliplr()", "intent": "See torch.fliplr ( )", "question_id": 52728}
{"snippet": "Tensor.logical_and()", "intent": "See torch.logical_and ( )", "question_id": 52729}
{"snippet": "torch.nn.utils.prune.random_unstructured(module, name, amount)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units selected at random .", "question_id": 52730}
{"snippet": "torch.nn.ModuleList()", "intent": "Holds submodules in a list .", "question_id": 52731}
{"snippet": "torch.nn.ModuleList(modules=None)", "intent": "Holds submodules in a list . ModuleList can be indexed like a regular Python list , but `modules` it contains are properly registered , and will be visible by all Module methods .", "question_id": 52732}
{"snippet": "module_list.append(module)", "intent": "Appends a given `module` to the end of the list .", "question_id": 52733}
{"snippet": "module_list.extend(modules)", "intent": "Appends `modules` from a Python iterable to the end of the list .", "question_id": 52734}
{"snippet": "module_list.insert(index, module)", "intent": "Insert a given `module` before a given `index` in the list .", "question_id": 52735}
{"snippet": "torch.fft.ifft2(input, - 1))", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`.", "question_id": 52736}
{"snippet": "torch.fft.ifft2(input, - 1), s=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`.", "question_id": 52737}
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`.", "question_id": 52738}
{"snippet": "torch.fft.ifft2(input, - 1), norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `norm`.", "question_id": 52739}
{"snippet": "torch.fft.ifft2(input, - 1), out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `out`.", "question_id": 52740}
{"snippet": "torch.fft.ifft2(input, - 1), s=None, dim=(- 2)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `dim`.", "question_id": 52741}
{"snippet": "torch.fft.ifft2(input, - 1), s=None, norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `norm`.", "question_id": 52742}
{"snippet": "torch.fft.ifft2(input, - 1), s=None, out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `s`, `out`.", "question_id": 52743}
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2, norm=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `norm`.", "question_id": 52744}
{"snippet": "torch.fft.ifft2(input, - 1), dim=(- 2, out=None)", "intent": "Computes the 2 dimensional inverse discrete Fourier transform of `input` . With arguments `- 1)`, `dim`, `out`.", "question_id": 52745}
{"snippet": "Tensor.is_coalesced()", "intent": "Returns True if self is a sparse COO tensor that is coalesced , False otherwise .", "question_id": 52746}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52747}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52748}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52749}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 52750}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52751}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52752}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52753}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52754}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 52755}
{"snippet": "torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 1D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 52756}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 52757}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 52758}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 52759}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 52760}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 52761}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 52762}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52763}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 52764}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 52765}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU1d module is a module fused from Conv1d , BatchNorm1d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 52766}
{"snippet": "torch.nn.Sequential(*args)", "intent": "A sequential container . With arguments `*args`.", "question_id": 52767}
{"snippet": "torch.nn.BCEWithLogitsLoss()", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class .", "question_id": 52768}
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc .", "question_id": 52769}
{"snippet": "torch.nn.BCEWithLogitsLoss(size_average=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . With arguments `size_average`.", "question_id": 52770}
{"snippet": "torch.nn.BCEWithLogitsLoss(reduce=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . With arguments `reduce`.", "question_id": 52771}
{"snippet": "torch.nn.BCEWithLogitsLoss(reduction='mean')", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 52772}
{"snippet": "torch.nn.BCEWithLogitsLoss(pos_weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . For example , if a dataset contains 100 positive and 300 negative examples of a single class , then `pos_weight` for the class should be equal to 300100=3\\frac { 300 } { 100 } =3100300\u200b=3 .", "question_id": 52773}
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, size_average=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . With arguments `size_average`.", "question_id": 52774}
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, reduce=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . With arguments `reduce`.", "question_id": 52775}
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, reduction='mean')", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 52776}
{"snippet": "torch.nn.BCEWithLogitsLoss(weight=None, pos_weight=None)", "intent": "This loss combines a Sigmoid layer and the BCELoss in one single class . where ccc is the class number ( c > 1c > 1c > 1 for multi-label binary classification , c=1c = 1c=1 for single-label binary classification ) , nnn is the number of the sample in the batch and pcp_cpc\u200b is the `weight` of the positive answer for the class ccc . For example , if a dataset contains 100 positive and 300 negative examples of a single class , then `pos_weight` for the class should be equal to 300100=3\\frac { 300 } { 100 } =3100300\u200b=3 .", "question_id": 52777}
{"snippet": "torch.nn.functional.mish(input)", "intent": "Applies the Mish function , element-wise . With arguments `input`.", "question_id": 52778}
{"snippet": "torch.nn.functional.mish(input, inplace=False)", "intent": "Applies the Mish function , element-wise . With arguments `input`, `inplace`.", "question_id": 52779}
{"snippet": "Tensor.kthvalue(k)", "intent": "See torch.kthvalue ( ) With arguments `k`.", "question_id": 52780}
{"snippet": "Tensor.kthvalue(k, dim=None)", "intent": "See torch.kthvalue ( ) With arguments `k`, `dim`.", "question_id": 52781}
{"snippet": "Tensor.kthvalue(k, keepdim=False)", "intent": "See torch.kthvalue ( ) With arguments `k`, `keepdim`.", "question_id": 52782}
{"snippet": "Tensor.kthvalue(k, dim=None, keepdim=False)", "intent": "See torch.kthvalue ( ) With arguments `k`, `dim`, `keepdim`.", "question_id": 52783}
{"snippet": "torch.nn.modules.module.register_module_forward_pre_hook(hook)", "intent": "Registers a forward pre-hook common to all modules . The `hook` will be called every time before forward ( ) is invoked .", "question_id": 52784}
{"snippet": "torch.dot(input, other)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`.", "question_id": 52785}
{"snippet": "torch.dot(input, other, out=None)", "intent": "Computes the dot product of two 1D tensors . With arguments `input`, `other`, `out`.", "question_id": 52786}
{"snippet": "Tensor.addmm_(mat1, mat2)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`.", "question_id": 52787}
{"snippet": "Tensor.addmm_(mat1, mat2, beta=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `beta`.", "question_id": 52788}
{"snippet": "Tensor.addmm_(mat1, mat2, alpha=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `alpha`.", "question_id": 52789}
{"snippet": "Tensor.addmm_(mat1, mat2, beta=1, alpha=1)", "intent": "In-place version of addmm ( ) With arguments `mat1`, `mat2`, `beta`, `alpha`.", "question_id": 52790}
{"snippet": "torch.promote_types(type1, type2)", "intent": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either `type1` or `type2` .", "question_id": 52791}
{"snippet": "Tensor.bitwise_not()", "intent": "See torch.bitwise_not ( )", "question_id": 52792}
{"snippet": "Tensor.frexp(input) -> (Tensor mantissa, Tensor exponent)", "intent": "See torch.frexp ( ) With arguments `input) -> (Tensor mantissa`, `Tensor exponent`.", "question_id": 52793}
{"snippet": "torch.linalg.svd(A)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 52794}
{"snippet": "torch.linalg.svd(A, full_matrices=True)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `full_matrices` chooses between the full ( default ) and reduced SVD .", "question_id": 52795}
{"snippet": "torch.linalg.svd(A, out=None)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 52796}
{"snippet": "torch.linalg.svd(A, full_matrices=True, out=None)", "intent": "Computes the singular value decomposition ( SVD ) of a matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . The parameter `full_matrices` chooses between the full ( default ) and reduced SVD . With arguments `out`.", "question_id": 52797}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`.", "question_id": 52798}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`.", "question_id": 52799}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`.", "question_id": 52800}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `device`.", "question_id": 52801}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `dtype`.", "question_id": 52802}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, head_bias=False)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `head_bias`.", "question_id": 52803}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `device`.", "question_id": 52804}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, div_value=4.0, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `div_value`, `dtype`.", "question_id": 52805}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False, device=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`, `device`.", "question_id": 52806}
{"snippet": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs, head_bias=False, dtype=None)", "intent": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave , Armand Joulin , Moustapha Ciss\u00e9 , David Grangier , and Herv\u00e9 J\u00e9gou . With arguments `in_features`, `n_classes`, `cutoffs`, `head_bias`, `dtype`.", "question_id": 52807}
{"snippet": "adaptive_log_softmax_with_loss.log_prob(input)", "intent": "Computes log probabilities for all n_classes\\texttt { n\\_classes } n_classes With arguments `input`.", "question_id": 52808}
{"snippet": "adaptive_log_softmax_with_loss.predict(input)", "intent": "This is equivalent to self.log_pob ( `input` ) .argmax ( dim=1 ) , but is more efficient in some cases .", "question_id": 52809}
{"snippet": "torch.trace(input)", "intent": "Returns the sum of the elements of the diagonal of the `input` 2-D matrix .", "question_id": 52810}
{"snippet": "torch.addr(input, vec1, vec2)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` .", "question_id": 52811}
{"snippet": "torch.addr(input, vec1, vec2, beta=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 52812}
{"snippet": "torch.addr(input, vec1, vec2, alpha=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 52813}
{"snippet": "torch.addr(input, vec1, vec2, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 52814}
{"snippet": "torch.addr(input, vec1, vec2, beta=1, alpha=1)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively .", "question_id": 52815}
{"snippet": "torch.addr(input, vec1, vec2, beta=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 52816}
{"snippet": "torch.addr(input, vec1, vec2, alpha=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 52817}
{"snippet": "torch.addr(input, vec1, vec2, beta=1, alpha=1, out=None)", "intent": "Performs the outer-product of vectors `vec1` and `vec2` and adds it to the matrix `input` . Optional values `beta` and `alpha` are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively . If vec1 is a vector of size n and vec2 is a vector of size m , then input must be broadcastable with a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) and `out` will be a matrix of size ( n\u00d7m ) ( n \\times m ) ( n\u00d7m ) .", "question_id": 52818}
{"snippet": "torch.view_as_real(input)", "intent": "Returns a view of `input` as a real tensor .", "question_id": 52819}
{"snippet": "torch.nn.functional.softsign(input)", "intent": "Applies element-wise , the function SoftSign ( x ) =x1+\u2223x\u2223\\text { SoftSign } ( x ) = \\frac { x } { 1 + |x| } SoftSign ( x ) =1+\u2223x\u2223x\u200b With arguments `input`.", "question_id": 52820}
{"snippet": "torch.nn.Hardswish()", "intent": "Applies the hardswish function , element-wise , as described in the paper :", "question_id": 52821}
{"snippet": "torch.nn.Hardswish(inplace=False)", "intent": "Applies the hardswish function , element-wise , as described in the paper : With arguments `inplace`.", "question_id": 52822}
{"snippet": "Tensor.is_quantized", "intent": "Is True if the Tensor is quantized, False otherwise.", "question_id": 52823}
{"snippet": "torch.autograd.gradcheck(func, inputs)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 52824}
{"snippet": "torch.autograd.gradcheck(func, inputs, eps=1e-06)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `eps`.", "question_id": 52825}
{"snippet": "torch.autograd.gradcheck(func, inputs, atol=1e-05)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `atol`.", "question_id": 52826}
{"snippet": "torch.autograd.gradcheck(func, inputs, rtol=0.001)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `rtol`.", "question_id": 52827}
{"snippet": "torch.autograd.gradcheck(func, inputs, raise_exception=True)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `raise_exception`.", "question_id": 52828}
{"snippet": "torch.autograd.gradcheck(func, inputs, check_sparse_nnz=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_sparse_nnz`.", "question_id": 52829}
{"snippet": "torch.autograd.gradcheck(func, inputs, nondet_tol=0.0)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `nondet_tol`.", "question_id": 52830}
{"snippet": "torch.autograd.gradcheck(func, inputs, check_undefined_grad=True)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_undefined_grad`.", "question_id": 52831}
{"snippet": "torch.autograd.gradcheck(func, inputs, check_grad_dtypes=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_grad_dtypes`.", "question_id": 52832}
{"snippet": "torch.autograd.gradcheck(func, inputs, check_batched_grad=False)", "intent": "Check gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_batched_grad`.", "question_id": 52833}
{"snippet": "torch.nn.NLLLoss()", "intent": "The negative log likelihood loss .", "question_id": 52834}
{"snippet": "torch.nn.NLLLoss(weight=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes .", "question_id": 52835}
{"snippet": "torch.nn.NLLLoss(size_average=None)", "intent": "The negative log likelihood loss . With arguments `size_average`.", "question_id": 52836}
{"snippet": "torch.nn.NLLLoss(ignore_index=- 100)", "intent": "The negative log likelihood loss . The target that this loss expects should be a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] where C = number of classes ; if `ignore_index` is specified , this loss also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 52837}
{"snippet": "torch.nn.NLLLoss(reduce=None)", "intent": "The negative log likelihood loss . With arguments `reduce`.", "question_id": 52838}
{"snippet": "torch.nn.NLLLoss(reduction='mean')", "intent": "The negative log likelihood loss . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 52839}
{"snippet": "torch.nn.NLLLoss(weight=None, size_average=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `size_average`.", "question_id": 52840}
{"snippet": "torch.nn.NLLLoss(weight=None, ignore_index=- 100)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . The target that this loss expects should be a class index in the range [ 0 , C\u22121 ] [ 0 , C-1 ] [ 0 , C\u22121 ] where C = number of classes ; if `ignore_index` is specified , this loss also accepts this class index ( this index may not necessarily be in the class range ) .", "question_id": 52841}
{"snippet": "torch.nn.NLLLoss(weight=None, reduce=None)", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . With arguments `reduce`.", "question_id": 52842}
{"snippet": "torch.nn.NLLLoss(weight=None, reduction='mean')", "intent": "The negative log likelihood loss . If provided , the optional argument `weight` should be a 1D Tensor assigning weight to each of the classes . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 52843}
{"snippet": "torch.greater_equal(input, other)", "intent": "Alias for torch.ge ( ) . With arguments `input`, `other`.", "question_id": 52844}
{"snippet": "torch.greater_equal(input, other, out=None)", "intent": "Alias for torch.ge ( ) . With arguments `input`, `other`, `out`.", "question_id": 52845}
{"snippet": "torch.gradient(input)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`.", "question_id": 52846}
{"snippet": "torch.gradient(input, spacing=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`.", "question_id": 52847}
{"snippet": "torch.gradient(input, dim=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `dim`.", "question_id": 52848}
{"snippet": "torch.gradient(input, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `edge_order`.", "question_id": 52849}
{"snippet": "torch.gradient(input, spacing=None, dim=None)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `dim`.", "question_id": 52850}
{"snippet": "torch.gradient(input, spacing=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `edge_order`.", "question_id": 52851}
{"snippet": "torch.gradient(input, dim=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `dim`, `edge_order`.", "question_id": 52852}
{"snippet": "torch.gradient(input, spacing=None, dim=None, edge_order=1)", "intent": "This function is analogous to NumPy \u2019 s gradient function . With arguments `input`, `spacing`, `dim`, `edge_order`.", "question_id": 52853}
{"snippet": "torch.cuda.set_rng_state_all(new_states)", "intent": "Sets the random number generator state of all devices . With arguments `new_states`.", "question_id": 52854}
{"snippet": "torch.nn.BatchNorm2d(num_features)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 52855}
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 52856}
{"snippet": "torch.nn.BatchNorm2d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 52857}
{"snippet": "torch.nn.BatchNorm2d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 52858}
{"snippet": "torch.nn.BatchNorm2d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 52859}
{"snippet": "torch.nn.BatchNorm2d(num_features, device=None)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 52860}
{"snippet": "torch.nn.BatchNorm2d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 52861}
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 52862}
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 52863}
{"snippet": "torch.nn.BatchNorm2d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 4D input ( a mini-batch of 2D inputs with additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 52864}
{"snippet": "torch.cuda.set_rng_state(new_state)", "intent": "Sets the random number generator state of the specified GPU . With arguments `new_state`.", "question_id": 52865}
{"snippet": "torch.cuda.set_rng_state(new_state, device='cuda')", "intent": "Sets the random number generator state of the specified GPU . With arguments `new_state`, `device`.", "question_id": 52866}
{"snippet": "torch.fft.rfftn(input)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` .", "question_id": 52867}
{"snippet": "torch.fft.rfftn(input, s=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`.", "question_id": 52868}
{"snippet": "torch.fft.rfftn(input, dim=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`.", "question_id": 52869}
{"snippet": "torch.fft.rfftn(input, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `norm`.", "question_id": 52870}
{"snippet": "torch.fft.rfftn(input, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `out`.", "question_id": 52871}
{"snippet": "torch.fft.rfftn(input, s=None, dim=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `dim`.", "question_id": 52872}
{"snippet": "torch.fft.rfftn(input, s=None, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `norm`.", "question_id": 52873}
{"snippet": "torch.fft.rfftn(input, s=None, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `s`, `out`.", "question_id": 52874}
{"snippet": "torch.fft.rfftn(input, dim=None, norm=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`, `norm`.", "question_id": 52875}
{"snippet": "torch.fft.rfftn(input, dim=None, out=None)", "intent": "Computes the N-dimensional discrete Fourier transform of real `input` . With arguments `dim`, `out`.", "question_id": 52876}
{"snippet": "torch.linalg.eigvalsh(A)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 52877}
{"snippet": "torch.linalg.eigvalsh(A, UPLO='L')", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`.", "question_id": 52878}
{"snippet": "torch.linalg.eigvalsh(A, out=None)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 52879}
{"snippet": "torch.linalg.eigvalsh(A, UPLO='L', out=None)", "intent": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `UPLO`, `out`.", "question_id": 52880}
{"snippet": "Tensor.svd()", "intent": "See torch.svd ( )", "question_id": 52881}
{"snippet": "Tensor.svd(some=True)", "intent": "See torch.svd ( ) With arguments `some`.", "question_id": 52882}
{"snippet": "Tensor.svd(compute_uv=True)", "intent": "See torch.svd ( ) With arguments `compute_uv`.", "question_id": 52883}
{"snippet": "Tensor.svd(some=True, compute_uv=True)", "intent": "See torch.svd ( ) With arguments `some`, `compute_uv`.", "question_id": 52884}
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 52885}
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 52886}
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 52887}
{"snippet": "torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an `input` signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 52888}
{"snippet": "Tensor.masked_scatter(mask, tensor)", "intent": "Out-of-place version of torch.Tensor.masked_scatter_ ( ) With arguments `mask`, `tensor`.", "question_id": 52889}
{"snippet": "torch.angle(input)", "intent": "Computes the element-wise angle ( in radians ) of the given `input` tensor .", "question_id": 52890}
{"snippet": "torch.angle(input, out=None)", "intent": "Computes the element-wise angle ( in radians ) of the given `input` tensor . With arguments `out`.", "question_id": 52891}
{"snippet": "torch.autograd.gradgradcheck(func, inputs)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 52892}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, grad_outputs=None)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`.", "question_id": 52893}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, eps=1e-06)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `eps`.", "question_id": 52894}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, atol=1e-05)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `atol`.", "question_id": 52895}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, rtol=0.001)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `rtol`.", "question_id": 52896}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, gen_non_contig_grad_outputs=False)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `gen_non_contig_grad_outputs`.", "question_id": 52897}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, raise_exception=True)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `raise_exception`.", "question_id": 52898}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, nondet_tol=0.0)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `nondet_tol`.", "question_id": 52899}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, check_undefined_grad=True)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_undefined_grad`.", "question_id": 52900}
{"snippet": "torch.autograd.gradgradcheck(func, inputs, check_grad_dtypes=False)", "intent": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t . tensors in `inputs` and `grad_outputs` that are of floating point or complex type and with requires_grad=True . With arguments `func`, `check_grad_dtypes`.", "question_id": 52901}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`.", "question_id": 52902}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, stride=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`.", "question_id": 52903}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, padding=0)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding`.", "question_id": 52904}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, dilation=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dilation`.", "question_id": 52905}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, groups=1)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `groups`.", "question_id": 52906}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, bias=True)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . The attributes that will be lazily initialized are weight and `bias` . With arguments `out_channels`, `kernel_size`.", "question_id": 52907}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, padding_mode='zeros')", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 52908}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, device=None)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `device`.", "question_id": 52909}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, dtype=None)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `dtype`.", "question_id": 52910}
{"snippet": "torch.nn.LazyConv1d(out_channels, kernel_size, stride=1, padding=0)", "intent": "A torch.nn.Conv1d module with lazy initialization of the in_channels argument of the Conv1d that is inferred from the input.size ( 1 ) . With arguments `out_channels`, `kernel_size`, `stride`, `padding`.", "question_id": 52911}
{"snippet": "lazy_conv1d.cls_to_become", "intent": "alias of torch.nn.modules.conv.Conv1d", "question_id": 52912}
{"snippet": "Tensor.square_()", "intent": "In-place version of square ( )", "question_id": 52913}
{"snippet": "torch.arcsin(input)", "intent": "Alias for torch.asin ( ) . With arguments `input`.", "question_id": 52914}
{"snippet": "torch.arcsin(input, out=None)", "intent": "Alias for torch.asin ( ) . With arguments `input`, `out`.", "question_id": 52915}
{"snippet": "torch.nanmedian(input)", "intent": "Returns the median of the values in `input` , ignoring NaN values .", "question_id": 52916}
{"snippet": "torch.isinf(input)", "intent": "Tests if each element of `input` is infinite ( positive or negative infinity ) or not .", "question_id": 52917}
{"snippet": "torch.cumprod(input, dim)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` .", "question_id": 52918}
{"snippet": "torch.cumprod(input, dim, dtype=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `dtype`.", "question_id": 52919}
{"snippet": "torch.cumprod(input, dim, out=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 52920}
{"snippet": "torch.cumprod(input, dim, dtype=None, out=None)", "intent": "Returns the cumulative product of elements of `input` in the dimension `dim` . With arguments `dtype`, `out`.", "question_id": 52921}
{"snippet": "torch.not_equal(input, other)", "intent": "Alias for torch.ne ( ) . With arguments `input`, `other`.", "question_id": 52922}
{"snippet": "torch.not_equal(input, other, out=None)", "intent": "Alias for torch.ne ( ) . With arguments `input`, `other`, `out`.", "question_id": 52923}
{"snippet": "torch.cuda.manual_seed_all(seed)", "intent": "Sets the `seed` for generating random numbers on all GPUs .", "question_id": 52924}
{"snippet": "torch.fix(input)", "intent": "Alias for torch.trunc ( ) With arguments `input`.", "question_id": 52925}
{"snippet": "torch.fix(input, out=None)", "intent": "Alias for torch.trunc ( ) With arguments `input`, `out`.", "question_id": 52926}
{"snippet": "Tensor.roll(shifts, dims)", "intent": "See torch.roll ( ) With arguments `shifts`, `dims`.", "question_id": 52927}
{"snippet": "torch.fft.ifftn(input)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` .", "question_id": 52928}
{"snippet": "torch.fft.ifftn(input, s=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`.", "question_id": 52929}
{"snippet": "torch.fft.ifftn(input, dim=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`.", "question_id": 52930}
{"snippet": "torch.fft.ifftn(input, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `norm`.", "question_id": 52931}
{"snippet": "torch.fft.ifftn(input, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `out`.", "question_id": 52932}
{"snippet": "torch.fft.ifftn(input, s=None, dim=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `dim`.", "question_id": 52933}
{"snippet": "torch.fft.ifftn(input, s=None, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `norm`.", "question_id": 52934}
{"snippet": "torch.fft.ifftn(input, s=None, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `s`, `out`.", "question_id": 52935}
{"snippet": "torch.fft.ifftn(input, dim=None, norm=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `norm`.", "question_id": 52936}
{"snippet": "torch.fft.ifftn(input, dim=None, out=None)", "intent": "Computes the N dimensional inverse discrete Fourier transform of `input` . With arguments `dim`, `out`.", "question_id": 52937}
{"snippet": "torch.dsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with three or more dimensions , into multiple tensors depthwise according to `indices_or_sections` .", "question_id": 52938}
{"snippet": "torch.cuda.memory_summary()", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` .", "question_id": 52939}
{"snippet": "torch.cuda.memory_summary(device=None)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` .", "question_id": 52940}
{"snippet": "torch.cuda.memory_summary(abbreviated=False)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` . With arguments `abbreviated`.", "question_id": 52941}
{"snippet": "torch.cuda.memory_summary(device=None, abbreviated=False)", "intent": "Returns a human-readable printout of the current memory allocator statistics for a given `device` . With arguments `abbreviated`.", "question_id": 52942}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`.", "question_id": 52943}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`.", "question_id": 52944}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`.", "question_id": 52945}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 52946}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `_random_samples`.", "question_id": 52947}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, output_ratio=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `output_ratio`.", "question_id": 52948}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `return_indices`.", "question_id": 52949}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_size=None, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_size`, `_random_samples`.", "question_id": 52950}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None, return_indices=False)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `return_indices`.", "question_id": 52951}
{"snippet": "torch.nn.FractionalMaxPool3d(kernel_size, output_ratio=None, _random_samples=None)", "intent": "Applies a 3D fractional max pooling over an input signal composed of several input planes . With arguments `kernel_size`, `output_ratio`, `_random_samples`.", "question_id": 52952}
{"snippet": "torch.sparse_coo_tensor(indices, values)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` .", "question_id": 52953}
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`.", "question_id": 52954}
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`.", "question_id": 52955}
{"snippet": "torch.sparse_coo_tensor(indices, values, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `device`.", "question_id": 52956}
{"snippet": "torch.sparse_coo_tensor(indices, values, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `requires_grad`.", "question_id": 52957}
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, dtype=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `dtype`.", "question_id": 52958}
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `device`.", "question_id": 52959}
{"snippet": "torch.sparse_coo_tensor(indices, values, size=None, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `size`, `requires_grad`.", "question_id": 52960}
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None, device=None)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`, `device`.", "question_id": 52961}
{"snippet": "torch.sparse_coo_tensor(indices, values, dtype=None, requires_grad=False)", "intent": "Constructs a sparse tensor in COO ( rdinate ) format with specified `values` at the given `indices` . With arguments `dtype`, `requires_grad`.", "question_id": 52962}
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`.", "question_id": 52963}
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `norm_type`.", "question_id": 52964}
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, error_if_nonfinite=False)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `error_if_nonfinite`.", "question_id": 52965}
{"snippet": "torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)", "intent": "Clips gradient norm of an iterable of `parameters` . With arguments `max_norm`, `norm_type`, `error_if_nonfinite`.", "question_id": 52966}
{"snippet": "Tensor.minimum(other)", "intent": "See torch.minimum ( ) With arguments `other`.", "question_id": 52967}
{"snippet": "torch.set_default_dtype(d)", "intent": "Sets the default floating point dtype to d. This dtype is : With arguments `d`.", "question_id": 52968}
{"snippet": "Tensor.arccosh()", "intent": "acosh ( ) - > Tensor", "question_id": 52969}
{"snippet": "torch.nn.utils.parametrize.cached()", "intent": "Context manager that enables the caching system within parametrizations registered with register_parametrization ( ) .", "question_id": 52970}
{"snippet": "Tensor.triangular_solve(A)", "intent": "See torch.triangular_solve ( ) With arguments `A`.", "question_id": 52971}
{"snippet": "Tensor.triangular_solve(A, upper=True)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`.", "question_id": 52972}
{"snippet": "Tensor.triangular_solve(A, transpose=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `transpose`.", "question_id": 52973}
{"snippet": "Tensor.triangular_solve(A, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `unitriangular`.", "question_id": 52974}
{"snippet": "Tensor.triangular_solve(A, upper=True, transpose=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `transpose`.", "question_id": 52975}
{"snippet": "Tensor.triangular_solve(A, upper=True, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `unitriangular`.", "question_id": 52976}
{"snippet": "Tensor.triangular_solve(A, transpose=False, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `transpose`, `unitriangular`.", "question_id": 52977}
{"snippet": "Tensor.triangular_solve(A, upper=True, transpose=False, unitriangular=False)", "intent": "See torch.triangular_solve ( ) With arguments `A`, `upper`, `transpose`, `unitriangular`.", "question_id": 52978}
{"snippet": "torch.nn.ZeroPad2d(padding)", "intent": "Pads the input tensor boundaries with zero . For N-dimensional `padding` , use torch.nn.functional.pad ( ) .", "question_id": 52979}
{"snippet": "Tensor.new_ones(size)", "intent": "Returns a Tensor of `size` size filled with 1 .", "question_id": 52980}
{"snippet": "Tensor.new_ones(size, dtype=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`.", "question_id": 52981}
{"snippet": "Tensor.new_ones(size, device=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `device`.", "question_id": 52982}
{"snippet": "Tensor.new_ones(size, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `requires_grad`.", "question_id": 52983}
{"snippet": "Tensor.new_ones(size, dtype=None, device=None)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `device`.", "question_id": 52984}
{"snippet": "Tensor.new_ones(size, dtype=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `requires_grad`.", "question_id": 52985}
{"snippet": "Tensor.new_ones(size, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `device`, `requires_grad`.", "question_id": 52986}
{"snippet": "Tensor.new_ones(size, dtype=None, device=None, requires_grad=False)", "intent": "Returns a Tensor of `size` size filled with 1 . With arguments `dtype`, `device`, `requires_grad`.", "question_id": 52987}
{"snippet": "Tensor.abs_()", "intent": "In-place version of abs ( )", "question_id": 52988}
{"snippet": "torch.cuda.get_device_name()", "intent": "Gets the name of a `device` .", "question_id": 52989}
{"snippet": "torch.cuda.get_device_name(device=None)", "intent": "Gets the name of a `device` .", "question_id": 52990}
{"snippet": "torch.symeig(input)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) .", "question_id": 52991}
{"snippet": "torch.symeig(input, eigenvectors=False)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) .", "question_id": 52992}
{"snippet": "torch.symeig(input, upper=True)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default .", "question_id": 52993}
{"snippet": "torch.symeig(input, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . With arguments `out`.", "question_id": 52994}
{"snippet": "torch.symeig(input, eigenvectors=False, upper=True)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default .", "question_id": 52995}
{"snippet": "torch.symeig(input, eigenvectors=False, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . With arguments `out`.", "question_id": 52996}
{"snippet": "torch.symeig(input, upper=True, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default . With arguments `out`.", "question_id": 52997}
{"snippet": "torch.symeig(input, eigenvectors=False, upper=True, out=None)", "intent": "This function returns eigenvalues and `eigenvectors` of a real symmetric or complex Hermitian matrix `input` or a batch thereof , represented by a namedtuple ( eigenvalues , eigenvectors ) . Since the input matrix input is supposed to be symmetric or Hermitian , only the `upper` triangular portion is used by default . With arguments `out`.", "question_id": 52998}
{"snippet": "torch.nn.AdaptiveAvgPool3d(output_size)", "intent": "Applies a 3D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 52999}
{"snippet": "torch.slogdet(input)", "intent": "Alias for torch.linalg.slogdet ( ) With arguments `input`.", "question_id": 53000}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 53001}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 53002}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 53003}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 53004}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 53005}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 53006}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `device`.", "question_id": 53007}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `dtype`.", "question_id": 53008}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True, device=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`, `device`.", "question_id": 53009}
{"snippet": "torch.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, affine=True, dtype=None)", "intent": "This is the quantized version of GroupNorm . With arguments `num_groups`, `num_channels`, `weight`, `bias`, `scale`, `zero_point`, `affine`, `dtype`.", "question_id": 53010}
{"snippet": "Tensor.eig()", "intent": "See torch.eig ( )", "question_id": 53011}
{"snippet": "Tensor.eig(eigenvectors=False)", "intent": "See torch.eig ( ) With arguments `eigenvectors`.", "question_id": 53012}
{"snippet": "torch.isposinf(input)", "intent": "Tests if each element of `input` is positive infinity or not .", "question_id": 53013}
{"snippet": "torch.isposinf(input, out=None)", "intent": "Tests if each element of `input` is positive infinity or not . With arguments `out`.", "question_id": 53014}
{"snippet": "torch.argsort(input)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`.", "question_id": 53015}
{"snippet": "torch.argsort(input, dim=- 1)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `dim`.", "question_id": 53016}
{"snippet": "torch.argsort(input, descending=False)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `descending`.", "question_id": 53017}
{"snippet": "torch.argsort(input, dim=- 1, descending=False)", "intent": "Returns the indices that sort a tensor along a given dimension in ascending order by value . With arguments `input`, `dim`, `descending`.", "question_id": 53018}
{"snippet": "Tensor.max()", "intent": "See torch.max ( )", "question_id": 53019}
{"snippet": "Tensor.max(dim=None)", "intent": "See torch.max ( ) With arguments `dim`.", "question_id": 53020}
{"snippet": "Tensor.max(keepdim=False)", "intent": "See torch.max ( ) With arguments `keepdim`.", "question_id": 53021}
{"snippet": "Tensor.max(dim=None, keepdim=False)", "intent": "See torch.max ( ) With arguments `dim`, `keepdim`.", "question_id": 53022}
{"snippet": "Optimizer.state_dict()", "intent": "Returns the state of the optimizer as a dict .", "question_id": 53023}
{"snippet": "Tensor.unique_consecutive()", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements .", "question_id": 53024}
{"snippet": "Tensor.unique_consecutive(return_inverse=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`.", "question_id": 53025}
{"snippet": "Tensor.unique_consecutive(return_counts=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_counts`.", "question_id": 53026}
{"snippet": "Tensor.unique_consecutive(dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `dim`.", "question_id": 53027}
{"snippet": "Tensor.unique_consecutive(return_inverse=False, return_counts=False)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `return_counts`.", "question_id": 53028}
{"snippet": "Tensor.unique_consecutive(return_inverse=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `dim`.", "question_id": 53029}
{"snippet": "Tensor.unique_consecutive(return_counts=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_counts`, `dim`.", "question_id": 53030}
{"snippet": "Tensor.unique_consecutive(return_inverse=False, return_counts=False, dim=None)", "intent": "Eliminates all but the first element from every consecutive group of equivalent elements . With arguments `return_inverse`, `return_counts`, `dim`.", "question_id": 53031}
{"snippet": "torch.cuda.comm.scatter(tensor)", "intent": "Scatters `tensor` across multiple GPUs .", "question_id": 53032}
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`.", "question_id": 53033}
{"snippet": "torch.cuda.comm.scatter(tensor, chunk_sizes=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `chunk_sizes`.", "question_id": 53034}
{"snippet": "torch.cuda.comm.scatter(tensor, dim=0)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `dim`.", "question_id": 53035}
{"snippet": "torch.cuda.comm.scatter(tensor, streams=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `streams`.", "question_id": 53036}
{"snippet": "torch.cuda.comm.scatter(tensor, out=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `out`.", "question_id": 53037}
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `chunk_sizes`.", "question_id": 53038}
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, dim=0)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `dim`.", "question_id": 53039}
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, streams=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `streams`.", "question_id": 53040}
{"snippet": "torch.cuda.comm.scatter(tensor, devices=None, out=None)", "intent": "Scatters `tensor` across multiple GPUs . With arguments `devices`, `out`.", "question_id": 53041}
{"snippet": "torch.masked_select(input, mask)", "intent": "Returns a new 1-D tensor which indexes the `input` tensor according to the boolean `mask` mask which is a BoolTensor .", "question_id": 53042}
{"snippet": "torch.masked_select(input, mask, out=None)", "intent": "Returns a new 1-D tensor which indexes the `input` tensor according to the boolean `mask` mask which is a BoolTensor . With arguments `out`.", "question_id": 53043}
{"snippet": "torch.pow(input, exponent)", "intent": "Takes the power of each element in `input` with `exponent` and returns a tensor with the result .", "question_id": 53044}
{"snippet": "torch.pow(input, exponent, out=None)", "intent": "Takes the power of each element in `input` with `exponent` and returns a tensor with the result . The returned tensor `out` is of the same shape as exponent", "question_id": 53045}
{"snippet": "Tensor.lgamma()", "intent": "See torch.lgamma ( )", "question_id": 53046}
{"snippet": "torch.nn.functional.log_softmax(input)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`.", "question_id": 53047}
{"snippet": "torch.nn.functional.log_softmax(input, dim=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`.", "question_id": 53048}
{"snippet": "torch.nn.functional.log_softmax(input, _stacklevel=3)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `_stacklevel`.", "question_id": 53049}
{"snippet": "torch.nn.functional.log_softmax(input, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dtype`.", "question_id": 53050}
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `_stacklevel`.", "question_id": 53051}
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `dtype`.", "question_id": 53052}
{"snippet": "torch.nn.functional.log_softmax(input, _stacklevel=3, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `_stacklevel`, `dtype`.", "question_id": 53053}
{"snippet": "torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)", "intent": "Applies a softmax followed by a logarithm . With arguments `input`, `dim`, `_stacklevel`, `dtype`.", "question_id": 53054}
{"snippet": "torch.fft.fftshift(input)", "intent": "Reorders n-dimensional FFT data , as provided by fftn ( ) , to have negative frequency terms first . With arguments `input`.", "question_id": 53055}
{"snippet": "torch.fft.fftshift(input, dim=None)", "intent": "Reorders n-dimensional FFT data , as provided by fftn ( ) , to have negative frequency terms first . Specifically , to input.shape [ `dim` ] // 2 in each selected dimension . With arguments `input`.", "question_id": 53056}
{"snippet": "torch.seed()", "intent": "Sets the seed for generating random numbers to a non-deterministic random number .", "question_id": 53057}
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`.", "question_id": 53058}
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, stride=None)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`.", "question_id": 53059}
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `ceil_mode`.", "question_id": 53060}
{"snippet": "torch.nn.LPPool1d(norm_type, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D power-average pooling over an input signal composed of several input planes . With arguments `norm_type`, `kernel_size`, `stride`, `ceil_mode`.", "question_id": 53061}
{"snippet": "Tensor.values()", "intent": "Return the values tensor of a sparse COO tensor .", "question_id": 53062}
{"snippet": "torch.hamming_window(window_length)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size .", "question_id": 53063}
{"snippet": "torch.hamming_window(window_length, periodic=True)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) .", "question_id": 53064}
{"snippet": "torch.hamming_window(window_length, alpha=0.54)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `alpha`.", "question_id": 53065}
{"snippet": "torch.hamming_window(window_length, beta=0.46)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `beta`.", "question_id": 53066}
{"snippet": "torch.hamming_window(window_length, dtype=None)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `dtype`.", "question_id": 53067}
{"snippet": "torch.hamming_window(window_length, layout=torch.strided)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `layout`.", "question_id": 53068}
{"snippet": "torch.hamming_window(window_length, device=None)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `device`.", "question_id": 53069}
{"snippet": "torch.hamming_window(window_length, requires_grad=False)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . With arguments `requires_grad`.", "question_id": 53070}
{"snippet": "torch.hamming_window(window_length, periodic=True, alpha=0.54)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `alpha`.", "question_id": 53071}
{"snippet": "torch.hamming_window(window_length, periodic=True, beta=0.46)", "intent": "Hamming window function . The input `window_length` is a positive integer controlling the returned window size . `periodic` flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft ( ) . With arguments `beta`.", "question_id": 53072}
{"snippet": "torch.float_power(input, exponent)", "intent": "Raises `input` to the power of `exponent` , elementwise , in double precision .", "question_id": 53073}
{"snippet": "torch.float_power(input, exponent, out=None)", "intent": "Raises `input` to the power of `exponent` , elementwise , in double precision . With arguments `out`.", "question_id": 53074}
{"snippet": "torch.linalg.cholesky_ex(A)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions .", "question_id": 53075}
{"snippet": "torch.linalg.cholesky_ex(A, check_errors=False)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `check_errors`.", "question_id": 53076}
{"snippet": "torch.linalg.cholesky_ex(A, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `out`.", "question_id": 53077}
{"snippet": "torch.linalg.cholesky_ex(A, check_errors=False, out=None)", "intent": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix . Also supports batches of matrices , and if `A` is a batch of matrices then the output has the same batch dimensions . With arguments `check_errors`, `out`.", "question_id": 53078}
{"snippet": "torch.autograd.functional.vjp(func, inputs)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`.", "question_id": 53079}
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`.", "question_id": 53080}
{"snippet": "torch.autograd.functional.vjp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 53081}
{"snippet": "torch.autograd.functional.vjp(func, inputs, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 53082}
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 53083}
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 53084}
{"snippet": "torch.autograd.functional.vjp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 53085}
{"snippet": "torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Jacobian of the given function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 53086}
{"snippet": "torch.smm(input, mat)", "intent": "Performs a matrix multiplication of the sparse matrix `input` with the dense matrix `mat` .", "question_id": 53087}
{"snippet": "torch.nn.utils.rnn.pack_sequence(sequences)", "intent": "Packs a list of variable length Tensors `sequences` should be a list of Tensors of size L x * , where L is the length of a sequence and * is any number of trailing dimensions , including zero .", "question_id": 53088}
{"snippet": "torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)", "intent": "Packs a list of variable length Tensors `sequences` should be a list of Tensors of size L x * , where L is the length of a sequence and * is any number of trailing dimensions , including zero . For unsorted sequences , use `enforce_sorted` = False .", "question_id": 53089}
{"snippet": "Tensor.greater_equal_(other)", "intent": "In-place version of greater_equal ( ) . With arguments `other`.", "question_id": 53090}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 53091}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 53092}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 53093}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 53094}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 53095}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 53096}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 53097}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 53098}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 53099}
{"snippet": "torch.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 53100}
{"snippet": "torch.autograd.functional.vhp(func, inputs)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`.", "question_id": 53101}
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`.", "question_id": 53102}
{"snippet": "torch.autograd.functional.vhp(func, inputs, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 53103}
{"snippet": "torch.autograd.functional.vhp(func, inputs, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 53104}
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`.", "question_id": 53105}
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `strict`.", "question_id": 53106}
{"snippet": "torch.autograd.functional.vhp(func, inputs, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 53107}
{"snippet": "torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False)", "intent": "Function that computes the dot product between a vector `v` and the Hessian of a given scalar function at the point given by the `inputs` . With arguments `func`, `create_graph`, `strict`.", "question_id": 53108}
{"snippet": "Tensor.clip()", "intent": "Alias for clamp ( ) .", "question_id": 53109}
{"snippet": "Tensor.clip(min=None)", "intent": "Alias for clamp ( ) . With arguments `min`.", "question_id": 53110}
{"snippet": "Tensor.clip(max=None)", "intent": "Alias for clamp ( ) . With arguments `max`.", "question_id": 53111}
{"snippet": "Tensor.clip(min=None, max=None)", "intent": "Alias for clamp ( ) . With arguments `min`, `max`.", "question_id": 53112}
{"snippet": "torch.take(input, index)", "intent": "Returns a new tensor with the elements of `input` at the given indices . With arguments `index`.", "question_id": 53113}
{"snippet": "torch.floor(input)", "intent": "Returns a new tensor with the floor of the elements of `input` , the largest integer less than or equal to each element .", "question_id": 53114}
{"snippet": "torch.floor(input, out=None)", "intent": "Returns a new tensor with the floor of the elements of `input` , the largest integer less than or equal to each element . With arguments `out`.", "question_id": 53115}
{"snippet": "Tensor.igammac_(other)", "intent": "In-place version of igammac ( ) With arguments `other`.", "question_id": 53116}
{"snippet": "torch.nn.parameter.Parameter()", "intent": "A kind of Tensor that is to be considered a module parameter .", "question_id": 53117}
{"snippet": "torch.nn.parameter.Parameter(data=None)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `data`.", "question_id": 53118}
{"snippet": "torch.nn.parameter.Parameter(requires_grad=True)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `requires_grad`.", "question_id": 53119}
{"snippet": "torch.nn.parameter.Parameter(data=None, requires_grad=True)", "intent": "A kind of Tensor that is to be considered a module parameter . With arguments `data`, `requires_grad`.", "question_id": 53120}
{"snippet": "torch.is_floating_point(input)", "intent": "Returns True if the data type of `input` is a floating point data type i.e. , one of torch.float64 , torch.float32 , torch.float16 , and torch.bfloat16 .", "question_id": 53121}
{"snippet": "Tensor.is_set_to(tensor)", "intent": "Returns True if both tensors are pointing to the exact same memory ( same storage , offset , size and stride ) . With arguments `tensor`.", "question_id": 53122}
{"snippet": "torch.nn.functional.hardsigmoid(input)", "intent": "Applies the element-wise function With arguments `input`.", "question_id": 53123}
{"snippet": "torch.nn.functional.hardsigmoid(input, inplace=False)", "intent": "Applies the element-wise function With arguments `input`, `inplace`.", "question_id": 53124}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 53125}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 53126}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 53127}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 53128}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 53129}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, bias=None)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 53130}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 53131}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, eps=1e-05)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `eps`.", "question_id": 53132}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, momentum=0.1)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `momentum`.", "question_id": 53133}
{"snippet": "torch.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, freeze_bn=False)", "intent": "A ConvBnReLU3d module is a module fused from Conv3d , BatchNorm3d and ReLU , attached with FakeQuantize modules for weight , used in quantization aware training . With arguments `in_channels`, `out_channels`, `kernel_size`, `freeze_bn`.", "question_id": 53134}
{"snippet": "Tensor.index_add(tensor1, dim, index, tensor2)", "intent": "Out-of-place version of torch.Tensor.index_add_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_add_ ( ) . With arguments `dim`, `index`, `tensor2`.", "question_id": 53135}
{"snippet": "torch.reshape(input, shape)", "intent": "Returns a tensor with the same data and number of elements as `input` , but with the specified `shape` .", "question_id": 53136}
{"snippet": "torch.atleast_2d(*tensors)", "intent": "Returns a 2-dimensional view of each input tensor with zero dimensions . With arguments `*tensors`.", "question_id": 53137}
{"snippet": "Tensor.numpy()", "intent": "Returns self tensor as a NumPy ndarray .", "question_id": 53138}
{"snippet": "Tensor.sub_(other)", "intent": "In-place version of sub ( ) With arguments `other`.", "question_id": 53139}
{"snippet": "Tensor.sub_(other, alpha=1)", "intent": "In-place version of sub ( ) With arguments `other`, `alpha`.", "question_id": 53140}
{"snippet": "Tensor.divide(value)", "intent": "See torch.divide ( ) With arguments `value`.", "question_id": 53141}
{"snippet": "Tensor.divide(value, rounding_mode=None)", "intent": "See torch.divide ( ) With arguments `value`, `rounding_mode`.", "question_id": 53142}
{"snippet": "torch.amin(input, dim)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` .", "question_id": 53143}
{"snippet": "torch.amin(input, dim, keepdim=False)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is True , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 .", "question_id": 53144}
{"snippet": "torch.amin(input, dim, out=None)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . With arguments `out`.", "question_id": 53145}
{"snippet": "torch.amin(input, dim, keepdim=False, out=None)", "intent": "Returns the minimum value of each slice of the `input` tensor in the given dimension ( s ) `dim` . If `keepdim` is True , the output tensors are of the same size as input except in the dimension ( s ) dim where they are of size 1 . With arguments `out`.", "question_id": 53146}
{"snippet": "Tensor.inverse()", "intent": "See torch.inverse ( )", "question_id": 53147}
{"snippet": "torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)", "intent": "Applies a 2D adaptive max pooling over an input signal composed of several input planes . With arguments `*args`, `**kwargs`.", "question_id": 53148}
{"snippet": "torch.linspace(start, end, steps)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive .", "question_id": 53149}
{"snippet": "torch.linspace(start, end, steps, out=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`.", "question_id": 53150}
{"snippet": "torch.linspace(start, end, steps, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `dtype`.", "question_id": 53151}
{"snippet": "torch.linspace(start, end, steps, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `layout`.", "question_id": 53152}
{"snippet": "torch.linspace(start, end, steps, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `device`.", "question_id": 53153}
{"snippet": "torch.linspace(start, end, steps, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `requires_grad`.", "question_id": 53154}
{"snippet": "torch.linspace(start, end, steps, out=None, dtype=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `dtype`.", "question_id": 53155}
{"snippet": "torch.linspace(start, end, steps, out=None, layout=torch.strided)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `layout`.", "question_id": 53156}
{"snippet": "torch.linspace(start, end, steps, out=None, device=None)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `device`.", "question_id": 53157}
{"snippet": "torch.linspace(start, end, steps, out=None, requires_grad=False)", "intent": "Creates a one-dimensional tensor of size `steps` whose values are evenly spaced from `start` to `end` , inclusive . With arguments `out`, `requires_grad`.", "question_id": 53158}
{"snippet": "torch.set_default_tensor_type(t)", "intent": "Sets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor ( ) . With arguments `t`.", "question_id": 53159}
{"snippet": "Tensor.i0_()", "intent": "In-place version of i0 ( )", "question_id": 53160}
{"snippet": "Tensor.baddbmm(batch1, batch2)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 53161}
{"snippet": "Tensor.baddbmm(batch1, batch2, beta=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 53162}
{"snippet": "Tensor.baddbmm(batch1, batch2, alpha=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 53163}
{"snippet": "Tensor.baddbmm(batch1, batch2, beta=1, alpha=1)", "intent": "See torch.baddbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 53164}
{"snippet": "torch.index_select(input, dim, index)", "intent": "Returns a new tensor which indexes the `input` tensor along dimension `dim` using the entries in `index` which is a LongTensor .", "question_id": 53165}
{"snippet": "torch.index_select(input, dim, index, out=None)", "intent": "Returns a new tensor which indexes the `input` tensor along dimension `dim` using the entries in `index` which is a LongTensor . With arguments `out`.", "question_id": 53166}
{"snippet": "torch.nn.functional.nll_loss(input, target)", "intent": "The negative log likelihood loss . With arguments `input`, `target`.", "question_id": 53167}
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`.", "question_id": 53168}
{"snippet": "torch.nn.functional.nll_loss(input, target, size_average=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `size_average`.", "question_id": 53169}
{"snippet": "torch.nn.functional.nll_loss(input, target, ignore_index=- 100)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `ignore_index`.", "question_id": 53170}
{"snippet": "torch.nn.functional.nll_loss(input, target, reduce=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `reduce`.", "question_id": 53171}
{"snippet": "torch.nn.functional.nll_loss(input, target, reduction='mean')", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `reduction`.", "question_id": 53172}
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, size_average=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `size_average`.", "question_id": 53173}
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, ignore_index=- 100)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `ignore_index`.", "question_id": 53174}
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, reduce=None)", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `reduce`.", "question_id": 53175}
{"snippet": "torch.nn.functional.nll_loss(input, target, weight=None, reduction='mean')", "intent": "The negative log likelihood loss . With arguments `input`, `target`, `weight`, `reduction`.", "question_id": 53176}
{"snippet": "torch.cuda.comm.gather(tensors)", "intent": "Gathers `tensors` from multiple GPU devices .", "question_id": 53177}
{"snippet": "torch.cuda.comm.gather(tensors, dim=0)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`.", "question_id": 53178}
{"snippet": "torch.cuda.comm.gather(tensors, destination=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `destination`.", "question_id": 53179}
{"snippet": "torch.cuda.comm.gather(tensors, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `out`.", "question_id": 53180}
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, destination=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `destination`.", "question_id": 53181}
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `out`.", "question_id": 53182}
{"snippet": "torch.cuda.comm.gather(tensors, destination=None, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `destination`, `out`.", "question_id": 53183}
{"snippet": "torch.cuda.comm.gather(tensors, dim=0, destination=None, out=None)", "intent": "Gathers `tensors` from multiple GPU devices . With arguments `dim`, `destination`, `out`.", "question_id": 53184}
{"snippet": "Tensor.double()", "intent": "self.double ( ) is equivalent to self.to ( torch.float64 ) .", "question_id": 53185}
{"snippet": "Tensor.double(memory_format=torch.preserve_format)", "intent": "self.double ( ) is equivalent to self.to ( torch.float64 ) . With arguments `memory_format`.", "question_id": 53186}
{"snippet": "Tensor.geqrf()", "intent": "See torch.geqrf ( )", "question_id": 53187}
{"snippet": "torch.is_warn_always_enabled()", "intent": "Returns True if the global warn_always flag is turned on .", "question_id": 53188}
{"snippet": "torch.nn.intrinsic.ConvBn3d(conv, bn)", "intent": "This is a sequential container which calls the Conv 3d and Batch Norm 3d modules . With arguments `conv`, `bn`.", "question_id": 53189}
{"snippet": "Tensor.logical_and_()", "intent": "In-place version of logical_and ( )", "question_id": 53190}
{"snippet": "Tensor.log()", "intent": "See torch.log ( )", "question_id": 53191}
{"snippet": "torch.nn.MSELoss()", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy .", "question_id": 53192}
{"snippet": "torch.nn.MSELoss(size_average=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `size_average`.", "question_id": 53193}
{"snippet": "torch.nn.MSELoss(reduce=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `reduce`.", "question_id": 53194}
{"snippet": "torch.nn.MSELoss(reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as :", "question_id": 53195}
{"snippet": "torch.nn.MSELoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . With arguments `size_average`, `reduce`.", "question_id": 53196}
{"snippet": "torch.nn.MSELoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 53197}
{"snippet": "torch.nn.MSELoss(reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `reduce`.", "question_id": 53198}
{"snippet": "torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')", "intent": "Creates a criterion that measures the mean squared error ( squared L2 norm ) between each element in the input xxx and target yyy . with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`, `reduce`.", "question_id": 53199}
{"snippet": "torch.roll(input, shifts)", "intent": "Roll the tensor along the given dimension ( s ) . With arguments `input`, `shifts`.", "question_id": 53200}
{"snippet": "torch.roll(input, shifts, dims=None)", "intent": "Roll the tensor along the given dimension ( s ) . With arguments `input`, `shifts`, `dims`.", "question_id": 53201}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`.", "question_id": 53202}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`.", "question_id": 53203}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, full=False)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `full`.", "question_id": 53204}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, size_average=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `size_average`.", "question_id": 53205}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, eps=1e-08)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `eps`.", "question_id": 53206}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, reduce=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `reduce`.", "question_id": 53207}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, reduction='mean')", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `reduction`.", "question_id": 53208}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `full`.", "question_id": 53209}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, size_average=None)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `size_average`.", "question_id": 53210}
{"snippet": "torch.nn.functional.poisson_nll_loss(input, target, log_input=True, eps=1e-08)", "intent": "Poisson negative log likelihood loss . With arguments `input`, `target`, `log_input`, `eps`.", "question_id": 53211}
{"snippet": "torch.nn.AvgPool3d(kernel_size)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as :", "question_id": 53212}
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be :", "question_id": 53213}
{"snippet": "torch.nn.AvgPool3d(kernel_size, padding=0)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on all three sides for padding number of points .", "question_id": 53214}
{"snippet": "torch.nn.AvgPool3d(kernel_size, ceil_mode=False)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 53215}
{"snippet": "torch.nn.AvgPool3d(kernel_size, count_include_pad=True)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `count_include_pad`.", "question_id": 53216}
{"snippet": "torch.nn.AvgPool3d(kernel_size, divisor_override=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `divisor_override`.", "question_id": 53217}
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, padding=0)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : If `padding` is non-zero , then the input is implicitly zero-padded on all three sides for padding number of points .", "question_id": 53218}
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `ceil_mode`.", "question_id": 53219}
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, count_include_pad=True)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `count_include_pad`.", "question_id": 53220}
{"snippet": "torch.nn.AvgPool3d(kernel_size, stride=None, divisor_override=None)", "intent": "Applies a 3D average pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` can either be : With arguments `divisor_override`.", "question_id": 53221}
{"snippet": "Tensor.cpu()", "intent": "Returns a copy of this object in CPU memory .", "question_id": 53222}
{"snippet": "Tensor.cpu(memory_format=torch.preserve_format)", "intent": "Returns a copy of this object in CPU memory . With arguments `memory_format`.", "question_id": 53223}
{"snippet": "torch.orgqr(input, tau)", "intent": "Alias for torch.linalg.householder_product ( ) . With arguments `input`, `tau`.", "question_id": 53224}
{"snippet": "torch.nn.MaxPool3d(kernel_size)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as :", "question_id": 53225}
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be :", "question_id": 53226}
{"snippet": "torch.nn.MaxPool3d(kernel_size, padding=0)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 53227}
{"snippet": "torch.nn.MaxPool3d(kernel_size, dilation=1)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : `dilation` controls the spacing between the kernel points .", "question_id": 53228}
{"snippet": "torch.nn.MaxPool3d(kernel_size, return_indices=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `return_indices`.", "question_id": 53229}
{"snippet": "torch.nn.MaxPool3d(kernel_size, ceil_mode=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : With arguments `ceil_mode`.", "question_id": 53230}
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, padding=0)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : If `padding` is non-zero , then the input is implicitly zero-padded on both sides for padding number of points .", "question_id": 53231}
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, dilation=1)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : `dilation` controls the spacing between the kernel points .", "question_id": 53232}
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, return_indices=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `return_indices`.", "question_id": 53233}
{"snippet": "torch.nn.MaxPool3d(kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 3D max pooling over an input signal composed of several input planes . In the simplest case , the output value of the layer with input size ( N , C , D , H , W ) ( N , C , D , H , W ) ( N , C , D , H , W ) , output ( N , C , Dout , Hout , Wout ) ( N , C , D_ { out } , H_ { out } , W_ { out } ) ( N , C , Dout\u200b , Hout\u200b , Wout\u200b ) and `kernel_size` ( kD , kH , kW ) ( kD , kH , kW ) ( kD , kH , kW ) can be precisely described as : The parameters kernel_size , `stride` , padding , dilation can either be : With arguments `ceil_mode`.", "question_id": 53234}
{"snippet": "Tensor.arctan_()", "intent": "In-place version of arctan ( )", "question_id": 53235}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`.", "question_id": 53236}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `eps`.", "question_id": 53237}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, momentum=0.1)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `momentum`.", "question_id": 53238}
{"snippet": "torch.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05, momentum=0.1)", "intent": "A BNReLU3d module is a fused module of BatchNorm3d and ReLU With arguments `num_features`, `eps`, `momentum`.", "question_id": 53239}
{"snippet": "Tensor.le_(other)", "intent": "In-place version of le ( ) . With arguments `other`.", "question_id": 53240}
{"snippet": "Tensor.addbmm(batch1, batch2)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`.", "question_id": 53241}
{"snippet": "Tensor.addbmm(batch1, batch2, beta=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `beta`.", "question_id": 53242}
{"snippet": "Tensor.addbmm(batch1, batch2, alpha=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `alpha`.", "question_id": 53243}
{"snippet": "Tensor.addbmm(batch1, batch2, beta=1, alpha=1)", "intent": "See torch.addbmm ( ) With arguments `batch1`, `batch2`, `beta`, `alpha`.", "question_id": 53244}
{"snippet": "torch.vsplit(input, indices_or_sections)", "intent": "Splits `input` , a tensor with two or more dimensions , into multiple tensors vertically according to `indices_or_sections` .", "question_id": 53245}
{"snippet": "torch.nn.utils.remove_spectral_norm(module)", "intent": "Removes the spectral normalization reparameterization from a `module` .", "question_id": 53246}
{"snippet": "torch.nn.utils.remove_spectral_norm(module, name='weight')", "intent": "Removes the spectral normalization reparameterization from a `module` . With arguments `name`.", "question_id": 53247}
{"snippet": "torch.cross(input, other)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` .", "question_id": 53248}
{"snippet": "torch.cross(input, other, dim=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` .", "question_id": 53249}
{"snippet": "torch.cross(input, other, out=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` . With arguments `out`.", "question_id": 53250}
{"snippet": "torch.cross(input, other, dim=None, out=None)", "intent": "Returns the cross product of vectors in dimension `dim` of `input` and `other` . With arguments `out`.", "question_id": 53251}
{"snippet": "torch.nn.functional.celu(input)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`.", "question_id": 53252}
{"snippet": "torch.nn.functional.celu(input, alpha=1.)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `alpha`.", "question_id": 53253}
{"snippet": "torch.nn.functional.celu(input, inplace=False)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `inplace`.", "question_id": 53254}
{"snippet": "torch.nn.functional.celu(input, alpha=1., inplace=False)", "intent": "Applies element-wise , CELU ( x ) =max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x/\u03b1 ) \u22121 ) ) \\text { CELU } ( x ) = \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x/\\alpha ) - 1 ) ) CELU ( x ) =max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x/\u03b1 ) \u22121 ) ) . With arguments `input`, `alpha`, `inplace`.", "question_id": 53255}
{"snippet": "torch.autograd.enable_grad", "intent": "Context-manager that enables gradient calculation.", "question_id": 53256}
{"snippet": "Tensor.polygamma_(n)", "intent": "In-place version of polygamma ( ) With arguments `n`.", "question_id": 53257}
{"snippet": "torch.mode(input)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e .", "question_id": 53258}
{"snippet": "torch.mode(input, dim=- 1)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e .", "question_id": 53259}
{"snippet": "torch.mode(input, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 .", "question_id": 53260}
{"snippet": "torch.mode(input, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . With arguments `out`.", "question_id": 53261}
{"snippet": "torch.mode(input, dim=- 1, keepdim=False)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 .", "question_id": 53262}
{"snippet": "torch.mode(input, dim=- 1, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . With arguments `out`.", "question_id": 53263}
{"snippet": "torch.mode(input, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 53264}
{"snippet": "torch.mode(input, dim=- 1, keepdim=False, out=None)", "intent": "Returns a namedtuple ( values , indices ) where values is the mode value of each row of the `input` tensor in the given dimension `dim` , i.e . If `keepdim` is True , the output tensors are of the same size as input except in the dimension dim where they are of size 1 . With arguments `out`.", "question_id": 53265}
{"snippet": "Tensor.addmv(mat, vec)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`.", "question_id": 53266}
{"snippet": "Tensor.addmv(mat, vec, beta=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `beta`.", "question_id": 53267}
{"snippet": "Tensor.addmv(mat, vec, alpha=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `alpha`.", "question_id": 53268}
{"snippet": "Tensor.addmv(mat, vec, beta=1, alpha=1)", "intent": "See torch.addmv ( ) With arguments `mat`, `vec`, `beta`, `alpha`.", "question_id": 53269}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`.", "question_id": 53270}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `stride`.", "question_id": 53271}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding`.", "question_id": 53272}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, output_padding=0)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `output_padding`.", "question_id": 53273}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, groups=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `groups`.", "question_id": 53274}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, bias=True)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `bias`.", "question_id": 53275}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, dilation=1)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dilation`.", "question_id": 53276}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, padding_mode='zeros')", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `padding_mode`.", "question_id": 53277}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, device=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `device`.", "question_id": 53278}
{"snippet": "torch.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, dtype=None)", "intent": "Applies a 3D transposed convolution operator over an input image composed of several input planes . With arguments `in_channels`, `out_channels`, `kernel_size`, `dtype`.", "question_id": 53279}
{"snippet": "torch.nn.CTCLoss()", "intent": "The Connectionist Temporal Classification loss .", "question_id": 53280}
{"snippet": "torch.nn.CTCLoss(blank=0)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`.", "question_id": 53281}
{"snippet": "torch.nn.CTCLoss(reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `reduction`.", "question_id": 53282}
{"snippet": "torch.nn.CTCLoss(zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `zero_infinity`.", "question_id": 53283}
{"snippet": "torch.nn.CTCLoss(blank=0, reduction='mean')", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `reduction`.", "question_id": 53284}
{"snippet": "torch.nn.CTCLoss(blank=0, zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `zero_infinity`.", "question_id": 53285}
{"snippet": "torch.nn.CTCLoss(reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `reduction`, `zero_infinity`.", "question_id": 53286}
{"snippet": "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)", "intent": "The Connectionist Temporal Classification loss . With arguments `blank`, `reduction`, `zero_infinity`.", "question_id": 53287}
{"snippet": "Tensor.ndimension()", "intent": "Alias for dim ( )", "question_id": 53288}
{"snippet": "torch.nn.quantized.Hardswish(scale, zero_point)", "intent": "This is the quantized version of Hardswish . With arguments `scale`, `zero_point`.", "question_id": 53289}
{"snippet": "torch.nn.Softmax()", "intent": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0,1 ] and sum to 1 .", "question_id": 53290}
{"snippet": "torch.nn.Softmax(dim=None)", "intent": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [ 0,1 ] and sum to 1 . With arguments `dim`.", "question_id": 53291}
{"snippet": "torch.cuda.comm.broadcast_coalesced(tensors, devices)", "intent": "Broadcasts a sequence `tensors` to the specified GPUs . With arguments `devices`.", "question_id": 53292}
{"snippet": "torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)", "intent": "Broadcasts a sequence `tensors` to the specified GPUs . With arguments `devices`, `buffer_size`.", "question_id": 53293}
{"snippet": "torch.nn.utils.parametrize.remove_parametrizations(module, tensor_name)", "intent": "Removes the parametrizations on a tensor in a `module` . With arguments `tensor_name`.", "question_id": 53294}
{"snippet": "torch.nn.utils.parametrize.remove_parametrizations(module, tensor_name, leave_parametrized=True)", "intent": "Removes the parametrizations on a tensor in a `module` . With arguments `tensor_name`, `leave_parametrized`.", "question_id": 53295}
{"snippet": "torch.flatten(input)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor .", "question_id": 53296}
{"snippet": "torch.flatten(input, start_dim=0)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 53297}
{"snippet": "torch.flatten(input, end_dim=- 1)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 53298}
{"snippet": "torch.flatten(input, start_dim=0, end_dim=- 1)", "intent": "Flattens `input` by reshaping it into a one-dimensional tensor . If `start_dim` or `end_dim` are passed , only dimensions starting with start_dim and ending with end_dim are flattened .", "question_id": 53299}
{"snippet": "torch.nn.AdaptiveAvgPool2d(output_size)", "intent": "Applies a 2D adaptive average pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 53300}
{"snippet": "Tensor.igammac(other)", "intent": "See torch.igammac ( ) With arguments `other`.", "question_id": 53301}
{"snippet": "Tensor.abs()", "intent": "See torch.abs ( )", "question_id": 53302}
{"snippet": "torch.set_printoptions()", "intent": "Set options for printing .", "question_id": 53303}
{"snippet": "torch.set_printoptions(precision=None)", "intent": "Set options for printing . With arguments `precision`.", "question_id": 53304}
{"snippet": "torch.set_printoptions(threshold=None)", "intent": "Set options for printing . With arguments `threshold`.", "question_id": 53305}
{"snippet": "torch.set_printoptions(edgeitems=None)", "intent": "Set options for printing . With arguments `edgeitems`.", "question_id": 53306}
{"snippet": "torch.set_printoptions(linewidth=None)", "intent": "Set options for printing . With arguments `linewidth`.", "question_id": 53307}
{"snippet": "torch.set_printoptions(profile=None)", "intent": "Set options for printing . With arguments `profile`.", "question_id": 53308}
{"snippet": "torch.set_printoptions(sci_mode=None)", "intent": "Set options for printing . With arguments `sci_mode`.", "question_id": 53309}
{"snippet": "torch.set_printoptions(precision=None, threshold=None)", "intent": "Set options for printing . With arguments `precision`, `threshold`.", "question_id": 53310}
{"snippet": "torch.set_printoptions(precision=None, edgeitems=None)", "intent": "Set options for printing . With arguments `precision`, `edgeitems`.", "question_id": 53311}
{"snippet": "torch.set_printoptions(precision=None, linewidth=None)", "intent": "Set options for printing . With arguments `precision`, `linewidth`.", "question_id": 53312}
{"snippet": "torch.inner(input, other)", "intent": "Computes the dot product for 1D tensors . For higher dimensions , sums the product of elements from `input` and `other` along their last dimension .", "question_id": 53313}
{"snippet": "torch.inner(input, other, out=None)", "intent": "Computes the dot product for 1D tensors . For higher dimensions , sums the product of elements from `input` and `other` along their last dimension . With arguments `out`.", "question_id": 53314}
{"snippet": "Tensor.triu_()", "intent": "In-place version of triu ( )", "question_id": 53315}
{"snippet": "Tensor.triu_(k=0)", "intent": "In-place version of triu ( ) With arguments `k`.", "question_id": 53316}
{"snippet": "torch.nn.functional.leaky_relu(input)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`.", "question_id": 53317}
{"snippet": "torch.nn.functional.leaky_relu(input, negative_slope=0.01)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `negative_slope`.", "question_id": 53318}
{"snippet": "torch.nn.functional.leaky_relu(input, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `inplace`.", "question_id": 53319}
{"snippet": "torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False)", "intent": "Applies element-wise , LeakyReLU ( x ) =max\u2061 ( 0 , x ) +negative_slope\u2217min\u2061 ( 0 , x ) \\text { LeakyReLU } ( x ) = \\max ( 0 , x ) + \\text { negative\\_slope } * \\min ( 0 , x ) LeakyReLU ( x ) =max ( 0 , x ) +negative_slope\u2217min ( 0 , x ) With arguments `input`, `negative_slope`, `inplace`.", "question_id": 53320}
{"snippet": "torch.empty_like(input)", "intent": "Returns an uninitialized tensor with the same size as `input` .", "question_id": 53321}
{"snippet": "torch.empty_like(input, dtype=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`.", "question_id": 53322}
{"snippet": "torch.empty_like(input, layout=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `layout`.", "question_id": 53323}
{"snippet": "torch.empty_like(input, device=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `device`.", "question_id": 53324}
{"snippet": "torch.empty_like(input, requires_grad=False)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `requires_grad`.", "question_id": 53325}
{"snippet": "torch.empty_like(input, memory_format=torch.preserve_format)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `memory_format`.", "question_id": 53326}
{"snippet": "torch.empty_like(input, dtype=None, layout=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `layout`.", "question_id": 53327}
{"snippet": "torch.empty_like(input, dtype=None, device=None)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `device`.", "question_id": 53328}
{"snippet": "torch.empty_like(input, dtype=None, requires_grad=False)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `requires_grad`.", "question_id": 53329}
{"snippet": "torch.empty_like(input, dtype=None, memory_format=torch.preserve_format)", "intent": "Returns an uninitialized tensor with the same size as `input` . With arguments `dtype`, `memory_format`.", "question_id": 53330}
{"snippet": "Tensor.subtract(other)", "intent": "See torch.subtract ( ) . With arguments `other`.", "question_id": 53331}
{"snippet": "Tensor.subtract(other, alpha=1)", "intent": "See torch.subtract ( ) . With arguments `other`, `alpha`.", "question_id": 53332}
{"snippet": "Tensor.sparse_dim()", "intent": "Return the number of sparse dimensions in a sparse tensor self .", "question_id": 53333}
{"snippet": "Tensor.geometric_(p)", "intent": "Fills self tensor with elements drawn from the geometric distribution : With arguments `p`.", "question_id": 53334}
{"snippet": "Tensor.geometric_(p, generator=None)", "intent": "Fills self tensor with elements drawn from the geometric distribution : With arguments `p`, `generator`.", "question_id": 53335}
{"snippet": "torch.nn.functional.selu(input)", "intent": "Applies element-wise , SELU ( x ) =scale\u2217 ( max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) ) \\text { SELU } ( x ) = scale * ( \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ) SELU ( x ) =scale\u2217 ( max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) ) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . With arguments `input`.", "question_id": 53336}
{"snippet": "torch.nn.functional.selu(input, inplace=False)", "intent": "Applies element-wise , SELU ( x ) =scale\u2217 ( max\u2061 ( 0 , x ) +min\u2061 ( 0 , \u03b1\u2217 ( exp\u2061 ( x ) \u22121 ) ) ) \\text { SELU } ( x ) = scale * ( \\max ( 0 , x ) + \\min ( 0 , \\alpha * ( \\exp ( x ) - 1 ) ) ) SELU ( x ) =scale\u2217 ( max ( 0 , x ) +min ( 0 , \u03b1\u2217 ( exp ( x ) \u22121 ) ) ) , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946 . With arguments `input`, `inplace`.", "question_id": 53337}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`.", "question_id": 53338}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`.", "question_id": 53339}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`.", "question_id": 53340}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `reduce`.", "question_id": 53341}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `reduction`.", "question_id": 53342}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `size_average`.", "question_id": 53343}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `reduce`.", "question_id": 53344}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, weight=None, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `weight`, `reduction`.", "question_id": 53345}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None, reduce=None)", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`, `reduce`.", "question_id": 53346}
{"snippet": "torch.nn.functional.binary_cross_entropy(input, target, size_average=None, reduction='mean')", "intent": "Function that measures the Binary Cross Entropy between the `target` and the output . With arguments `input`, `size_average`, `reduction`.", "question_id": 53347}
{"snippet": "torch.nn.quantized.functional.hardsigmoid(input)", "intent": "This is the quantized version of hardsigmoid ( ) . With arguments `input`.", "question_id": 53348}
{"snippet": "torch.cuda.reset_peak_memory_stats()", "intent": "Resets the \u201c peak \u201d stats tracked by the CUDA memory allocator .", "question_id": 53349}
{"snippet": "torch.cuda.reset_peak_memory_stats(device=None)", "intent": "Resets the \u201c peak \u201d stats tracked by the CUDA memory allocator . With arguments `device`.", "question_id": 53350}
{"snippet": "Tensor.true_divide_(value)", "intent": "In-place version of true_divide_ ( ) With arguments `value`.", "question_id": 53351}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`.", "question_id": 53352}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`.", "question_id": 53353}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `eps`.", "question_id": 53354}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `reduction`.", "question_id": 53355}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, eps=1e-06)", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `eps`.", "question_id": 53356}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `reduction`.", "question_id": 53357}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `eps`, `reduction`.", "question_id": 53358}
{"snippet": "torch.nn.functional.gaussian_nll_loss(input, target, var, full=False, eps=1e-06, reduction='mean')", "intent": "Gaussian negative log likelihood loss . With arguments `input`, `target`, `var`, `full`, `eps`, `reduction`.", "question_id": 53359}
{"snippet": "torch.nn.Dropout3d()", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) .", "question_id": 53360}
{"snippet": "torch.nn.Dropout3d(p=0.5)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution .", "question_id": 53361}
{"snippet": "torch.nn.Dropout3d(inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . With arguments `inplace`.", "question_id": 53362}
{"snippet": "torch.nn.Dropout3d(p=0.5, inplace=False)", "intent": "Randomly zero out entire channels ( a channel is a 3D feature map , e.g. , the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input [ i , j ] \\text { input } [ i , j ] input [ i , j ] ) . Each channel will be zeroed out independently on every forward call with probability `p` using samples from a Bernoulli distribution . With arguments `inplace`.", "question_id": 53363}
{"snippet": "torch.save(obj, f)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`.", "question_id": 53364}
{"snippet": "torch.save(obj, f, pickle_module=pickle)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`.", "question_id": 53365}
{"snippet": "torch.save(obj, f, pickle_protocol=DEFAULT_PROTOCOL)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_protocol`.", "question_id": 53366}
{"snippet": "torch.save(obj, f, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `_use_new_zipfile_serialization`.", "question_id": 53367}
{"snippet": "torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `pickle_protocol`.", "question_id": 53368}
{"snippet": "torch.save(obj, f, pickle_module=pickle, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `_use_new_zipfile_serialization`.", "question_id": 53369}
{"snippet": "torch.save(obj, f, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_protocol`, `_use_new_zipfile_serialization`.", "question_id": 53370}
{"snippet": "torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)", "intent": "Saves an object to a disk file . With arguments `obj`, `f`, `pickle_module`, `pickle_protocol`, `_use_new_zipfile_serialization`.", "question_id": 53371}
{"snippet": "torch.searchsorted(sorted_sequence, values)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved .", "question_id": 53372}
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out_int32`.", "question_id": 53373}
{"snippet": "torch.searchsorted(sorted_sequence, values, right=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed .", "question_id": 53374}
{"snippet": "torch.searchsorted(sorted_sequence, values, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out`.", "question_id": 53375}
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, right=False)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out_int32`.", "question_id": 53376}
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . With arguments `out_int32`, `out`.", "question_id": 53377}
{"snippet": "torch.searchsorted(sorted_sequence, values, right=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out`.", "question_id": 53378}
{"snippet": "torch.searchsorted(sorted_sequence, values, out_int32=False, right=False, out=None)", "intent": "Find the indices from the innermost dimension of `sorted_sequence` such that , if the corresponding `values` in values were inserted before the indices , the order of the corresponding innermost dimension within sorted_sequence would be preserved . If `right` is False ( default ) , then the left boundary of sorted_sequence is closed . With arguments `out_int32`, `out`.", "question_id": 53379}
{"snippet": "torch.kron(input, other)", "intent": "Computes the Kronecker product , denoted by \u2297\\otimes\u2297 , of `input` and `other` .", "question_id": 53380}
{"snippet": "torch.kron(input, other, out=None)", "intent": "Computes the Kronecker product , denoted by \u2297\\otimes\u2297 , of `input` and `other` . With arguments `out`.", "question_id": 53381}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 53382}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 53383}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`.", "question_id": 53384}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 53385}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 53386}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `elementwise_affine`.", "question_id": 53387}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `device`.", "question_id": 53388}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `dtype`.", "question_id": 53389}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True, device=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`, `device`.", "question_id": 53390}
{"snippet": "torch.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, elementwise_affine=True, dtype=None)", "intent": "This is the quantized version of LayerNorm . With arguments `normalized_shape`, `weight`, `bias`, `scale`, `zero_point`, `elementwise_affine`, `dtype`.", "question_id": 53391}
{"snippet": "Tensor.lgamma_()", "intent": "In-place version of lgamma ( )", "question_id": 53392}
{"snippet": "Tensor.tan()", "intent": "See torch.tan ( )", "question_id": 53393}
{"snippet": "torch.erfc(input)", "intent": "Alias for torch.special.erfc ( ) . With arguments `input`.", "question_id": 53394}
{"snippet": "torch.erfc(input, out=None)", "intent": "Alias for torch.special.erfc ( ) . With arguments `input`, `out`.", "question_id": 53395}
{"snippet": "torch.dist(input, other)", "intent": "Returns the p-norm of ( `input` - `other` )", "question_id": 53396}
{"snippet": "torch.dist(input, other, p=2)", "intent": "Returns the p-norm of ( `input` - `other` ) With arguments `p`.", "question_id": 53397}
{"snippet": "Tensor.put_(index, source)", "intent": "Copies the elements from `source` into the positions specified by `index` .", "question_id": 53398}
{"snippet": "Tensor.put_(index, source, accumulate=False)", "intent": "Copies the elements from `source` into the positions specified by `index` . If `accumulate` is True , the elements in source are added to self .", "question_id": 53399}
{"snippet": "Tensor.narrow(dimension, start, length)", "intent": "See torch.narrow ( ) With arguments `dimension`, `start`, `length`.", "question_id": 53400}
{"snippet": "Tensor.signbit()", "intent": "See torch.signbit ( )", "question_id": 53401}
{"snippet": "torch.nn.PixelShuffle(upscale_factor)", "intent": "Rearranges elements in a tensor of shape ( \u2217 , C\u00d7r2 , H , W ) ( * , C \\times r^2 , H , W ) ( \u2217 , C\u00d7r2 , H , W ) to a tensor of shape ( \u2217 , C , H\u00d7r , W\u00d7r ) ( * , C , H \\times r , W \\times r ) ( \u2217 , C , H\u00d7r , W\u00d7r ) , where r is an upscale factor . With arguments `upscale_factor`.", "question_id": 53402}
{"snippet": "torch.t(input)", "intent": "Expects `input` to be < = 2-D tensor and transposes dimensions 0 and 1 .", "question_id": 53403}
{"snippet": "Tensor.expand_as(other)", "intent": "Expand this tensor to the same size as `other` .", "question_id": 53404}
{"snippet": "profile.export_chrome_trace(path)", "intent": "Exports an EventList as a Chrome tracing tools file . With arguments `path`.", "question_id": 53405}
{"snippet": "torch.nn.functional.conv3d(input, weight)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`.", "question_id": 53406}
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`.", "question_id": 53407}
{"snippet": "torch.nn.functional.conv3d(input, weight, stride=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `stride`.", "question_id": 53408}
{"snippet": "torch.nn.functional.conv3d(input, weight, padding=0)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `padding`.", "question_id": 53409}
{"snippet": "torch.nn.functional.conv3d(input, weight, dilation=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `dilation`.", "question_id": 53410}
{"snippet": "torch.nn.functional.conv3d(input, weight, groups=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `groups`.", "question_id": 53411}
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, stride=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `stride`.", "question_id": 53412}
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, padding=0)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `padding`.", "question_id": 53413}
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, dilation=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `dilation`.", "question_id": 53414}
{"snippet": "torch.nn.functional.conv3d(input, weight, bias=None, groups=1)", "intent": "Applies a 3D convolution over an `input` image composed of several input planes . With arguments `weight`, `bias`, `groups`.", "question_id": 53415}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53416}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53417}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53418}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53419}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53420}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, qscheme=torch.per_tensor_affine)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53421}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53422}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, dtype=torch.quint8, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53423}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine, quant_min=0)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53424}
{"snippet": "torch.quantization.fake_quantize.FixedQParamsFakeQuantize(scale, zero_point, qscheme=torch.per_tensor_affine, quant_max=255)", "intent": "Simulate quantize and dequantize with fixed quantization parameters in training time . : param `scale` : fixed scale for the fake quantize module : type scale : float : param `zero_point` : fixed zero point for the fake quantize module : type zero_point : int : param `dtype` : : param `qscheme` : : param `quant_min` : : param `quant_max` :", "question_id": 53425}
{"snippet": "torch.nn.utils.prune.remove(module, name)", "intent": "Removes the pruning reparameterization from a `module` and the pruning method from the forward hook . The pruned parameter named `name` remains permanently pruned , and the parameter named name+'_orig ' is removed from the parameter list .", "question_id": 53426}
{"snippet": "torch.arctan(input)", "intent": "Alias for torch.atan ( ) . With arguments `input`.", "question_id": 53427}
{"snippet": "torch.arctan(input, out=None)", "intent": "Alias for torch.atan ( ) . With arguments `input`, `out`.", "question_id": 53428}
{"snippet": "Tensor.smm(mat)", "intent": "See torch.smm ( ) With arguments `mat`.", "question_id": 53429}
{"snippet": "torch.reciprocal(input)", "intent": "Returns a new tensor with the reciprocal of the elements of `input`", "question_id": 53430}
{"snippet": "torch.reciprocal(input, out=None)", "intent": "Returns a new tensor with the reciprocal of the elements of `input` With arguments `out`.", "question_id": 53431}
{"snippet": "torch.nn.AdaptiveMaxPool3d(output_size)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`.", "question_id": 53432}
{"snippet": "torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)", "intent": "Applies a 3D adaptive max pooling over an input signal composed of several input planes . With arguments `output_size`, `return_indices`.", "question_id": 53433}
{"snippet": "Tensor.not_equal_(other)", "intent": "In-place version of not_equal ( ) . With arguments `other`.", "question_id": 53434}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`.", "question_id": 53435}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`.", "question_id": 53436}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, bias=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `bias`.", "question_id": 53437}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, training=False)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `training`.", "question_id": 53438}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, momentum=0.1)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `momentum`.", "question_id": 53439}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, eps=1e-05)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `eps`.", "question_id": 53440}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `bias`.", "question_id": 53441}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, training=False)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `training`.", "question_id": 53442}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, momentum=0.1)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `momentum`.", "question_id": 53443}
{"snippet": "torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, eps=1e-05)", "intent": "Applies Batch Normalization for each channel across a batch of data . With arguments `input`, `running_mean`, `running_var`, `weight`, `eps`.", "question_id": 53444}
{"snippet": "torch.cuda.is_available()", "intent": "Returns a bool indicating if CUDA is currently available .", "question_id": 53445}
{"snippet": "Tensor.index_add_(dim, index, tensor)", "intent": "Accumulate the elements of attr : `alpha` times `tensor` into the self tensor by adding to the indices in the order given in `index` . For example , if `dim` == 0 , index [ i ] == j , and alpha=-1 , then the ith row of tensor is subtracted from the jth row of self .", "question_id": 53446}
{"snippet": "Tensor.index_add_(dim, index, tensor, alpha=1)", "intent": "Accumulate the elements of attr : `alpha` times `tensor` into the self tensor by adding to the indices in the order given in `index` . For example , if `dim` == 0 , index [ i ] == j , and alpha=-1 , then the ith row of tensor is subtracted from the jth row of self .", "question_id": 53447}
{"snippet": "torch.cuda.get_device_properties(device)", "intent": "Gets the properties of a `device` .", "question_id": 53448}
{"snippet": "Tensor.istft(n_fft)", "intent": "See torch.istft ( ) With arguments `n_fft`.", "question_id": 53449}
{"snippet": "Tensor.istft(n_fft, hop_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `hop_length`.", "question_id": 53450}
{"snippet": "Tensor.istft(n_fft, win_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `win_length`.", "question_id": 53451}
{"snippet": "Tensor.istft(n_fft, window=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `window`.", "question_id": 53452}
{"snippet": "Tensor.istft(n_fft, center=True)", "intent": "See torch.istft ( ) With arguments `n_fft`, `center`.", "question_id": 53453}
{"snippet": "Tensor.istft(n_fft, normalized=False)", "intent": "See torch.istft ( ) With arguments `n_fft`, `normalized`.", "question_id": 53454}
{"snippet": "Tensor.istft(n_fft, onesided=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `onesided`.", "question_id": 53455}
{"snippet": "Tensor.istft(n_fft, length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `length`.", "question_id": 53456}
{"snippet": "Tensor.istft(n_fft, return_complex=False)", "intent": "See torch.istft ( ) With arguments `n_fft`, `return_complex`.", "question_id": 53457}
{"snippet": "Tensor.istft(n_fft, hop_length=None, win_length=None)", "intent": "See torch.istft ( ) With arguments `n_fft`, `hop_length`, `win_length`.", "question_id": 53458}
{"snippet": "torch.nn.utils.parameters_to_vector(parameters)", "intent": "Convert `parameters` to one vector", "question_id": 53459}
{"snippet": "Tensor.deg2rad()", "intent": "See torch.deg2rad ( )", "question_id": 53460}
{"snippet": "Tensor.igamma_(other)", "intent": "In-place version of igamma ( ) With arguments `other`.", "question_id": 53461}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 53462}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`.", "question_id": 53463}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `momentum`.", "question_id": 53464}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, affine=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `affine`.", "question_id": 53465}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `track_running_stats`.", "question_id": 53466}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, device=None)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `device`.", "question_id": 53467}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, dtype=None)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `dtype`.", "question_id": 53468}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `momentum`.", "question_id": 53469}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, affine=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `affine`.", "question_id": 53470}
{"snippet": "torch.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, track_running_stats=False)", "intent": "This is the quantized version of InstanceNorm2d . With arguments `num_features`, `weight`, `bias`, `scale`, `zero_point`, `eps`, `track_running_stats`.", "question_id": 53471}
{"snippet": "torch.nn.utils.prune.l1_unstructured(module, name, amount)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units with the lowest L1-norm .", "question_id": 53472}
{"snippet": "torch.nn.utils.prune.l1_unstructured(module, name, amount, importance_scores=None)", "intent": "Prunes tensor corresponding to parameter called `name` in `module` by removing the specified `amount` of ( currently unpruned ) units with the lowest L1-norm . With arguments `importance_scores`.", "question_id": 53473}
{"snippet": "Tensor.cumprod_(dim)", "intent": "In-place version of cumprod ( ) With arguments `dim`.", "question_id": 53474}
{"snippet": "Tensor.cumprod_(dim, dtype=None)", "intent": "In-place version of cumprod ( ) With arguments `dim`, `dtype`.", "question_id": 53475}
{"snippet": "Tensor.log1p()", "intent": "See torch.log1p ( )", "question_id": 53476}
{"snippet": "torch.nn.BCELoss()", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output :", "question_id": 53477}
{"snippet": "torch.nn.BCELoss(weight=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`.", "question_id": 53478}
{"snippet": "torch.nn.BCELoss(size_average=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `size_average`.", "question_id": 53479}
{"snippet": "torch.nn.BCELoss(reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `reduce`.", "question_id": 53480}
{"snippet": "torch.nn.BCELoss(reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as :", "question_id": 53481}
{"snippet": "torch.nn.BCELoss(weight=None, size_average=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`, `size_average`.", "question_id": 53482}
{"snippet": "torch.nn.BCELoss(weight=None, reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `weight`, `reduce`.", "question_id": 53483}
{"snippet": "torch.nn.BCELoss(weight=None, reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as : With arguments `weight`.", "question_id": 53484}
{"snippet": "torch.nn.BCELoss(size_average=None, reduce=None)", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : With arguments `size_average`, `reduce`.", "question_id": 53485}
{"snippet": "torch.nn.BCELoss(size_average=None, reduction='mean')", "intent": "Creates a criterion that measures the Binary Cross Entropy between the target and the output : with `reduction` set to 'none ' ) loss can be described as : With arguments `size_average`.", "question_id": 53486}
{"snippet": "Tensor.is_signed()", "intent": "Returns True if the data type of self is a signed data type .", "question_id": 53487}
{"snippet": "torch.narrow(input, dim, start, length)", "intent": "Returns a new tensor that is a narrowed version of `input` tensor . The dimension `dim` is input from `start` to start + `length` .", "question_id": 53488}
{"snippet": "torch.quantization.observer.ObserverBase(dtype)", "intent": "Base observer Module . With arguments `dtype`.", "question_id": 53489}
{"snippet": "observer_base.with_args(**kwargs)", "intent": "Wrapper that allows creation of class factories . With arguments `**kwargs`.", "question_id": 53490}
{"snippet": "torch.nn.LocalResponseNorm(size)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`.", "question_id": 53491}
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`.", "question_id": 53492}
{"snippet": "torch.nn.LocalResponseNorm(size, beta=0.75)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`.", "question_id": 53493}
{"snippet": "torch.nn.LocalResponseNorm(size, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `k`.", "question_id": 53494}
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`.", "question_id": 53495}
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `k`.", "question_id": 53496}
{"snippet": "torch.nn.LocalResponseNorm(size, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `beta`, `k`.", "question_id": 53497}
{"snippet": "torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)", "intent": "Applies local response normalization over an input signal composed of several input planes , where channels occupy the second dimension . With arguments `size`, `alpha`, `beta`, `k`.", "question_id": 53498}
{"snippet": "torch.erfinv(input)", "intent": "Alias for torch.special.erfinv ( ) . With arguments `input`.", "question_id": 53499}
{"snippet": "torch.erfinv(input, out=None)", "intent": "Alias for torch.special.erfinv ( ) . With arguments `input`, `out`.", "question_id": 53500}
{"snippet": "torch.geqrf(input)", "intent": "This is a low-level function for calling LAPACK \u2019 s geqrf directly . Computes a QR decomposition of `input` .", "question_id": 53501}
{"snippet": "torch.geqrf(input, out=None)", "intent": "This is a low-level function for calling LAPACK \u2019 s geqrf directly . Computes a QR decomposition of `input` . With arguments `out`.", "question_id": 53502}
{"snippet": "Tensor.norm()", "intent": "See torch.norm ( )", "question_id": 53503}
{"snippet": "Tensor.norm(p='fro')", "intent": "See torch.norm ( ) With arguments `p`.", "question_id": 53504}
{"snippet": "Tensor.norm(dim=None)", "intent": "See torch.norm ( ) With arguments `dim`.", "question_id": 53505}
{"snippet": "Tensor.norm(keepdim=False)", "intent": "See torch.norm ( ) With arguments `keepdim`.", "question_id": 53506}
{"snippet": "Tensor.norm(dtype=None)", "intent": "See torch.norm ( ) With arguments `dtype`.", "question_id": 53507}
{"snippet": "Tensor.norm(p='fro', dim=None)", "intent": "See torch.norm ( ) With arguments `p`, `dim`.", "question_id": 53508}
{"snippet": "Tensor.norm(p='fro', keepdim=False)", "intent": "See torch.norm ( ) With arguments `p`, `keepdim`.", "question_id": 53509}
{"snippet": "Tensor.norm(p='fro', dtype=None)", "intent": "See torch.norm ( ) With arguments `p`, `dtype`.", "question_id": 53510}
{"snippet": "Tensor.norm(dim=None, keepdim=False)", "intent": "See torch.norm ( ) With arguments `dim`, `keepdim`.", "question_id": 53511}
{"snippet": "Tensor.norm(dim=None, dtype=None)", "intent": "See torch.norm ( ) With arguments `dim`, `dtype`.", "question_id": 53512}
{"snippet": "torch.nn.parameter.UninitializedBuffer()", "intent": "A buffer that is not initialized .", "question_id": 53513}
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False)", "intent": "A buffer that is not initialized . With arguments `requires_grad`.", "question_id": 53514}
{"snippet": "torch.nn.parameter.UninitializedBuffer(device=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor .", "question_id": 53515}
{"snippet": "torch.nn.parameter.UninitializedBuffer(dtype=None)", "intent": "A buffer that is not initialized . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g .", "question_id": 53516}
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . With arguments `requires_grad`.", "question_id": 53517}
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, dtype=None)", "intent": "A buffer that is not initialized . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 53518}
{"snippet": "torch.nn.parameter.UninitializedBuffer(device=None, dtype=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g .", "question_id": 53519}
{"snippet": "torch.nn.parameter.UninitializedBuffer(requires_grad=False, device=None, dtype=None)", "intent": "A buffer that is not initialized . The only operations that can be performed on a uninitialized parameter are changing its datatype , moving it to a different `device` and converting it to a regular torch.Tensor . The default device or `dtype` to use when the buffer is materialized can be set during construction using e.g . With arguments `requires_grad`.", "question_id": 53520}
{"snippet": "torch.hypot(input, other)", "intent": "Given the legs of a right triangle , return its hypotenuse . The shapes of `input` and `other` must be broadcastable .", "question_id": 53521}
{"snippet": "torch.hypot(input, other, out=None)", "intent": "Given the legs of a right triangle , return its hypotenuse . The shapes of `input` and `other` must be broadcastable . With arguments `out`.", "question_id": 53522}
{"snippet": "Tensor.conj()", "intent": "See torch.conj ( )", "question_id": 53523}
{"snippet": "torch.nn.Threshold(threshold, value)", "intent": "Thresholds each element of the input Tensor . With arguments `threshold`, `value`.", "question_id": 53524}
{"snippet": "torch.nn.Threshold(threshold, value, inplace=False)", "intent": "Thresholds each element of the input Tensor . With arguments `threshold`, `value`, `inplace`.", "question_id": 53525}
{"snippet": "torch.squeeze(input)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed .", "question_id": 53526}
{"snippet": "torch.squeeze(input, dim=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . When `dim` is given , a squeeze operation is done only in the given dimension .", "question_id": 53527}
{"snippet": "torch.squeeze(input, out=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . For example , if input is of shape : ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) ( A \\times 1 \\times B \\times C \\times 1 \\times D ) ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) then the `out` tensor will be of shape : ( A\u00d7B\u00d7C\u00d7D ) ( A \\times B \\times C \\times D ) ( A\u00d7B\u00d7C\u00d7D ) .", "question_id": 53528}
{"snippet": "torch.squeeze(input, dim=None, out=None)", "intent": "Returns a tensor with all the dimensions of `input` of size 1 removed . When `dim` is given , a squeeze operation is done only in the given dimension . For example , if input is of shape : ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) ( A \\times 1 \\times B \\times C \\times 1 \\times D ) ( A\u00d71\u00d7B\u00d7C\u00d71\u00d7D ) then the `out` tensor will be of shape : ( A\u00d7B\u00d7C\u00d7D ) ( A \\times B \\times C \\times D ) ( A\u00d7B\u00d7C\u00d7D ) .", "question_id": 53529}
{"snippet": "torch.ones(*size)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`.", "question_id": 53530}
{"snippet": "torch.ones(*size, out=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`.", "question_id": 53531}
{"snippet": "torch.ones(*size, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `dtype`.", "question_id": 53532}
{"snippet": "torch.ones(*size, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `layout`.", "question_id": 53533}
{"snippet": "torch.ones(*size, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `device`.", "question_id": 53534}
{"snippet": "torch.ones(*size, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `requires_grad`.", "question_id": 53535}
{"snippet": "torch.ones(*size, out=None, dtype=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `dtype`.", "question_id": 53536}
{"snippet": "torch.ones(*size, out=None, layout=torch.strided)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `layout`.", "question_id": 53537}
{"snippet": "torch.ones(*size, out=None, device=None)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `device`.", "question_id": 53538}
{"snippet": "torch.ones(*size, out=None, requires_grad=False)", "intent": "Returns a tensor filled with the scalar value 1 , with the shape defined by the variable argument size . With arguments `*size`, `out`, `requires_grad`.", "question_id": 53539}
{"snippet": "Tensor.map_(tensor, callable)", "intent": "Applies `callable` for each element in self `tensor` and the given tensor and stores the results in self tensor .", "question_id": 53540}
{"snippet": "torch.nn.BatchNorm1d(num_features)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`.", "question_id": 53541}
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`.", "question_id": 53542}
{"snippet": "torch.nn.BatchNorm1d(num_features, momentum=0.1)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`.", "question_id": 53543}
{"snippet": "torch.nn.BatchNorm1d(num_features, affine=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `affine`.", "question_id": 53544}
{"snippet": "torch.nn.BatchNorm1d(num_features, track_running_stats=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`.", "question_id": 53545}
{"snippet": "torch.nn.BatchNorm1d(num_features, device=None)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `device`.", "question_id": 53546}
{"snippet": "torch.nn.BatchNorm1d(num_features, dtype=None)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `dtype`.", "question_id": 53547}
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . The running estimates are kept with a default `momentum` of 0.1 . With arguments `num_features`, `eps`.", "question_id": 53548}
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, affine=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . With arguments `num_features`, `eps`, `affine`.", "question_id": 53549}
{"snippet": "torch.nn.BatchNorm1d(num_features, eps=1e-05, track_running_stats=True)", "intent": "Applies Batch Normalization over a 2D or 3D input ( a mini-batch of 1D inputs with optional additional channel dimension ) as described in the paper Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift . If `track_running_stats` is set to False , this layer then does not keep running estimates , and batch statistics are instead used during evaluation time as well . With arguments `num_features`, `eps`.", "question_id": 53550}
{"snippet": "torch.quasirandom.SobolEngine(dimension)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 .", "question_id": 53551}
{"snippet": "torch.quasirandom.SobolEngine(dimension, scramble=False)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `scramble`.", "question_id": 53552}
{"snippet": "torch.quasirandom.SobolEngine(dimension, seed=None)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `seed`.", "question_id": 53553}
{"snippet": "torch.quasirandom.SobolEngine(dimension, scramble=False, seed=None)", "intent": "The torch.quasirandom.SobolEngine is an engine for generating ( scrambled ) Sobol sequences . This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum `dimension` of 21201 . With arguments `scramble`, `seed`.", "question_id": 53554}
{"snippet": "sobol_engine.draw()", "intent": "Function to draw a sequence of `n` points from a Sobol sequence .", "question_id": 53555}
{"snippet": "sobol_engine.draw(n=1)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence .", "question_id": 53556}
{"snippet": "sobol_engine.draw(out=None)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`.", "question_id": 53557}
{"snippet": "sobol_engine.draw(dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `dtype`.", "question_id": 53558}
{"snippet": "sobol_engine.draw(n=1, out=None)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`.", "question_id": 53559}
{"snippet": "sobol_engine.draw(n=1, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `dtype`.", "question_id": 53560}
{"snippet": "sobol_engine.draw(out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 53561}
{"snippet": "sobol_engine.draw(n=1, out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of `n` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 53562}
{"snippet": "sobol_engine.draw_base2(m)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence .", "question_id": 53563}
{"snippet": "sobol_engine.draw_base2(m, out=None)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `out`.", "question_id": 53564}
{"snippet": "sobol_engine.draw_base2(m, dtype=torch.float32)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `dtype`.", "question_id": 53565}
{"snippet": "sobol_engine.draw_base2(m, out=None, dtype=torch.float32)", "intent": "Function to draw a sequence of 2 * * `m` points from a Sobol sequence . With arguments `out`, `dtype`.", "question_id": 53566}
{"snippet": "sobol_engine.fast_forward(n)", "intent": "Function to fast-forward the state of the SobolEngine by `n` steps .", "question_id": 53567}
{"snippet": "sobol_engine.reset()", "intent": "Function to reset the SobolEngine to base state .", "question_id": 53568}
{"snippet": "torch.cumsum(input, dim)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` .", "question_id": 53569}
{"snippet": "torch.cumsum(input, dim, dtype=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `dtype`.", "question_id": 53570}
{"snippet": "torch.cumsum(input, dim, out=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `out`.", "question_id": 53571}
{"snippet": "torch.cumsum(input, dim, dtype=None, out=None)", "intent": "Returns the cumulative sum of elements of `input` in the dimension `dim` . With arguments `dtype`, `out`.", "question_id": 53572}
{"snippet": "Tensor.cumsum_(dim)", "intent": "In-place version of cumsum ( ) With arguments `dim`.", "question_id": 53573}
{"snippet": "Tensor.cumsum_(dim, dtype=None)", "intent": "In-place version of cumsum ( ) With arguments `dim`, `dtype`.", "question_id": 53574}
{"snippet": "torch.inverse(input)", "intent": "Alias for torch.linalg.inv ( ) With arguments `input`.", "question_id": 53575}
{"snippet": "torch.inverse(input, out=None)", "intent": "Alias for torch.linalg.inv ( ) With arguments `input`, `out`.", "question_id": 53576}
{"snippet": "Tensor.nan_to_num_()", "intent": "In-place version of nan_to_num ( ) .", "question_id": 53577}
{"snippet": "Tensor.nan_to_num_(nan=0.0)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`.", "question_id": 53578}
{"snippet": "Tensor.nan_to_num_(posinf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `posinf`.", "question_id": 53579}
{"snippet": "Tensor.nan_to_num_(neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `neginf`.", "question_id": 53580}
{"snippet": "Tensor.nan_to_num_(nan=0.0, posinf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `posinf`.", "question_id": 53581}
{"snippet": "Tensor.nan_to_num_(nan=0.0, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `neginf`.", "question_id": 53582}
{"snippet": "Tensor.nan_to_num_(posinf=None, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `posinf`, `neginf`.", "question_id": 53583}
{"snippet": "Tensor.nan_to_num_(nan=0.0, posinf=None, neginf=None)", "intent": "In-place version of nan_to_num ( ) . With arguments `nan`, `posinf`, `neginf`.", "question_id": 53584}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 53585}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `max_norm`.", "question_id": 53586}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, norm_type=2.0)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `norm_type`.", "question_id": 53587}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, scale_grad_by_freq=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `scale_grad_by_freq`.", "question_id": 53588}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, mode='mean')", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . This scales the output of the Embedding before performing a weighted reduction as specified by `mode` . With arguments `num_embeddings`, `embedding_dim`.", "question_id": 53589}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, sparse=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `sparse`.", "question_id": 53590}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, _weight=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `_weight`.", "question_id": 53591}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, include_last_offset=False)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `include_last_offset`.", "question_id": 53592}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, padding_idx=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . For bags of constant length , no per_sample_weights , no indices equal to `padding_idx` , and with 2D inputs , this class With arguments `num_embeddings`, `embedding_dim`.", "question_id": 53593}
{"snippet": "torch.nn.EmbeddingBag(num_embeddings, embedding_dim, device=None)", "intent": "Computes sums or means of \u2018 bags \u2019 of embeddings , without instantiating the intermediate embeddings . With arguments `num_embeddings`, `embedding_dim`, `device`.", "question_id": 53594}
{"snippet": "embedding_bag.forward(input)", "intent": "Forward pass of EmbeddingBag . With arguments `input`.", "question_id": 53595}
{"snippet": "embedding_bag.forward(input, offsets=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `offsets`.", "question_id": 53596}
{"snippet": "embedding_bag.forward(input, per_sample_weights=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `per_sample_weights`.", "question_id": 53597}
{"snippet": "embedding_bag.forward(input, offsets=None, per_sample_weights=None)", "intent": "Forward pass of EmbeddingBag . With arguments `input`, `offsets`, `per_sample_weights`.", "question_id": 53598}
{"snippet": "embedding_bag.from_pretrained(embeddings)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`.", "question_id": 53599}
{"snippet": "embedding_bag.from_pretrained(embeddings, freeze=True)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`.", "question_id": 53600}
{"snippet": "embedding_bag.from_pretrained(embeddings, max_norm=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `max_norm`.", "question_id": 53601}
{"snippet": "embedding_bag.from_pretrained(embeddings, norm_type=2.0)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `norm_type`.", "question_id": 53602}
{"snippet": "embedding_bag.from_pretrained(embeddings, scale_grad_by_freq=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `scale_grad_by_freq`.", "question_id": 53603}
{"snippet": "embedding_bag.from_pretrained(embeddings, mode='mean')", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `mode`.", "question_id": 53604}
{"snippet": "embedding_bag.from_pretrained(embeddings, sparse=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `sparse`.", "question_id": 53605}
{"snippet": "embedding_bag.from_pretrained(embeddings, include_last_offset=False)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `include_last_offset`.", "question_id": 53606}
{"snippet": "embedding_bag.from_pretrained(embeddings, padding_idx=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `padding_idx`.", "question_id": 53607}
{"snippet": "embedding_bag.from_pretrained(embeddings, freeze=True, max_norm=None)", "intent": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor . With arguments `embeddings`, `freeze`, `max_norm`.", "question_id": 53608}
{"snippet": "torch.nn.quantized.functional.linear(input, weight)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`.", "question_id": 53609}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`.", "question_id": 53610}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, scale=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `scale`.", "question_id": 53611}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `zero_point`.", "question_id": 53612}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `scale`.", "question_id": 53613}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `zero_point`.", "question_id": 53614}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, scale=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `scale`, `zero_point`.", "question_id": 53615}
{"snippet": "torch.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None)", "intent": "Applies a linear transformation to the incoming quantized data : y=xAT+by = xA^T + by=xAT+b . With arguments `input`, `weight`, `bias`, `scale`, `zero_point`.", "question_id": 53616}
{"snippet": "torch.equal(input, other)", "intent": "True if two tensors have the same size and elements , False otherwise . With arguments `input`, `other`.", "question_id": 53617}
{"snippet": "torch.std_mean(input, dim, unbiased)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`.", "question_id": 53618}
{"snippet": "torch.std_mean(input, dim, unbiased, keepdim=False)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`.", "question_id": 53619}
{"snippet": "torch.std_mean(input, dim, unbiased, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `out`.", "question_id": 53620}
{"snippet": "torch.std_mean(input, dim, unbiased, keepdim=False, out=None)", "intent": "If `unbiased` is True , Bessel \u2019 s correction will be used to calculate the standard deviation . Calculates the standard deviation and mean of all elements in the `input` tensor . With arguments `dim`, `keepdim`, `out`.", "question_id": 53621}
{"snippet": "Tensor.addcdiv(tensor1, tensor2)", "intent": "See torch.addcdiv ( ) With arguments `tensor1`, `tensor2`.", "question_id": 53622}
{"snippet": "Tensor.addcdiv(tensor1, tensor2, value=1)", "intent": "See torch.addcdiv ( ) With arguments `tensor1`, `tensor2`, `value`.", "question_id": 53623}
{"snippet": "torch.nn.Flatten()", "intent": "Flattens a contiguous range of dims into a tensor .", "question_id": 53624}
{"snippet": "torch.nn.Flatten(start_dim=1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `start_dim`.", "question_id": 53625}
{"snippet": "torch.nn.Flatten(end_dim=- 1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `end_dim`.", "question_id": 53626}
{"snippet": "torch.nn.Flatten(start_dim=1, end_dim=- 1)", "intent": "Flattens a contiguous range of dims into a tensor . With arguments `start_dim`, `end_dim`.", "question_id": 53627}
{"snippet": "torch.det(input)", "intent": "Alias for torch.linalg.det ( ) With arguments `input`.", "question_id": 53628}
{"snippet": "torch.nn.functional.adaptive_avg_pool3d(input, output_size)", "intent": "Applies a 3D adaptive average pooling over an `input` signal composed of several input planes . With arguments `output_size`.", "question_id": 53629}
{"snippet": "torch.gt(input, other)", "intent": "Computes `input` > other\\text { input } > \\text { `other` } input > other element-wise .", "question_id": 53630}
{"snippet": "torch.gt(input, other, out=None)", "intent": "Computes `input` > other\\text { input } > \\text { `other` } input > other element-wise . With arguments `out`.", "question_id": 53631}
{"snippet": "torch.sort(input)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value .", "question_id": 53632}
{"snippet": "torch.sort(input, dim=- 1)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen .", "question_id": 53633}
{"snippet": "torch.sort(input, descending=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value .", "question_id": 53634}
{"snippet": "torch.sort(input, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 53635}
{"snippet": "torch.sort(input, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . With arguments `out`.", "question_id": 53636}
{"snippet": "torch.sort(input, dim=- 1, descending=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . If `descending` is True then the elements are sorted in descending order by value .", "question_id": 53637}
{"snippet": "torch.sort(input, dim=- 1, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 53638}
{"snippet": "torch.sort(input, dim=- 1, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `dim` is not given , the last dimension of the input is chosen . With arguments `out`.", "question_id": 53639}
{"snippet": "torch.sort(input, descending=False, stable=False)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value . If `stable` is True then the sorting routine becomes stable , preserving the order of equivalent elements .", "question_id": 53640}
{"snippet": "torch.sort(input, descending=False, out=None)", "intent": "Sorts the elements of the `input` tensor along a given dimension in ascending order by value . If `descending` is True then the elements are sorted in descending order by value . With arguments `out`.", "question_id": 53641}
{"snippet": "torch.cuda.seed_all()", "intent": "Sets the seed for generating random numbers to a random number on all GPUs .", "question_id": 53642}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`.", "question_id": 53643}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`.", "question_id": 53644}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`.", "question_id": 53645}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `device`.", "question_id": 53646}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `dtype`.", "question_id": 53647}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `momentum`.", "question_id": 53648}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `device`.", "question_id": 53649}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, eps=1e-05, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `eps`, `dtype`.", "question_id": 53650}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1, device=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`, `device`.", "question_id": 53651}
{"snippet": "torch.nn.quantized.BatchNorm2d(num_features, momentum=0.1, dtype=None)", "intent": "This is the quantized version of BatchNorm2d . With arguments `num_features`, `momentum`, `dtype`.", "question_id": 53652}
{"snippet": "Tensor.index_put(tensor1, indices, values)", "intent": "Out-place version of index_put_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_put_ ( ) . With arguments `indices`, `values`.", "question_id": 53653}
{"snippet": "Tensor.index_put(tensor1, indices, values, accumulate=False)", "intent": "Out-place version of index_put_ ( ) . `tensor1` corresponds to self in torch.Tensor.index_put_ ( ) . With arguments `indices`, `values`, `accumulate`.", "question_id": 53654}
{"snippet": "torch.nn.ParameterList()", "intent": "Holds `parameters` in a list .", "question_id": 53655}
{"snippet": "torch.nn.ParameterList(parameters=None)", "intent": "Holds `parameters` in a list .", "question_id": 53656}
{"snippet": "parameter_list.append(parameter)", "intent": "Appends a given `parameter` at the end of the list .", "question_id": 53657}
{"snippet": "parameter_list.extend(parameters)", "intent": "Appends `parameters` from a Python iterable to the end of the list .", "question_id": 53658}
{"snippet": "Tensor.swapdims(dim0, dim1)", "intent": "See torch.swapdims ( ) With arguments `dim0`, `dim1`.", "question_id": 53659}
{"snippet": "Tensor.det()", "intent": "See torch.det ( )", "question_id": 53660}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`.", "question_id": 53661}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`.", "question_id": 53662}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, padding=0)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `padding`.", "question_id": 53663}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `ceil_mode`.", "question_id": 53664}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `count_include_pad`.", "question_id": 53665}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `divisor_override`.", "question_id": 53666}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 53667}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 53668}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, count_include_pad=True)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `count_include_pad`.", "question_id": 53669}
{"snippet": "torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, divisor_override=None)", "intent": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps . The number of output features is equal to \u230ainput planessT\u230b\\lfloor\\frac { \\text { `input` planes } } { sT } \\rfloor\u230asTinput planes\u200b\u230b . With arguments `kernel_size`, `stride`, `divisor_override`.", "question_id": 53670}
{"snippet": "Tensor.hypot(other)", "intent": "See torch.hypot ( ) With arguments `other`.", "question_id": 53671}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`.", "question_id": 53672}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`.", "question_id": 53673}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, padding=0)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `padding`.", "question_id": 53674}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, dilation=1)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `dilation`.", "question_id": 53675}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, ceil_mode=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `ceil_mode`.", "question_id": 53676}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, return_indices=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `return_indices`.", "question_id": 53677}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, padding=0)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `padding`.", "question_id": 53678}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, dilation=1)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `dilation`.", "question_id": 53679}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, ceil_mode=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `ceil_mode`.", "question_id": 53680}
{"snippet": "torch.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, return_indices=False)", "intent": "Applies a 1D max pooling over a quantized `input` signal composed of several quantized input planes . With arguments `kernel_size`, `stride`, `return_indices`.", "question_id": 53681}
